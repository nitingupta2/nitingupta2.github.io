[{"authors":["admin"],"categories":null,"content":"I am the founder of Quantitative Data Technologies, a consulting firm specializing in data analytics and modeling. I have deep expertise in the application of data science and machine learning that provide actionable insights from data.\nPreviously, I worked at a leading hedge fund, where I built automated trading systems and systematic investing strategies.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am the founder of Quantitative Data Technologies, a consulting firm specializing in data analytics and modeling. I have deep expertise in the application of data science and machine learning that provide actionable insights from data.\nPreviously, I worked at a leading hedge fund, where I built automated trading systems and systematic investing strategies.","tags":null,"title":"Nitin Gupta","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"","date":1561593600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561593600,"objectID":"2d598053d384ae5634ddf7f72d67be80","permalink":"/project/market-dashboard/","publishdate":"2019-06-27T00:00:00Z","relpermalink":"/project/market-dashboard/","section":"project","summary":"An overview of major markets using representative ETFs","tags":["Shiny Apps"],"title":"Market Dashboard","type":"project"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":["datascience"],"content":"\r\rIn my previous post, we’ve seen how kids improve year-by-year in swimming. Certainly kids get stronger and faster as they grow. But how much impact does practice and coaching have? As I mentioned in my previous post, coaches do not keep a record of attendance during practice. There are daily hour long practice sessions during the summers. But even though it isn’t known how regular the kids are in attending practice, the number of meets that a kid participates in during the season, can be a good proxy. Afterall, it’s highly unlikely that a kid participates in meets while mostly skipping practice.\nTo judge improvement in a stroke, we’d like to see whether a kid got enough opportunities to practice as well as participate in meets. The summer league lasts for about 2 months. So, a reasonable start would be to take the data of kids whose first and last meet participations in a stroke were spaced at least 4 weeks apart.\nLet’s see the distribution of the number of meets those kids participate in a season:\nSeems like, for every stroke, most kids participate in 4 or more meets in a season. So we’ll select it as a threshold to judge improvement. Then, for each kid who participates in 4 or more meets in a season, the improvement will be the difference between the first and the last swim times of the season.\nEstimating Improvement\rHere are the distributions of improvement in swim times by age:\nWe see wide variance among 6 to 10 year olds. But as the kids grow older, the variance reduces.\nIn Breaststroke and Butterfly, there aren’t any 6 year old boys who satisfy the criteria for inclusion. So, instead of looking at improvement by age, it would be better to categorize it by age groups. We could classify kids into 3 age groups, younger kids (6-10), pre-teens and early teens (11-14) and mid to late teens (15-18).\nLet’s see a similar plot by age groups:\n\rIs Improvement Significant?\rNow that we know the distribution of improvement in swim times, the question arises, is it statistically significant? In other words, does practice and coaching have a significant effect within a short season? It is hard to tell from the plot of distributions itself. Within each age group and stroke, we have a paired sample of swim times, where for the same swimmer, we know their first and last swim times of a season. By using statistical tests, we could quantify mean improvements and determine whether or not they are significant.\nFrequentist Method\rIn traditional statistics, a statistical test known as dependent-samples t test can be used to evaluate whether there is a significant difference between the means of first and last swim times of a season. The null hypothesis is that any change in mean swim times is due to random chance.\nLet’s run this test on the sample of Freestyle swim times of 15-18 year old boys.\n\rPaired t-test\rdata: first_time_of_season and last_time_of_season\rt = 1.8, df = 31, p-value = 0.08\ralternative hypothesis: true difference in means is not equal to 0\r95 percent confidence interval:\r-0.03343 0.52031\rsample estimates:\rmean of the differences 0.2434 \rThe mean improvement is about 0.24 seconds, but the 95% confidence interval includes 0, which means we cannot reject the null hypothesis.\nLet’s look at the mean improvement and 95% confidence intervals for all the age groups, plotted below:\nLooking at the plot, we could say girls of all age groups show a statistically significant improvement in swim times in all the strokes. But for boys, the picture is not so clear cut. Boys aged 6-10 and 11-14 show all around statistically significant improvement. But, as seen before, in Freestyle and also in Butterfly, boys aged 15-18 do not. In both the latter cases, confidence intervals include zero. Point estimates are shown with a hollow circle.\nBut look closely at the p values. Even though the improvements in Backstroke and Breaststroke among 15-18 year old boys appear to be statistically significant, it is only because of the arbitrary choice of 95% confidence level, which has become a de facto standard in published research. With p values of 0.01 and 0.028, we wouldn’t reject the null hypothesis under a 99% confidence level.\nIf this were a study to be published by a researcher in an academic journal, there would be a temptation to run multiple experiments with different filter criteria and attempt to produce final results according to their own bias; a.k.a. p-hacking.\nUnder the null hypothesis, we’d draw the conclusion that 15-18 year old boys do not show any statistically significant improvement in a season. We might interpret that the hard work they put in practice is barely enough to maintain their time during a season. With a biased view, we might even question whether they’re putting in as much hard work as the girls of their age.\nBut, would either of these conclusions be correct? Afterall, girls of all age groups and most boys show statistically significant improvement. So, why would 15-18 year old boys be any different?\n\rBayesian Method\rThis is where a Bayesian approach works best. Instead of determining whether the mean difference between the first and last swim times of the season is zero, which is uninformative, the Bayesian way is to determine how much have mean swim times improved, with associated probabilities.\nLet’s see the results from a Bayesian counterpart to the t test, on a sample of Freestyle swim times of 15-18 year old boys.\n\rBayesian estimation supersedes the t test (BEST) - paired samples\rdata: first_time_of_season and last_time_of_season, n = 32\rEstimates [95% credible interval]\rmean paired difference: 0.24 [-0.026, 0.50]\rsd of the paired differences: 0.70 [0.45, 0.96]\rThe mean difference is more than 0 by a probability of 0.96 and less than 0 by a probability of 0.04 \rInstead of p-values, this Bayesian test provides an actual probability of improvement in swim times. We get estimates for mean and standard deviation of swim time improvements, along with credible or high density intervals. The probability that the mean improvement is more than zero is 95.9%, though the estimate is less precise and the 95% confidence interval includes zero.\nLet’s run the Bayesian test for all strokes and age groups in both genders:\nLooking at the distributions, we can see that 15-18 year old boys show a very small improvement in their Freestyle and Butterfly times, but the estimate is less precise and the confidence intervals include zero (those distributions are shaded with a light blue color). Nevertheless the probability of mean improvement being greater than zero is in the high 90s, which from a practical standpoint, is convincing enough.\n\r\rConclusion\rWith a reasonable set of filters, we created sample datasets by age groups to determine whether kids show significant improvement in swim times within a short summer season. We applied Bayesian testing which turns out to be more informative than traditional statistical testing. For all intents and purposes, we can conclude that kids of all age groups show improvement in their swim times, even within a short summer season.\nThe R markdown file with code for this post is available here.\n\r","date":1539561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539561600,"objectID":"fcbb7f50660f638259f10fa0fb803cf8","permalink":"/post/how-much-do-swimmers-improve-in-a-season/","publishdate":"2018-10-15T00:00:00Z","relpermalink":"/post/how-much-do-swimmers-improve-in-a-season/","section":"post","summary":"In my previous post, we’ve seen how kids improve year-by-year in swimming. Certainly kids get stronger and faster as they grow. But how much impact does practice and coaching have? As I mentioned in my previous post, coaches do not keep a record of attendance during practice. There are daily hour long practice sessions during the summers. But even though it isn’t known how regular the kids are in attending practice, the number of meets that a kid participates in during the season, can be a good proxy.","tags":["rstats","dataviz","bayesian","swimming"],"title":"How Much Do Swimmers Improve in a Season?","type":"post"},{"authors":null,"categories":["datascience"],"content":"\r\rThis past summer, my daughter joined a swim team. Her team is among several teams that are part of an area league. All summer long, developmental and competitive meets are organized where teams compete on a one-to-one basis. All kids in a team swim in developmental meets. For competitive meets, top 3 or 4 kids are chosen by age groups. Meets are organized in community pools that are either 25 meters or 25 yards in length. The team schedules are packed with practices and events which are intense but thoroughly enjoyable.\nThis is one sport where progress can be measured on almost daily basis. While most young kids take it up as a hobby, older kids seem to be very competitive and quite mindful of their swim times. Besides her own results this year, I was curious to know how my daughter did compared to a larger cohort of kids her own age. And what could we expect, if she keeps at it in the years to come?\nLuckily, I found past several years of her team’s results to analyze. Gathering this data, cleaning and transforming it for analysis was a huge challenge onto itself. Not something I intend to discuss in this post. But I digress.\nLet’s take a look at the number of kids by age and gender in this dataset.\nThe pyramid plot above shows the number of competitors from ages 6 to 18 throughout the years. Seems like 6 to 8 are the most popular ages for kids to begin competitive swimming. There are some late joiners by age 10, after which we see a steady decline in the number of kids competing.\nLet’s take a look at the ages when swimmers compete in each stroke.\nWe can observe a few things here:\n\rEveryone starts with Freestyle swimming. The participation rate is close to 100% for both genders. But towards the late teens, some boys seem to give up Freestyle to specialize in other strokes.\n\rGirls seem to learn Backstroke a little faster than boys, and a wide majority of girls keep swimming Backstroke until their late teens.\n\rGirls catch up with learning Breaststroke much more quickly than boys, and a majority of them keep at it until their late teens, where as some boys choose to specialize in either Backstroke or Butterfly.\n\rLearning Butterfly appears to be a much bigger challenge for both genders. Relatively fewer kids compete in it early on. Boys in their early teens appear to participate at a faster clip than girls. But towards the late teens, girls appear to participate more consistently.\n\r\rSteep Learning Curve\rThere’s a steep learning curve in swimming. Each stroke has its own level of difficulty and challenges but once the basics are learned, rapid progress could be made. However, there’s a huge difference between knowing how to swim a stroke, and swimming it well. It requires both speed and precision.\nSwimming also happens to be one of the most unforgiving sports. In other sports, minor violations result in fouls or penalties; disqualifications or ejections are rare and usually result from violent or reckless behavior. But in swimming, the moment a competitor violates a rule, it results in disqualification. Even in summer leagues, the swim officials are trained and expected to follow USA Swimming standards. So, a 6 year old is to be judged the same way as an adult at USA Swimming events.\nTo get an idea, here are some ways of getting disqualified:\n\rFreestyle: As the name suggests, everything is legal except the following:\r\rWalking on the bottom of the pool.\rStopping and pushing off of the bottom of the pool.\rPulling on a lane line for an assist.\rNot touching the wall before turning or at finish.\r\rBackstroke\r\rNot swimming on back off the wall.\rDelaying initiating a turn.\rPulling on a lane line for an assist.\rNot touching the wall before turning or at finish.\r\rBreaststroke\r\rPulling hands beyond hips.\rNon-simultaneous or single hand touch at the turn or finish.\rDoing alternating kicks (as in Freestyle) or dolphin kicks (as in Butterfly).\rNot being on breast after leaving the wall.\r\rButterfly\r\rArms underwater during the recovery phase of stroke.\rNon-simulatenous arm movements during strokes.\rNon-simultaneous or single hand touch at the turn or finish.\rDoing alternating kicks (as in Freestyle) or Breaststroke kicks.\r\r\rFrom what I observed throughout the season, in a majority of cases, rule violations are not deliberate. They happen inadvertently.\nLet’s take a look at the total number of disqualifications by age, gender and stroke in the dataset:\nThe absolute numbers of DQs are a proxy for the level of difficulty of each stroke. Most kids learn each stroke in this order from Freestyle to Butterfly. However, as seen in the previous plot, the level of participation in Butterfly is far less as compared to other strokes. For instance, the level of participation in Freestyle and Backstroke is consistently at ~ 80% or above. About 80% of 7 year olds attempt to swim Breaststroke, where as only ~ 33% of 7 year old boys and ~ 48% of 7 year old girls attempt Butterfly in comparison. Hence the absolute numbers of disqualifications merely reflect the total number of unique competitors at that age.\nA better way to look at this is to compute the number of DQs per person, as shown below.\nHere, we can see the number of disqualifications per person reduce as the kids get better with age.\rIn Freestyle, Backstroke and Breaststroke races, kids upto 8 years swim only 1 lap. In Butterfly, kids upto 10 years swim only 1 lap.\rIn all other age brackets, kids swim 2 laps, which means they turn after touching the opposite end of the pool. Learning to execute a legal turn without compromising pace is both an art and a science. We can see its challenges in Backstroke at age 9 and in Butterfly at age 11, when the kids start swimming 2 laps in a race. The number of disqualifications per person increase after decreasing upto the prior year when the kids are swimming only 1 lap.\nConsistently swimming a legal backstroke appears to remain a challenge until the mid-teens. It is indicative of the failures in executing legal turns. Backstroke could be thought of as swimming Freestyle on the back. Arm and leg movements have no constraints as long as the body is on the back. But the biggest challenge is the swimmers cannot see the wall in the direction they are swimming. Their eyes are always facing up. They train to spot a row of flags hung 5 meters from the wall over the pool and count the number of strokes they need to reach the wall from that point. While some inexperienced swimmers touch the wall with their hands and push off again on the back, it compromises their speed. The quickest way to turn is to flip on the belly while approaching a turn and push off against the wall with the feet so that the body regains its position on the back. This needs to be done in one smooth series of motions. As soon as the body flips on the belly, only a single or double hand pull is allowed to initiate the turn. If the swimmer does multiple strokes, they’re disqualified. If the swimmer turns too soon, it’s likely they’ll miss touching the wall. No sculling is allowed to reach the wall. Therefore, again a swimmer gets disqualified. We can see these mistakes happen well into the teenage years.\nFor every boy competing in Breaststroke at age 6, there has been more than 1 disqualification, which means 6 year old boys face multiple disqualifications in a season. Their level of participation is at 40%. It shows how challenging it is to learn Breaststroke. 6 year old girls appear to do a bit better. The good thing is that kids are persistent in learning Breaststroke from an early age. So, not only does the level of participation increase with age, but also the number of disqualifications per person decrease dramatically. But again, at age 9 when the kids start swimming 2 laps, the rate of improvement slows down. Other than the Breaststroke technique, kids have to remember to touch the wall with both hands simultaneously and then turn back on their breast. Most kids become very good with practice, but even a slight lapse of concentration results in disqualification.\nSimilarly, Butterfly is a bigger challenge in the early years. With every stroke, the arms have to break the surface of the water during recovery. Many young swimmers find it hard to have the upper body strength to do it consistently. As the kids grow, their strength improves and it becomes less hard.\rOnce again, we see a surge of DQs at age 11, when the kids start swimming 2 laps. As with Breaststroke, kids have to remember to touch the wall with both hands simultaneously and then turn back without doing any flutter or alternate kicks.\nOverall, girls seem to improve more consistently than boys.\n\rSwim Times\rNow, here is what I was most curious about before doing this analysis. Shown below are the distributions of swim times by age, gender and stroke. I have excluded the races in yards to focus only on the races in meters, which is the international standard.\nTo provide a reference point, I have added the current 50 Short Course Meters world records for each stroke and gender. As expected, the distributions of swim times are heavily skewed towards the right. In the plots, I have truncated the outliers exceeding 80 seconds.\nOn average, we see consistent improvement with age across the board. In nearly every plot, we see multi-modal distributions with multiple peaks that separate the best swimmers from merely good and the rest of the pack. As the kids progress to swimming 50 meters, i.e. 2 laps of the 25m length, the distributions during the early years once again become wider with heavier skews and fatter right tails. Not only does the difference in swim speed, but also the time taken to turn contributes to a large variance in swim times.\nThe variance reduces in later years and the distributions tend to become more normal. I reckon this is due to two reasons:\nKids get stronger with age and better with practice.\n\rA survivorship bias comes in play. As we’ve seen in the first plot, the number of competitors decrease with age. Basically the kids who have consistently done well are the ones who continue to swim beyond the early years. There might be some kids who swim just for fun or to hang out with their friends. But chances are, those who cannot compete effectively in any stroke would drop out.\n\r\rNevertheless, we see higher variance in strokes that are more technical, i.e. Backstroke, Breaststroke and Butterfly, as compared to Freestyle.\n\rMean Times\rLooking at the plots of distributions, we can observe a very interesting thing as the kids grow older. Note, all the plots are drawn to the same scale. But the distributions of the boys’ swim times curve more to the left than those of girls. It is evident that on average, boys continue to improve well into the late teens, where as the rate of improvement of girls slows down. Why does that happen?\nLet’s see the mean times to swim 50 Short Course Meters by age.\nWhat’s really interesting is that on average, boys and girls are doing just about the same until age 12. Thereafter, boys start pulling apart. Why? Do teenage boys work harder than teenage girls to improve? Hardly the case. On the contrary, we have seen evidence that girls learn and improve more consistently than boys. So what’s happening here? I am no expert in biology, but this is the about the age where puberty begins. Among other things, boys tend to grow taller and more muscular than girls at this stage. No wonder they start swimming faster than girls!\rThis might be among the few instances where the impact of puberty on both sexes, could be measurably observed.\nIn both genders, the trajectory of improvement is non-linear with age. Kids show dramatic improvement until they reach their teens. Thereafter, the rate of improvement slows down. There’s a physical limit to how fast a human could swim, so we could expect the curves to reach their minima at some point during the 20s - 30s when the swimmers reach the pinnacle of physical strength and performance. When the swimmers are past their prime, we could expect them to start slowing down.\nUSA Swimming has race data for swimmers ranging from ages 5 to 50. I sampled data for 50 Short Course Meters Freestyle races to explore.\nHere are the curves drawn using USA Swimming data:\nThe USA Swimming sample data produces a similar set of curves. The effects of puberty on average swim times are exhibited in this dataset too. Boys and girls have nearly the same average swim times until age 12, but from age 13 onwards, boys start swimming faster than girls.\nAnother interesting observation is that after age 18, the rate of improvement accelerates once again for both male and female swimmers. Both curves show a big shift towards the left. Yet again, survivorship bias appears to come into play. These aren’t recreational swimmers. Rather they are kids who continue to swim in college teams, and then compete in national and international events. Superior coaching and facilities at the top level, probably are big factors contributing to this dramatic improvement.\nJudging by the averages, male swimmers reach their prime in their early 20s and maintain their peak form until their early 30s. In comparison, female swimmers do not hit their prime until their late 20s, and have a much shorter peak span than males.\nThe average times start deteriorating by the mid 30s for both males and females. Unfortunately, the data is very sparse for swimmers 35 years and older. To reduce noise, I excluded the data for every age where there are fewer than 10 records. So it’s hard to see the trajectory well into the 40s.\nHere are plots of all swim times in this dataset with best fitting curves for both genders:\n\rConclusion\rFrom this study, we gain some interesting insights into the world of competitive swimming from the early years:\n\rMost kids join swim teams between 6 and 8. More and more kids drop out during the teenage years.\n\rAfter Freestyle, kids learn to swim Backstroke before they learn Breaststroke and Butterfly. Butterfly remains a challenging stroke to learn well into the teens.\n\rPersistence pays off and kids who don’t give up improve by leaps and bounds from 6 to 10.\n\rBoys and girls improve at the same rate until the onset of puberty. Then boys start gaining a big advantage.\n\rOn average, boys reach peak performance in the early 20s itself. On the flip side, they require all the hard work just to maintain it until the 30s. Girls reach peak performance only by the late 20s. On the brighter side, this is a big motivation to continue working hard well into adulthood.\n\r\rThe R markdown file with code for this post is available here.\n\r","date":1538265600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538265600,"objectID":"34e40b5ce20b07a5ce2390e66abfd072","permalink":"/post/a-deep-dive-in-analyzing-swimming-data/","publishdate":"2018-09-30T00:00:00Z","relpermalink":"/post/a-deep-dive-in-analyzing-swimming-data/","section":"post","summary":"This past summer, my daughter joined a swim team. Her team is among several teams that are part of an area league. All summer long, developmental and competitive meets are organized where teams compete on a one-to-one basis. All kids in a team swim in developmental meets. For competitive meets, top 3 or 4 kids are chosen by age groups. Meets are organized in community pools that are either 25 meters or 25 yards in length.","tags":["rstats","dataviz","swimming"],"title":"A Deep Dive in Analyzing Swimming Data","type":"post"},{"authors":null,"categories":["betting"],"content":"\rRecently I listened to a podcast featuring Victor Haghani of Elm Partners, who described a fascinating coin-flipping experiment. The experiment was designed to be played for 30 minutes by participants in groups of 2-15 in university classrooms or office conference rooms, without consulting each other or the internet or other resources. To conduct the experiment, a custom web app was built for placing bets on a simulated coin with a 60% chance of coming up heads. Participants used their personal laptops or work computers to play. They were offered a stake of \\(\\$25\\) to begin, with a promise of a check worth the final balance (subject to a caveat) in their game account. No out-of-pocket betting or any gimmicks. But they had to agree to remain in the room for 30 minutes.\nEven though the experiment in no longer active, the game URL is still active. Before reading any further, I strongly encourage readers to play the game and decide how they would bet and what outcomes they should expect? SPOILERS AHEAD\nWhat would be an optimal betting strategy?\rThe details of the experiment are described in a paper that would be very accessible to anyone with a rudimentary understanding of probability.\nThe probability of heads is reported to be 60%, which means that if the game’s virtual coin is flipped sufficiently large number of times, the number of heads should converge to 60% of the total outcomes. Since each flip is independent of all prior flips, there is a positive expectancy in betting on heads only. The expected gain in betting an amount \\(x\\) on heads is \\(0.6x - 0.4x = 0.2x\\)\nBut how should one bet? Intuitively it is apparent that betting too large would be counter-productive because if we lose, the balance would deplete quickly. A string of consecutive losses would quickly lead to bankruptcy. On the other hand, betting too little isn’t going to be very productive either, even though we know the odds of betting on heads are favorable.\nSo, somewhere along the spectrum between betting too little or too large, there is an optimal betting strategy. In 1955 John Kelly working at Bell Labs published a formula that showed how to maximize wealth while betting on games with favorable odds. Even though this formula has been known to professional gamblers for decades, to my knowledge, it is not a part of any high school or undergraduate curriculum on probability and statistics. I know this personally, having a background in science and engineering.\nWhen the Kelly formula is applied to a game like this with binary (heads/tails) outcomes, it suggests betting a constant fraction of the existing balance, denoted by \\(2*p - 1\\), where p is the probability of the favorable outcome. Clearly, the outcome would be favorable only if p \u0026gt; 0.5.\nGiven that the probability of heads in this game is 0.6, the optimal bet size is \\(2*0.6-1 = 0.2\\) or 20% of the existing balance at each flip. This optimality could be demonstrated by simulation.\nIn the original experiment, 61 participants flipped virtual coins 7253 times. So during the course of a 30 min game,\ra virtual coin was flipped ~ 120 times on average.\nI generated a sample set of 1000 games. In each game, a virtual coin is flipped 120 times with a 0.6 probability of getting heads.\nAssuming there are no constraints in the game and the upside is unbounded, the boxplots show the range of outcomes resulting from a constant percentage bet on heads on each flip.\nFew things stand out from this plot:\n\rSince the lowest balance could only be 0 and the maximum is unbounded, the median provides a better measure of the central tendency of the betting outcomes. The mean is highly skewed by extreme outcomes on the upside. The median provides a balance between those who got extremely lucky and those who weren’t so much. The median balance is the highest while maintaining a constant 20% bet, which is exactly what the Kelly formula suggests.\n\rIf you played the game and didn’t discover the maximum payout, it should be very evident looking at this plot that the game can’t be offered practically without an upper limit. In this sample, the final balance reaches ~ \\(\\$55\\) million in a game with 120 flips betting 50% on heads, even though this would be far from an ideal betting strategy. In theory, if someone got extremely lucky they could flip 120 heads in succession and bet 100% each time to get a final balance of \\(25*1.2^{120} = \\$79.4\\) billion; although this would be extremely unlikely.\n\rWhile betting 40% of the balance, odds are low that you would make more than the starting balance of \\(\\$25\\)\n\rWhile betting 60% or more of the balance, you are almost guaranteed to end up below the starting balance of \\(\\$25\\)\n\rWhile betting 80% or more of the balance, you are almost guaranteed to go bankrupt\n\r\rThe plots of median final balance and the percentage of games which lead to bankruptcy at each constant proportional bet are shown below, These plots make it clear why the Kelly formula provides the optimal betting strategy. The risk with respect to reward is well-balanced.\nThe Kelly formula assumes log utility of wealth. With a constant proportional bet of 20% with a 60% chance of heads, the expected utility for each flip is:\n\\(0.6*log(1.2) + 0.4*log(0.8) = 0.020136\\), which means that each flip gives a dollar equivalent increase in utility of \\(exp(0.020136)\\) =1.0203401 or ~ 2%\nSo, 120 flips starting at \\(\\$25\\), leads to \\(\\$25*(1.0203401^{120}) = \\$280.1\\)\nAlternatively, in 120 flips, if 60% or 72 were heads, the median outcome could be calculated by \\(\\$25*(1.2^{72})*(0.8^{48}) = \\$280.1\\)\nBoth of these values correspond to the peak of the median final balance determined from simulation.\rThe median values for other constant proportional bets could be calculated in the same manner.\n% set_names(paste0(\"Game_\", 1:n_games)) --\r% --\r% --\r% --\r% select(InitialBalance, NumFlips, ProbHeads, BetFraction, everything()) --\r% select(c(1:4)), --\r% select(starts_with(\"Game\")) %% apply(1, median)) %% --\r% --\r% --\r% --\r% --\r% --\r% --\r(MaxBalance - 1e-12)) %% --\r\rSimulating the game play\rIf you’ve read this far, you understand this game can’t be offered without an upper limit on the final payout. If you played the game and bet wisely or were just plain lucky, you would’ve discovered that the maximum payout is \\(\\$250\\). So the objective changes mid-game from growing to preserving existing balance. Once the maximum payout balance is reached, it is foolhardy to keep betting large sums no matter what betting strategy you have chosen.\nSecondly, the UI design of the app doesn’t have a mechanism to choose a constant percentage bet level. So, if you were following a systematic betting strategy like Kelly, then you would have to do mental calculations to determine the exact dollar amount. While this is not hard to do, I believe most people would just round up or down to the nearest dollar amount. Also it takes less time to input a whole dollar amount than dollars and cents. So even though the minimum bet size in the game is 1 cent, effectively it would be \\(\\$1\\).\nUnder these constraints and assumptions, the plot shows bimodal distributions at all bet levels. During the game, the objective changes to maximizing the chances of ending a game above the maximum payout of \\(\\$250\\). Here we can see that the Kelly formula turned out to be a close approximation to optimal, even though the optimal level is determined only in hindsight. Fewer bankruptcies result at higher bet levels than in unconstrained games. I made the assumption that the participants who got lucky early on betting big, decided to bet just \\(\\$1\\) on all subsequent bets after discovering the maximum payout.\n\rHow did the participants do in the experiment?\rNot very well to say the least. There’s an excellent discussion in the paper, but here are the salient points:\n\rOnly 21% of the participants reached the maximum payout of \\(\\$250\\)\r51% of the participants neither went bust nor reached the maximum payout. Their average final balance was \\(\\$75\\)\r33% of the participants ended up below the starting balance of \\(\\$25\\)\r28% of the participants went bust and received no payout\rAverage payout across all participants was \\(\\$91\\)\rOnly 5 among the 61 participants had heard of the Kelly criterion. Out of those, only 1 managed to barely double his stake while the other broke even after 100 flips\r\r\rBetting Patterns\rBetting patterns were found to be quite erratic.\n\r18 participants bet all-in on one flip\rSome bet too small and then too big\rMany participants adopted a Martingale strategy where the losing bets are doubled up to recover past losses\rSome bet small constant wagers to minimize chances of ruin and end up with a positive balance\r41 participants (67%!!) bet on tails at some point during the game play. 29 of them bet on tails 5 or more times\r\rDespite how terrible some of these betting strategies may have been, hitting the maximum payout limit might have saved some of them from ending up bankrupt. Well, assuming these participants were not foolish enough to keep betting high after knowing they couldn’t get more than \\(\\$250\\).\nLet’s see how some of these betting patterns work in simulation.\nAll-in Betting\r18 out of 61 participants reportedly bet 100% on a single flip. Assuming they continued this betting pattern, majority of these participants would’ve lost everything after their second bet. Those who reached the maximum payout would’ve had to win first 4 flips in succession. Assuming they kept their wits beyond that point, only 13% of such participants would’ve ended up with a balance of \\(\\$250\\) or more.\n\rBetting too small and then too big\rSome participants bet too small and then too big. It appears they were over-cautious at first, but when they built a sizeable balance, they threw caution to the wind. Some form of wealth effect or house money effect came into play.\nTo simulate this strategy, I assumed such participants bet 5% until they accumulated a balance of \\(\\$100\\), after which they started betting 30%. If their balance fell below \\(\\$100\\) again, they got spooked and adopted the 5% conservative bet again.\nThe simulated results are quite interesting. Although none of the participants would’ve been bankrupted (assuming they adopted the ultra safe 5% bets when their balance was lower), only ~ 10% of them would’ve made \\(\\$250\\) or more.\n\rMartingale betting\rMartingale bettors keep doubling up their bets after every loss. For instance, if they bet \\(\\$2\\) and lost, their next bet would be \\(\\$4\\), and the next one \\(\\$8\\) if they lost again. Their idea is to recover the former loss(es) and make profits equal to the original stake. Most often this strategy starts with betting small and doubles up on the previous bet even in successive losses. This idea mostly stems from gambler’s fallacy. So if a Martingale bettor was betting on heads but the coin flipped tails several times in succession, their belief is the next flip would be heads and they’ll recover everything they’ve lost thus far. After all mean reversion should come into play, right?.\nBut mean reversion could take much longer to manifest. The coin has no memory of the past flips. Every flip is independent of the prior flips. Even with a biased coin like in this game, it could very well happen that it flips tails 20 times in succession and then flips heads 30 times in succession. The probability of heads converges to 0.6 only in the long run. After losing 20 bets in succession starting with \\(\\$1\\),\ra Martingale bettor would need to bet ~ $1.05 million to recover past losses.\nTo simulate this betting strategy, I assumed a minimum bet size of \\(\\$1\\) as long as the balance is \\(\\$25\\) or lower i.e. 4% to start. The minimum bet is readjusted proportionally as the balance increases or decreases in multiples of\r\\(\\$25\\). So it goes up to \\(\\$2\\) when the balance is above \\(\\$50\\) and \\(\\$4\\) when the balance is above \\(\\$100\\). The previous losing bet is doubled up in succession.\nThe simulated results show 42% of the bettors going bankrupt with this strategy.\rOnly 38% of the bettors would’ve ended up with \\(\\$250\\) or more.\n\rBetting on tails\rThere were 41 participants who bet on tails at some point during the game. 29 of them bet on tails more than 5 times in the game. 13 of them bet on tails more than 25% of the time. These participants were more likely to make that bet after a string of consecutive heads. As the authors state in the paper “…some combination of the illusion of control, law of small number bias, gambler’s fallacy or hot hand fallacy was at work”.\nLet’s assume, these participants bet on heads, until a string of ‘X’ heads, after which they kept betting on tails until they got one, after which they bet on heads again repeating the same process. What would be the value of ‘X’ for the participants to bet on tails more than 25% of the time?\rWe could get a sense from the simulated game data.\nHeads_Threshold_2 Heads_Threshold_3 Heads_Threshold_4 Heads_Threshold_5 Heads_Threshold_6 0.35 0.21 0.12 0.07 0.04 \rIt turns out most of these participants were expecting tails after a string of 3 or more heads. Assuming they followed this heuristic, even while making the optimal 20% bet, the simulated results show ~ 7% of the participants going bankrupt and only 25% of them ending up with \\(\\$250\\).\n\r\rComparison of Betting Strategies\rThe plot below shows the performance of the betting strategies modelled above, with respect to a game optimal strategy of betting 20% of the balance. Most of them end up with a much higher proportion of participants going bankrupt in the game. No strategy comes anywhere close to the game optimal strategy, when it comes to the number of participants reaching the maximum payout offered in the game.\nIn fact, discovering the maximum payout of \\(\\$250\\) would’ve helped some of the participants employing these wayward betting strategies into adopting more conservative approaches in the latter part of their game. But if the game was designed to payout a fraction of the final balance, some more interesting outcomes might emerge. For instance, the maximum balance in the game could be raised to 2500 and the actual payout could be a tenth of that\r(\\(\\$250\\) still). The evidence from simulated results suggests the outcomes in that case to be much worse.\n\rHow did I play the game?\rTo be sure, I was not a part of the cohort of 61 participants who played this game in person and were paid for participation. The game URL is still active, even though there’s no actual payout offered any more. I found it after listening to the podcast and reading the paper. So, unfortunately I already knew the optimal strategy before trying. In my gameplay, I reached the maximum payout by the 16th minute. I ended up with a balance of \\(\\$270\\), flipping the virtual coin a total of 122 times.\nI did however, get some loved ones to play the game uninformed and then tell me their betting strategies. In a sample of n = 4, they reported everything from Martingale betting, betting 10 cents per flip, finding patterns in flip sequences and even betting on tails because of said patterns. In other words, I got a microcosm of results in the paper.\nI would have bet erratically myself, had I not known the optimal strategy. A smaller proportion than optimal at first and then perhaps too big.\n\rConclusion\rDespite being offered favorable odds, most participants adopted strategies subject to their own biases and ended up with poor to suboptimal outcomes. Having a background in quantitative fields didn’t seem to help most of them. Through simulations we can explore outcomes of some of these biases, but actual gameplay involves much more complexity. Most participants would’ve been swayed by a combination of biases at different points during their gameplay.\nEven though betting a constant proportion is an optimal strategy for this game, and has been known since 1955, there persists a gap in education where even the most quantitatively oriented people like myself have no idea about it. Experiments like this have tremendous educational value and should be part of the curriculum in both high schools and colleges. I firmly agree with the thoughts echoed in the paper.\nIn a broader context, this experiment shares similarities with investing. Often the same biases come into play while investing. Even though the outcomes in investing are continuous in nature, and uncertainities abound, a systematic strategy could spell the difference between attaining financial objectives or not. It shows the importance and benefits of sticking to a well-thought-out systematic plan that is devoid of all biases.\nThe R markdown file for this post is available here.\n\r","date":1501718400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501718400,"objectID":"fd576a6ba6509389bd0c4d66987ebc2d","permalink":"/post/biased-coin-flipping-experiment/","publishdate":"2017-08-03T00:00:00Z","relpermalink":"/post/biased-coin-flipping-experiment/","section":"post","summary":"Recently I listened to a podcast featuring Victor Haghani of Elm Partners, who described a fascinating coin-flipping experiment. The experiment was designed to be played for 30 minutes by participants in groups of 2-15 in university classrooms or office conference rooms, without consulting each other or the internet or other resources. To conduct the experiment, a custom web app was built for placing bets on a simulated coin with a 60% chance of coming up heads.","tags":["rstats","simulations"],"title":"How would you bet? Lessons from a Biased Coin Flipping Experiment","type":"post"},{"authors":null,"categories":["investing"],"content":"\rBy all conventional and even some unconventional measures, the US stock market is trading way beyond historical valuation averages and is closer to all time highs. Passive stock index investors have enjoyed a period of extraordinary gains in one of the longest running bull markets. No other major asset class has come close in the last 7 years. Well diversified portfolios have had lackluster returns while the stock market keeps making new highs.\n\r\rTable 1: Performance by Asset Class\r\r\r\r\rUS Stocks\r\rForeign Stocks\r\rUS Bonds\r\rForeign Bonds\r\rTreasuries\r\rGold\r\rCommodities\r\rREITs\r\r\r\r\r\r7 Years\r\r\r\rMay 2010 - Apr 2017 Annualized Return\r\r12.72%\r\r4.47%\r\r3.36%\r\r0.79%\r\r7.28%\r\r0.66%\r\r-6.91%\r\r11.09%\r\r\r\r5 Years\r\r\r\rMay 2012 - Apr 2017 Annualized Return\r\r13.54%\r\r5.40%\r\r2.23%\r\r-0.61%\r\r3.53%\r\r-5.69%\r\r-12.23%\r\r9.16%\r\r\r\r3 Years\r\r\r\rMay 2014 - Apr 2017 Annualized Return\r\r10.35%\r\r1.09%\r\r2.64%\r\r-3.14%\r\r5.95%\r\r-0.93%\r\r-17.55%\r\r8.82%\r\r\r\r\rBut overstretched valuations in a contentious political climate raise fears of what’s around the corner. Small market gains accumulated gradually over months could be wiped out within days or weeks, when the sentiment turns. So what should investors do?\nMomentum strategies provide an attractive alternative in this scenario. Momentum has been described as a premier market anomaly. Studies have been published for over 2 decades showing why and how well it works. Despite being well publicized in the investment world, momentum strategies have continued to work well over long periods of time.\nSome of the biggest advantages of momentum strategies are:\n\rTheir entry and exit rules could be well-defined and understood\rThey could be implemented using highly liquid, low cost ETFs\rThey are relatively easier to scale and execute\rDuring market downturns, these strategies exhibit lower volatilities and drawdowns as compared to value and passive investing strategies\r\rThat being said, momentum is not a single monolithic strategy as many academics portray it to be. It could be implemented in many different ways. Here I present three momentum strategies that could be combined in simple ways to mitigate overall volatility and drawdowns, while maintaining attractive returns.\nGlobal Equities Momentum Strategy\rThe first of these is Global Equities Momentum (“GEM”), which has been detailed by Gary Antonacci in his fantastic book, Dual Momentum Investing. It is one of the simplest strategies to understand and implement. GEM effectively makes use of zero correlation betweeen stocks and bonds. It utilizes absolute momentum to switch to bonds when stocks show weak performance, and bonds tend to do well. Additionally, it uses relative momentum to switch between domestic and foreign stocks during times of stock market strength.\nHere are the concrete rules, dataset and assumptions to backtest GEM:\nRules : Between domestic and foreign equities, invest in the one which had higher outperformance relative to the risk free rate in the past 12 months. Otherwise invest in bonds.\nDataset : Total returns calculated from daily adjusted closing prices downloaded from Yahoo! Finance\n\rDomestic stocks (SPY, combined with VFINX prior to Feb 1993)\rForeign stocks (CWI, combined with VGTSX prior to Jan 2007)\rBonds (AGG, combined with VBMFX prior to Oct 2003)\rRisk free rate determined by T-bills data downloaded from FRED.\r\rAssumptions : Signals and weights determined at the end of month (t), rebalanced at the end of first business day of next month (t+1), transaction costs 20 bps per trade.\n\r\rTable 2: Comparison of GEM strategy with an equally weighted portfolio of SPY, CWI and AGG\r\r\r\r\rSP500TR\r\rGEM_Antonacci\r\rGEM\r\rEqualWts\r\r\r\r\r\rMay 1997 - Apr 2017 Annualized Return\r\r7.52%\r\r12.66%\r\r11.79%\r\r6.24%\r\r\r\rMay 1997 - Apr 2017 Annualized Std Dev\r\r15.17%\r\r11.93%\r\r11.97%\r\r10.68%\r\r\r\rMay 1997 - Apr 2017 Annualized Sharpe (Rf=2.04%)\r\r0.35\r\r0.87\r\r0.80\r\r0.38\r\r\r\rMay 1997 - Apr 2017 Worst Drawdown\r\r-50.95%\r\r-17.84%\r\r-19.32%\r\r-38.77%\r\r\r\r\rIt is easy to cross-check the backtesting results with simulated results from Gary’s website. I do not expect to match the annualized returns as Gary’s simulations use theoretical indexes, do not apply any fees or transaction costs, and monthly performance numbers on his website are rounded to the first decimal place. But a correlation of monthly returns of 0.99, annualized standard deviations, shape of the equity curves and the close alignment of drawdowns all suggest a close match.\nThe GEM strategy performance shows simple rules could be used for timing entries and exits and work exceedingly well over complete market cycles. GEM trounces the S\u0026amp;P 500 Total Returns Index on both absolute and risk adjusted basis over 2 market cycles in this backtesting period.\nThe results of an equally weighted long only strategy of SPY, CWI and AGG are also shown. Essentially it’s a simplified and slightly aggressive 66.67% stocks and 33.33% bonds portfolio, which gives a sense of the performance of a passive structurally allocated portfolio strategy. As compared to the broader market, it does reduce the volatility but a worst drawdown of ~ 40% is still driven by stocks.\nThe edge provided by simple market timing rules in GEM is evident as compared to an equally weighted long only portfolio of the 3 underlying assets. The drawdowns are much lower as there’s a clear rule for exiting the market when it underperforms relative to the risk free rate. Moreover, there have been very few trades over the past 20 years, and most of the gains have been long term in nature. Hence GEM is quite attractive from a tax standpoint too.\n\rSPY-TLT Universal Investment Strategy\rThis strategy has been detailed on LogicalInvest. It utilizes just 2 asset classes, stocks and long term treasuries, and takes advantage of a slighly inverse correlation between the two. While GEM rotates between stocks and bonds in a 100% risk-on / risk-off manner, this strategy is more nuanced. It rotates between stocks and long term treasuries in a more measured way. It selects between a mix of stocks and treasuries which has performed the best on a risk-adjusted basis, in the recent past.\nHere are the concrete rules, dataset and assumptions to backtest UIS:\nRules : Construct 11 portfolios ranging from 100% SPY, 0% TLT to 0% SPY, 100% TLT, by successively incrementing the weight of TLT by 10%. Determine the returns and volatility of these 11 portfolios in the past 60 days (~ 3 months). Select the portfolio that gives the best risk adjusted returns in this period and apply the same weights to the following month.\nDataset : Total returns calculated from daily adjusted closing prices downloaded from Yahoo! Finance\n\rStocks (SPY, combined with VFINX prior to Feb 1993)\rLong-term Treasuries (TLT, combined with VUSTX prior to Aug 2002).\r\rAssumptions : Weights determined at the end of month (t), rebalanced at the end of first business day of next month (t+1), transaction costs 20 bps per trade.\n\r\rTable 3: Comparison of UIS strategy with an equally weighted portfolio of SPY and TLT\r\r\r\r\rSP500TR\r\rUIS\r\rEqualWts\r\r\r\r\r\rSep 1986 - Apr 2017 Annualized Return\r\r10.01%\r\r11.46%\r\r9.12%\r\r\r\rSep 1986 - Apr 2017 Annualized Std Dev\r\r14.99%\r\r11.22%\r\r8.83%\r\r\r\rSep 1986 - Apr 2017 Annualized Sharpe (Rf=3.23%)\r\r0.44\r\r0.71\r\r0.64\r\r\r\rSep 1986 - Apr 2017 Worst Drawdown\r\r-50.95%\r\r-23.51%\r\r-22.35%\r\r\r\r\rThe performance of both UIS and an equally weighted long only mix of stocks and treasuries show the benefits of diversication between these 2 asset classes. In fact an equally weighted portfolio would’ve done just about as well as the S\u0026amp;P 500 Total Returns Index with far lower volatility and drawdowns. Since the global financial crisis of 2008, UIS has performed exceedingly well, which has led to its outperformance on both absolute as well as risk-adjusted basis.\n\rThree-Way Momentum Strategy\rThis strategy is a more simplified version of a three-way momentum model posted by Meb Faber on his blog, that was originally done by Ned Davis Research. As Meb has suggested, it is conceptually similar to the strategies in his QTAA paper. This strategy is also one of the simplest to understand and implement. It utilizes just 3 asset classes: stocks, bonds and gold and invests equally in all asset classes that show positive momentum.\nHere are the concrete rules, dataset and assumptions to backtest this strategy:\nRules : Invest equally in stocks, bonds and gold as long as they trade above their past 200 days moving average. If none of them trades above their 200 days moving average, then invest in T-bills.\nDataset : Total returns calculated from daily adjusted closing prices downloaded from Yahoo! Finance\n\rStocks (SPY, combined with VFINX prior to Feb 1993)\rBonds (AGG, combined with VBMFX prior to Oct 2003)\rGold (GLD, combined with LBMA/GOLD prior to Nov 2004).\rRisk free rate determined by T-bills data downloaded from FRED.\r\rAssumptions : Signals and weights determined at the end of month (t), rebalanced at the end of first business day of next month (t+1), transaction costs 20 bps per trade.\n\r\rTable 4: Comparison of Three-Way Momentum Strategy with an equally weighted portfolio of SPY, AGG and GLD\r\r\r\r\rSP500TR\r\rThreeWay\r\rEqualWts\r\r\r\r\r\rNov 1987 - Apr 2017 Annualized Return\r\r10.31%\r\r8.60%\r\r7.05%\r\r\r\rNov 1987 - Apr 2017 Annualized Std Dev\r\r14.31%\r\r7.57%\r\r7.22%\r\r\r\rNov 1987 - Apr 2017 Annualized Sharpe (Rf=3.13%)\r\r0.48\r\r0.70\r\r0.52\r\r\r\rNov 1987 - Apr 2017 Worst Drawdown\r\r-50.95%\r\r-13.34%\r\r-18.71%\r\r\r\r\r\rStrategy Correlations\rAlthough GEM works very well on its own, it is unlikely that the other two strategies could be followed on a stand-alone basis. Even the 100% risk-on / risk-off nature of GEM make it difficult to employ it at scale.\nBut let’s look at the correlations of these strategies:\n\r\rTable 5: Correlation of monthly returns (May 1997 to April 2017)\r\r\r\r\rSP500TR\r\rGEM\r\rUIS\r\rThreeWay\r\r\r\r\r\rSP500TR\r\r1.00\r\r0.67\r\r0.26\r\r0.35\r\r\r\rGEM\r\r0.67\r\r1.00\r\r0.31\r\r0.55\r\r\r\rUIS\r\r0.26\r\r0.31\r\r1.00\r\r0.37\r\r\r\rThreeWay\r\r0.35\r\r0.55\r\r0.37\r\r1.00\r\r\r\r\rThe UIS strategy has low correlations with both GEM and the Three-Way strategy. Even GEM and the Three-Way Strategy have a correlation of ~ 0.55. So they could be effectively blended together to form a portfolio of diversified strategies.\nHere are a couple of simple ways to blend these strategies:\n\rStrategy Blends\rIn the last 20 years backtesting period, an equal weight combination of GEM and UIS results in a significant reduction in portfolio volatility without compromising returns. Hence the blended strategy gets a solid boost on risk adjusted returns.\n\r\rTable 6: Comparison of a blended strategy with 50% each of GEM \u0026amp; UIS strategies\r\r\r\r\rSP500TR\r\rGEM\r\rUIS\r\rEqual_Blend\r\r\r\r\r\rMay 1997 - Apr 2017 Annualized Return\r\r7.52%\r\r11.79%\r\r12.10%\r\r12.20%\r\r\r\rMay 1997 - Apr 2017 Annualized Std Dev\r\r15.17%\r\r11.97%\r\r11.15%\r\r9.38%\r\r\r\rMay 1997 - Apr 2017 Annualized Sharpe (Rf=2.04%)\r\r0.35\r\r0.80\r\r0.88\r\r1.06\r\r\r\rMay 1997 - Apr 2017 Worst Drawdown\r\r-50.95%\r\r-19.32%\r\r-17.81%\r\r-11.81%\r\r\r\r\rCombining GEM, UIS and Three-Way strategies on an equal weight basis, leads to even further reduction in portfolio volatility. At 8% it is closer to the lowest volatility of the three strategies. Even though the annualized return of the blended strategy is slightly lower than that of GEM, the risk adjusted return is ~ 40% higher.\n\r\rTable 7: Comparison of a blended strategy with 33.33% each of GEM, UIS \u0026amp; ThreeWay strategies\r\r\r\r\rSP500TR\r\rGEM\r\rUIS\r\rThreeWay\r\rEqual_Blend\r\r\r\r\r\rALL Years\r\r\r\rMay 1997 - Apr 2017 Annualized Return\r\r7.52%\r\r11.79%\r\r12.10%\r\r9.34%\r\r11.32%\r\r\r\rMay 1997 - Apr 2017 Annualized Std Dev\r\r15.17%\r\r11.97%\r\r11.15%\r\r7.72%\r\r8.00%\r\r\r\rMay 1997 - Apr 2017 Annualized Sharpe (Rf=2.04%)\r\r0.35\r\r0.80\r\r0.88\r\r0.93\r\r1.14\r\r\r\rMay 1997 - Apr 2017 Worst Drawdown\r\r-50.95%\r\r-19.32%\r\r-17.81%\r\r-9.39%\r\r-9.46%\r\r\r\r10 Years\r\r\r\rMay 2007 - Apr 2017 Annualized Return\r\r7.00%\r\r6.86%\r\r16.07%\r\r8.39%\r\r10.69%\r\r\r\rMay 2007 - Apr 2017 Annualized Std Dev\r\r15.25%\r\r11.83%\r\r12.56%\r\r8.13%\r\r7.91%\r\r\r\rMay 2007 - Apr 2017 Annualized Sharpe (Rf=0.52%)\r\r0.42\r\r0.53\r\r1.23\r\r0.96\r\r1.28\r\r\r\rMay 2007 - Apr 2017 Worst Drawdown\r\r-50.95%\r\r-19.32%\r\r-14.40%\r\r-9.39%\r\r-9.46%\r\r\r\r5 Years\r\r\r\rMay 2012 - Apr 2017 Annualized Return\r\r13.36%\r\r9.05%\r\r10.24%\r\r5.29%\r\r8.25%\r\r\r\rMay 2012 - Apr 2017 Annualized Std Dev\r\r10.19%\r\r9.00%\r\r8.08%\r\r6.70%\r\r7.07%\r\r\r\rMay 2012 - Apr 2017 Annualized Sharpe (Rf=0.15%)\r\r1.29\r\r0.99\r\r1.25\r\r0.77\r\r1.14\r\r\r\rMay 2012 - Apr 2017 Worst Drawdown\r\r-8.36%\r\r-14.52%\r\r-10.50%\r\r-5.04%\r\r-9.46%\r\r\r\r1 Year\r\r\r\rMay 2016 - Apr 2017 Annualized Return\r\r17.77%\r\r17.77%\r\r17.53%\r\r8.05%\r\r14.43%\r\r\r\rMay 2016 - Apr 2017 Annualized Std Dev\r\r6.05%\r\r6.05%\r\r6.34%\r\r7.32%\r\r5.52%\r\r\r\rMay 2016 - Apr 2017 Annualized Sharpe (Rf=0.44%)\r\r2.85\r\r2.85\r\r2.69\r\r1.04\r\r2.52\r\r\r\rMay 2016 - Apr 2017 Worst Drawdown\r\r-1.73%\r\r-1.73%\r\r-3.02%\r\r-5.04%\r\r-2.44%\r\r\r\r\rHere are the equity curves and the distribution of weights in the blended strategy:\n\rAdvantages\rThere are many advantages of this blended strategy:\n\rWorks well in both bull and bear markets with low volatility\rUses low cost, highly liquid ETFs\rWith just 5 ETFs, it is very easy to implement and execute\rLow cost substitute ETFs and funds exist to employ it at scale\rHighly competitive with more complex strategies\r\r\rLeveraged Blends\rGiven that the blended strategy exhibits very low volatility, the returns could be boosted by using leverage. To get a good sense of leveraged returns, I benchmark the margin rate at the prevailing 1-month USD LIBOR rate + 2%. This is well above the rates offered by brokerages catering to professional investors.\nAt the same level of volatility, the blended strategy would have produced over twice the returns of S\u0026amp;P 500 Total Returns Index, with much lower drawdowns.\n\r\rTable 8: Comparison of leveraged blends\r\r\r\r\rSP500TR\r\rBlend_1X\r\rBlend_1.3X\r\rBlend_1.5X\r\rBlend_2X\r\r\r\r\r\rALL Years\r\r\r\rMay 1997 - Apr 2017 Annualized Return\r\r7.52%\r\r11.32%\r\r13.30%\r\r14.60%\r\r17.80%\r\r\r\rMay 1997 - Apr 2017 Annualized Std Dev\r\r15.17%\r\r8.00%\r\r10.40%\r\r11.99%\r\r15.99%\r\r\r\rMay 1997 - Apr 2017 Annualized Sharpe (Rf=2.04%)\r\r0.35\r\r1.14\r\r1.06\r\r1.03\r\r0.97\r\r\r\rMay 1997 - Apr 2017 Worst Drawdown\r\r-50.95%\r\r-9.46%\r\r-12.48%\r\r-14.46%\r\r-19.30%\r\r\r\r10 Years\r\r\r\rMay 2007 - Apr 2017 Annualized Return\r\r7.00%\r\r10.69%\r\r12.99%\r\r14.52%\r\r18.30%\r\r\r\rMay 2007 - Apr 2017 Annualized Std Dev\r\r15.25%\r\r7.91%\r\r10.29%\r\r11.88%\r\r15.86%\r\r\r\rMay 2007 - Apr 2017 Annualized Sharpe (Rf=0.52%)\r\r0.42\r\r1.28\r\r1.21\r\r1.17\r\r1.12\r\r\r\rMay 2007 - Apr 2017 Worst Drawdown\r\r-50.95%\r\r-9.46%\r\r-12.48%\r\r-14.46%\r\r-19.30%\r\r\r\r5 Years\r\r\r\rMay 2012 - Apr 2017 Annualized Return\r\r13.36%\r\r8.25%\r\r9.98%\r\r11.13%\r\r13.93%\r\r\r\rMay 2012 - Apr 2017 Annualized Std Dev\r\r10.19%\r\r7.07%\r\r9.20%\r\r10.61%\r\r14.14%\r\r\r\rMay 2012 - Apr 2017 Annualized Sharpe (Rf=0.15%)\r\r1.29\r\r1.14\r\r1.07\r\r1.03\r\r0.97\r\r\r\rMay 2012 - Apr 2017 Worst Drawdown\r\r-8.36%\r\r-9.46%\r\r-12.48%\r\r-14.46%\r\r-19.30%\r\r\r\r1 Year\r\r\r\rMay 2016 - Apr 2017 Annualized Return\r\r17.77%\r\r14.43%\r\r18.13%\r\r20.65%\r\r27.08%\r\r\r\rMay 2016 - Apr 2017 Annualized Std Dev\r\r6.05%\r\r5.52%\r\r7.18%\r\r8.28%\r\r11.04%\r\r\r\rMay 2016 - Apr 2017 Annualized Sharpe (Rf=0.44%)\r\r2.85\r\r2.52\r\r2.46\r\r2.43\r\r2.40\r\r\r\rMay 2016 - Apr 2017 Worst Drawdown\r\r-1.73%\r\r-2.44%\r\r-3.35%\r\r-3.96%\r\r-5.47%\r\r\r\r\r\rFurther Ideas\r\rScaling: Create weekly tranches, check signals and rebalance weekly instead of monthly. This way only 25% of the portfolio is exposed to whipsaw risk. Also, the trade sizes are reduced at any given point. I wrote about these ideas in a prior post.\n\rRisk Management: From the weights plot of the blended strategy, it is evident that during some periods, 100% of the portfolio risk is concentrated in SPY. A prudent risk management strategy would limit the concentration on any single asset in a portfolio. So sudden unfavorable movements have less of an impact.\n\rAdd more uncorrelated strategies: Instead of being all-in on momentum only, value strategies could be added to the mix, which exhibit low correlation to momentum strategies.\n\rAlternative Implementation: Implement this strategy using futures products which provide greater control over margins to scale up or down.\n\r\r\rConclusion\rInvestors are either totally passive or too fixated on finding a single unicorn strategy that’ll work well during both good and bad times. The key is to put together a team of sufficiently uncorrelated strategies that work well together to build a robust portfolio.\nDisclaimer\nAll content displayed here is for informational purposes only and is not guaranteed to be accurate, complete or up-to-date. The returns presented are hypothetical and do not represent returns attained by any investor. The content and commentary are intended to provide the views and observations of the author only, and are subject to change at any time without prior notice, and do NOT represent those of past, present or future employers. Nothing herein should be considered investment advice or recommendation to buy, sell or hold any securities.\n\r","date":1498521600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498521600,"objectID":"551125122954628ecd2f7985874e0995","permalink":"/investing/investment-strategy-diversification/","publishdate":"2017-06-27T00:00:00Z","relpermalink":"/investing/investment-strategy-diversification/","section":"investing","summary":"By all conventional and even some unconventional measures, the US stock market is trading way beyond historical valuation averages and is closer to all time highs. Passive stock index investors have enjoyed a period of extraordinary gains in one of the longest running bull markets. No other major asset class has come close in the last 7 years. Well diversified portfolios have had lackluster returns while the stock market keeps making new highs.","tags":["rfinance","models","momentum"],"title":"Investment Strategy Diversification","type":"investing"},{"authors":null,"categories":["investing"],"content":"\rAn important and often overlooked topic was raised by Corey Hoffstein at NewFound Research. Here are his first couple of tweets on that topic:\n\r\r\r\r\rIndeed, this is about rebalance timing and how little attention it gets. Within the construct of a systematic strategy, this is a part of the Execution Model.\n\rThe Black Box Revealed\n\rCourtesy: Inside the Black Box - The Simple Truth about Quantitative Trading by Rishi Narang\n\rUnless it’s an intraday trading model, when it comes to portfolio rebalancing, a common convention is to rebalance at the beginning of a new month. Perhaps, these are some of the reasons behind this convention:\nIn published literature, a monthly period is commonly used for portfolio rebalancing. Hence it has become a sort of common convention to follow.\rMost of the researchers are adept in using Excel, but less so in programming languages. While doing a backtest in Excel, it is easier to manipulate monthly data and do performance analysis, as opposed doing it with daily data.\rIn some cases, investible products for some asset classes have not existed beyond a couple of decades at best. So longer backtests are often done using theoretical indices which are often available only in monthly format. Hence it forces the choice of a monthly rebalancing cycle. This implies rebalancing at the beginning of a new month.\r\rBut should this be accepted just on the basis of a common convention? If a monthly rebalancing cycle is adopted, does it matter which day of the month is chosen for rebalance? Are there any major differences between portfolios rebalanced on the first business day of a month, as opposed to other days? Are these differences just completely random in nature?\nThese are some of the questions that need to be answered.\nStrategy\rTo answer these questions, I used one of the most basic market timing strategies:\nGo long S\u0026amp;P500 when it trades above its 200 days simple moving average (SMA), otherwise exit and invest in T-Bills.\nThere is nothing magical about using a 200 days SMA. Other periods in the same ballpark work just as well, as shown by Meb Faber in his QTAA paper. I used this simple strategy to illustrate the effect of rebalance timing on portfolio performance.\nI put together a historical daily total returns dataset of SPDR S\u0026amp;P 500 ETF (SPY), spliced together with historical daily total returns data of Vanguard 500 Index fund (VFINX), prior to Feb. 1993. This data was sourced from Yahoo! Finance, prior to their API fiasco earlier this month. Both of these products closely track the S\u0026amp;P500 Total Returns Index. This provides 37+ years of historical daily total returns data going back to the beginning of 1980.\n\rMethodology\rTo test the effect of rebalance timing, I constructed 21 portfolio series, assuming 21 business days in a month. The first portfolio series is rebalanced on the first business day of a month, using the trading signal as of the end of the previous month. In short, signal @ t, rebalance @ t+1. If there are fewer than 21 business days in a month, say for instance 19 days, then portfolio series 20 and 21 are rebalanced on the first business day of the next month. A transaction cost of 10 bps is applied on each trade.\nHere are the results:\nThe impact of luck could be observed in drawdowns as well as returns. The worst drawdown of series 13 is just a little more than half of series 14. Nevertheless, their annualized returns are almost the same. In all the cases, except series 13, the worst drawdown happened after the crash of 1987. Ironically, series 13 is the lucky one here ;). As Corey mentioned in his tweets, for an investment manager this is a difference between getting hired and fired.\nAt the moment, series 1, gives the best return. As we progress to series 9, the returns decrease quite significantly. There is a difference of over 2% in their annualized returns. Here are their performance summaries:\n\r\rTable 1: Best and worst rebalance series\r\r\r\r\rBest\r\rWorst\r\r\r\r\r\rALL Years\r\r\r\rJan 1981 - Apr 2017 Annualized Return\r\r10.68%\r\r8.63%\r\r\r\rJan 1981 - Apr 2017 Annualized Std Dev\r\r11.09%\r\r11.87%\r\r\r\rJan 1981 - Apr 2017 Annualized Sharpe (Rf=4.22%)\r\r0.55\r\r0.35\r\r\r\rJan 1981 - Apr 2017 Worst Drawdown\r\r-23.59%\r\r-26.20%\r\r\r\r\rLet’s take a look at how the best (blue) and the worst (red) series have fared relative to the other 19 series in the entire historical period:\nSo, turns out the current best performed worse than the median for most of the historical period. But since 2003, it really took off and has managed to avoid many whipsaws that appear to have affected other series. But will it continue to do better? The impact of luck is quite obvious here. Quite surprisingly, the worst series has been at or near the worst throughout the historical period. Perhaps a longer backtest or one with a different investment strategy would be useful to investigate this further.\n\rMitigating the impact of luck\rSo how could the impact of luck be mitigated without resorting to more frequent rebalancing, and thus incurring higher portfolio turnover costs?\nHere are a couple of ideas:\nCreate four weekly portfolio series staggered by a week, where each series is rebalanced at the interval of 4 weeks. This way, at any given time, only ~ 25% of the capital is exposed to whipsaw risk. But a practical challenge here is to ensure that each series remains balanced relative to others. Say for instance, no single series should have a weight greater than a third of the entire portfolio. This could be managed by leveraging / deleveraging at rebalance time.\n\rStick with rebalancing at the beginning of a new month (t+1). But instead of taking the signal at the last day of the prior month (t), take the median (or mean) of the daily signals in the prior month. This is similar to Corey’s idea of signal smoothing with 21 days SMA of signals.\n\r\rLet’s see how these methods compare to the best and worst portfolios:\nThe three methods with overlapping portfolios and signal smoothing are denoted by Weekly_avg, Signal_median, Signal_avg. We can observe that all of these methods have similar performance profiles in the historical period, with minor differences in shorter time periods. They perform much better than the Worst portfolio series, in all time periods. And during most of the historical period, they have even performed better than the current Best series.\n\r\rTable 2: Comparison of rebalance methods\r\r\r\r\rSP500TR\r\rBest\r\rWorst\r\rWeekly_avg\r\rSignal_median\r\rSignal_avg\r\r\r\r\r\rALL Years\r\r\r\rJan 1981 - Apr 2017 Annualized Return\r\r11.07%\r\r10.68%\r\r8.63%\r\r9.99%\r\r10.19%\r\r10.26%\r\r\r\rJan 1981 - Apr 2017 Annualized Std Dev\r\r14.85%\r\r11.09%\r\r11.87%\r\r11.01%\r\r11.45%\r\r11.11%\r\r\r\rJan 1981 - Apr 2017 Annualized Sharpe (Rf=4.22%)\r\r0.44\r\r0.55\r\r0.35\r\r0.50\r\r0.49\r\r0.51\r\r\r\rJan 1981 - Apr 2017 Worst Drawdown\r\r-50.95%\r\r-23.59%\r\r-26.20%\r\r-21.42%\r\r-26.87%\r\r-26.87%\r\r\r\r20 Years\r\r\r\rMay 1997 - Apr 2017 Annualized Return\r\r7.52%\r\r10.18%\r\r7.58%\r\r8.15%\r\r9.07%\r\r9.17%\r\r\r\rMay 1997 - Apr 2017 Annualized Std Dev\r\r15.17%\r\r10.02%\r\r10.98%\r\r10.09%\r\r10.38%\r\r9.98%\r\r\r\rMay 1997 - Apr 2017 Annualized Sharpe (Rf=2.04%)\r\r0.35\r\r0.80\r\r0.49\r\r0.59\r\r0.66\r\r0.70\r\r\r\rMay 1997 - Apr 2017 Worst Drawdown\r\r-50.95%\r\r-15.28%\r\r-17.37%\r\r-16.18%\r\r-15.28%\r\r-15.28%\r\r\r\r10 Years\r\r\r\rMay 2007 - Apr 2017 Annualized Return\r\r7.00%\r\r10.02%\r\r6.93%\r\r7.65%\r\r7.50%\r\r8.40%\r\r\r\rMay 2007 - Apr 2017 Annualized Std Dev\r\r15.25%\r\r9.18%\r\r10.62%\r\r9.86%\r\r9.74%\r\r9.41%\r\r\r\rMay 2007 - Apr 2017 Annualized Sharpe (Rf=0.52%)\r\r0.42\r\r1.03\r\r0.60\r\r0.72\r\r0.71\r\r0.83\r\r\r\rMay 2007 - Apr 2017 Worst Drawdown\r\r-50.95%\r\r-10.06%\r\r-17.37%\r\r-16.18%\r\r-14.77%\r\r-12.00%\r\r\r\r5 Years\r\r\r\rMay 2012 - Apr 2017 Annualized Return\r\r13.36%\r\r11.91%\r\r10.08%\r\r10.84%\r\r10.12%\r\r10.58%\r\r\r\rMay 2012 - Apr 2017 Annualized Std Dev\r\r10.19%\r\r8.77%\r\r9.55%\r\r8.82%\r\r9.29%\r\r8.98%\r\r\r\rMay 2012 - Apr 2017 Annualized Sharpe (Rf=0.15%)\r\r1.29\r\r1.34\r\r1.04\r\r1.21\r\r1.07\r\r1.16\r\r\r\rMay 2012 - Apr 2017 Worst Drawdown\r\r-8.36%\r\r-7.64%\r\r-14.78%\r\r-9.88%\r\r-14.77%\r\r-12.00%\r\r\r\r\rAs we can see, relatively simple solutions work well to mitigate the impact of luck in rebalance timing.\nDisclaimer\nAll content displayed here is for informational purposes only and is not guaranteed to be accurate, complete or up-to-date. Nothing herein should be considered investment advice or recommendation to buy, sell or hold any securities.\n\r","date":1496016000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496016000,"objectID":"37be4b78f32a033d5eb6f0d5ea9b4176","permalink":"/investing/luck-in-rebalance-timing/","publishdate":"2017-05-29T00:00:00Z","relpermalink":"/investing/luck-in-rebalance-timing/","section":"investing","summary":"An important and often overlooked topic was raised by Corey Hoffstein at NewFound Research. Here are his first couple of tweets on that topic:\n\r\r\r\r\rIndeed, this is about rebalance timing and how little attention it gets. Within the construct of a systematic strategy, this is a part of the Execution Model.\n\rThe Black Box Revealed\n\rCourtesy: Inside the Black Box - The Simple Truth about Quantitative Trading by Rishi Narang","tags":["rfinance","models","rebalance"],"title":"Luck in Rebalance Timing","type":"investing"},{"authors":null,"categories":["datascience"],"content":"\rAmazon is led by its charismatic founder and CEO, Jeff Bezos, who is widely hailed as one of the top visionaries of this era. At the end of the fiscal year in March, Amazon publishes a letter to shareholders written by Bezos, in which he summarises his thoughts, business and management philosophy. While reading these letters, among other things that strikes is Bezos’ clarity of thought, expressed in a concise and easy to comprehend way. It is a very rare quality possessed by very few business leaders. Few other names that come to mind are Warren Buffett, Charlie Munger, Elon Musk and late Steve Jobs.\nThe first 1997 Letter to Shareholders was published at the conclusion of the first fiscal year of Amazon as a public company. In that letter, Jeff Bezos outlined his vision for Amazon and what kind of company he wanted it to be. Obsessing over customers and offering compelling value, is a core principle he laid out in that letter. Focussing on long term investment decisions, company growth and profitability, is how he envisioned Amazon to thrive. The remarkable thing is that in the last 20+ years, Amazon has executed right along these principles and has become, as of this post, the fourth largest company in the United States by market cap. In the first shareholder letter, Bezos said it is Day 1 for the internet and Amazon. He echoes the same thoughts in the most recent 2016 Letter to Shareholders, published earlier this month. In other words, he believes, Amazon still functions as a startup, though now it’s the biggest one. In every shareholder letter, Bezos appends the first shareholder letter of 1997, which shows how he has stayed true to his vision.\nIn this post, I’ll do a text analysis of Amazon Shareholder Letters from 1997 to 2016.\nThese letters are in pdf format and could be downloaded from Amazon’s investor relations website. I have put them in a zipped file here. The first step is to combine the text and remove the 1997 letter from every subsequent letter, and do some additional cleaning. Since ‘Day 1’ and ‘Day 2’ are often used phrases in the letters, I decided to alter the numbers 1 and 2 to one and two and remove all other numbers from the letters.\nHow does Bezos start these letters?\rWe see that the letters are typically addressed to shareholders/shareowners. In 1998, they were addressed to shareholders, customers and employees. 2016 has been a marked departure in his approach. While the earlier letters were mixed with facts and opinion, the 2016 letter constructs a short narrative of why companies decline. He reminds employees about the pitfalls of ‘Day 2’ and beckons them to remain in ‘Day 1’ mode for the next couple of decades.\n\r\r\r\r\ryear\r\r\rfirst_line\r\r\r\r1\r\r1997\r\rTo our shareholders.\r\r\r\r2\r\r1998\r\rTo our shareholders, customers, and employees.\r\r\r\r3\r\r1999\r\rTo our shareholders.\r\r\r\r4\r\r2000\r\rTo our shareholders.\r\r\r\r5\r\r2001\r\rTo our shareholders.\r\r\r\r6\r\r2002\r\rTo our shareholders.\r\r\r\r7\r\r2003\r\rTo our shareholders.\r\r\r\r8\r\r2004\r\rTo our shareholders.\r\r\r\r9\r\r2005\r\rTo our shareholders.\r\r\r\r10\r\r2006\r\rTo our shareholders.\r\r\r\r11\r\r2007\r\rTo our shareowners.\r\r\r\r12\r\r2008\r\rTo our shareowners.\r\r\r\r13\r\r2009\r\rTo our shareowners.\r\r\r\r14\r\r2010\r\rTo our shareowners.\r\r\r\r15\r\r2011\r\rTo our shareowners.\r\r\r\r16\r\r2012\r\rTo our shareowners.\r\r\r\r17\r\r2013\r\rTo our shareowners.\r\r\r\r18\r\r2014\r\rTo our shareowners.\r\r\r\r19\r\r2015\r\rTo our shareowners.\r\r\r\r20\r\r2016\r\r“Jeff, what does Day two look like?”\r\r\r\r\r\n\rWords per Letter\rWe can see that Bezos doesn’t write very long letters. The mean word count is around 1700 words. But from 2013-2015, his letters were more than twice as long.\n\rTop Words\rAfter excluding the commonly used ‘stop words’, a wordcloud of top 100 words is shown below. A remarkable thing to note is how much more customers are emphasized in the letters. Besides the company name, other terms that gain prominence are business, sales, service, experience, cash, and shareholders. Also note the frequent mention of words related to some products and services - Kindle, Prime, AWS, Marketplace, Fulfillment. Bezos has been talking about ‘Day 1’ right from the first to the latest letter, hence we find both day and one in the mix as well.\n\rWords Commonly Associated with ‘customers’\rI tokenized the text into bigrams (sequence of two words occuring together) and excluded all instances containing commonly used ‘stop words’. Since we know customer or customers are the most common words, let’s examine which words appear the most with them.\nThe arrows point towards the second word in the bigram. And n denotes the frequency of occurance. We can see that ‘customer experience’ has the strongest focus. It has been mentioned 47 times in all the letters, followed by ‘customer service’ and ‘customer satisfaction’.\n\rDominant Themes\rFor each bigram and year pair, I calculate its tf-idf statistic, which measures how important a bigram is to a document in a collection of documents.\nPlotting the top 10 terms by this statistic every year, reveals what was on Bezos’ mind every year that was not so common with all other years.\nIn most of the cases, the top 10 tf-idf terms neatly summarise the important points mentioned in the letters. What’s really interesting is, we could get a sense of almost all major products and services offered by Amazon and when they were introduced or became popular. Beginning in 1998 with the launch of music and video stores (it means CDs and DVDs for you millennials out there :), to Prime Instant Video in 2013 and so many more. 2004 was all about explaining why free cash flow is important than earnings. 2010 was focussed on advancing machine learning and building advanced technology capabilities.\rThe latest one from 2016 pertains to the Day 1 and Day 2 narrative and the quick decision making philosophy at a juggernaut like Amazon.\n\rNetwork of Common Bigrams\rThese are bigrams that appear in 5 or more letters. In a nutshell. they show what has been most important to Amazon.\n\rSentiment Analysis\rFor computing sentiment scores, I use 2 lexicons from the tidytext package:\n\rbing : A general purpose lexicon created by Bing Liu et. al., containing a dictionary of words classified into 2 categories, positive and negative.\n\rloughran : Developed by Tim Loughran and Bill McDonald based on analyses of financial reports, it contains a dictionary of words classified into 6 categories - constraining, litiguous, negative, positive, superflous, uncertainity. (As of this writing, this lexicon is available only in the dev version of tidytext)\n\r\rI restrict the usage to only positive and negative sentiment words from the loughran lexicon. Words not found in these lexicons are classified as neutral.\nLet’s look at the top 10 most common words which drive the positive and negative sentiment in these letters.\nThe bing lexicon treats free and fulfillment as positive words, and cloud as a negative word. But these are mostly used in business terms by Amazon. free is used in the context of ‘free super saver shipping’ or ‘free shipping’, fulfillment is used in terms of ‘fulfillment center(s)’ and cloud in terms of ‘cloud service(s)’. None of these could be interpretted as positive or negative so it is best to remove these terms from our word dictionary before calculating the net sentiment scores.\nI define a net sentiment score as the sum of all positive terms, less the sum of all negative terms, divided by the sum of all positive, negative and neutral terms. The plot below shows the net sentiment scores by year, alongside the annual returns of Amazon stock. Amazon stock had a return of 966% in 1998, which distorts its plot, so I limited the annual returns plot to within 200% range.\nWe can observe the sentiment in these letters has remained quite positive, even in the wake of Amazon stock dropping more than 80% after the dot-com bubble bust. Judging from the sentiment scores of these letters, Bezos doesn’t seem to be swayed by Amazon stock performance at all. The annual returns have almost 0 correlation to the yearly sentiment scores.\nThe R markdown file for this post is available here.\n\r","date":1493337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493337600,"objectID":"ecd41c9a2cc7f6dec71944c7a7ec8929","permalink":"/post/text-analysis-of-amazon-shareholder-letters/","publishdate":"2017-04-28T00:00:00Z","relpermalink":"/post/text-analysis-of-amazon-shareholder-letters/","section":"post","summary":"Amazon is led by its charismatic founder and CEO, Jeff Bezos, who is widely hailed as one of the top visionaries of this era. At the end of the fiscal year in March, Amazon publishes a letter to shareholders written by Bezos, in which he summarises his thoughts, business and management philosophy. While reading these letters, among other things that strikes is Bezos’ clarity of thought, expressed in a concise and easy to comprehend way.","tags":["rstats","dataviz","tidytext"],"title":"Text Analysis of Amazon Shareholder Letters","type":"post"},{"authors":null,"categories":null,"content":"","date":1493251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493251200,"objectID":"0b40f44c62afde4dd98d0289a79bbe72","permalink":"/project/historical-stock-performance/","publishdate":"2017-04-27T00:00:00Z","relpermalink":"/project/historical-stock-performance/","section":"project","summary":"A dashboard to see a current snapshot and compare the historical performance of stocks and ETFs.","tags":["Shiny Apps"],"title":"Historical Performance of Stocks","type":"project"},{"authors":null,"categories":null,"content":"","date":1493251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493251200,"objectID":"a957d6c7c5eacfe248da03a5f7d08d04","permalink":"/project/predictive-typing/","publishdate":"2017-04-27T00:00:00Z","relpermalink":"/project/predictive-typing/","section":"project","summary":"Demonstrates a predictive typing engine built using ngram with backoff prediction model. The app suggests top 3 words to use next, as the user types.","tags":["Shiny Apps"],"title":"Predictive Typing Shiny App","type":"project"},{"authors":null,"categories":["datascience"],"content":"\rEveryone using a smartphone or a mobile device has used an onscreen smart keyboard that tries to predict the next set of words that the user might want to type. Typically, upto 3 words are predicted, which are displayed in a row at the top of the keyboard. Given that typing on a glass pane without tactile feedback, could be very frustrating at times, the smart keyboard goes a long way in alleviating these issues.\nBut, how does predictive typing work? Basically, it’s a predictive typing model built using an ngram with backoff prediction model. Makes complete sense, right? Well, it’s less complicated than it sounds, and actually not hard to understand, as we shall see further.\nCorpus\rTo begin, we need a large corpus of text with a wide-ranging vocabulary. The first name that comes to mind is that of William Shakespeare. Shakespeare wrote a number of plays and poems that are classics in the English language. These plays and poems are available in the public domain and are easily accessible through a number of sources, (see here and here).\rBy some accounts, Shakespeare’s vocabulary ranged from 15,000-30,000 words, which makes it excellent to use it for our corpus.\nDavid Robinson’s gutenbergr package makes it easy to download ‘The Complete Works Of Shakespeare’, from Project Gutenberg’s website. The downloaded dataset is a data frame with text appearing in rows along with the gutenberg id.\ndfRaw \u0026lt;- gutenberg_download(100)\rknitr::kable(head(dfRaw, 12))\r\r\rgutenberg_id\rtext\r\r\r\r100\rShakespeare\r\r100\r\r\r100\rThis Etext has certain copyright implications you should read!\r\r100\r\r\r100\r\u0026lt;\u0026lt;THIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM\r\r100\rSHAKESPEARE IS COPYRIGHT 1990-1993 BY WORLD LIBRARY, INC., AND IS\r\r100\rPROVIDED BY PROJECT GUTENBERG ETEXT OF ILLINOIS BENEDICTINE COLLEGE\r\r100\rWITH PERMISSION. ELECTRONIC AND MACHINE READABLE COPIES MAY BE\r\r100\rDISTRIBUTED SO LONG AS SUCH COPIES (1) ARE FOR YOUR OR OTHERS\r\r100\rPERSONAL USE ONLY, AND (2) ARE NOT DISTRIBUTED OR USED\r\r100\rCOMMERCIALLY. PROHIBITED COMMERCIAL DISTRIBUTION INCLUDES BY ANY\r\r100\rSERVICE THAT CHARGES FOR DOWNLOAD TIME OR FOR MEMBERSHIP.\u0026gt;\u0026gt;\r\r\r\rknitr::kable(dfRaw[sample(nrow(dfRaw), 6),])\r\r\rgutenberg_id\rtext\r\r\r\r100\rAssure yourselves, will never be unkind.\r\r100\r\r\r100\rLike one besotted on your sweet delights.\r\r100\rAnd spurn in pieces posts of adamant;\r\r100\rWhereto, when they shall know what men are rich,\r\r100\rTITUS. Marcus, even thou hast struck upon my crest,\r\r\r\r\rData Preparation\rThe copyright notice appears multiple times throughout the text. Fortunately, it doesn’t prevent us from using this data for non-commercial purposes. The first step is to merge the entire text and remove the copyright notices that are helpfully within \u0026lt;\u0026lt; … \u0026gt;\u0026gt;. Secondly, in plays, the characters and their relationships and titles are defined with ‘DRAMATIS PERSONAE’. These lines have to be removed too.\nAfter doing some quick cleaning, the text is reconverted to a data frame.\ncomplete_works \u0026lt;- str_c(dfRaw$text, collapse = \u0026quot;.\u0026quot;)\rcomplete_works \u0026lt;- str_split(complete_works, \u0026quot;\u0026lt;\u0026lt;[^\u0026gt;]*\u0026gt;\u0026gt;\u0026quot;)[[1]]\rcomplete_works \u0026lt;- complete_works[!(str_detect(complete_works, \u0026quot;Dramatis Personae|DRAMATIS PERSONAE\u0026quot;))]\rcomplete_works \u0026lt;- str_replace_all(complete_works, \u0026quot;\\\\[|\\\\]\u0026quot;, \u0026quot;.\u0026quot;)\rdfContent \u0026lt;- as_tibble(complete_works) %\u0026gt;% slice(3:n())\rrm(complete_works)\rFinally, we parse the content into sentences. The tidytext package by David Robinson and Julia Silge, is most useful in doing this within the tidyverse framework.\nHere’s what the function does below:\n\rReplaces all ‘logical’ sentence break delimiters by a period for easier tokenization by sentence\rRemoves extra periods and spaces\rTokenizes into sentences\rRemoves all sentences containing ACT or SCENE\rRemoves all digits from sentences\rFinally adds line numbers to each row (more on the importance of this later…)\r\rA sample of 10 sentences is shown below.\ngetSentences \u0026lt;- function(dfContent) {\rdf \u0026lt;- dfContent %\u0026gt;% mutate(value = str_replace_all(value, \u0026quot;;|:|!|\\\\?| \\\\-|\\\\- | \\\\- \u0026quot;, \u0026quot;\\\\. \u0026quot;)) %\u0026gt;% mutate(value = str_replace_all(value, \u0026quot;[\\\\.]+\u0026quot;, \u0026quot;\\\\.\u0026quot;)) %\u0026gt;% mutate(value = str_replace_all(value, \u0026quot;[[:space:]]+\u0026quot;, \u0026quot; \u0026quot;)) %\u0026gt;% unnest_tokens(sentence, value, token = \u0026quot;sentences\u0026quot;, to_lower = FALSE) %\u0026gt;% dplyr::filter(!str_detect(sentence, \u0026quot;Act|ACT|Scene|SCENE\u0026quot;)) %\u0026gt;% mutate(sentence = str_replace_all(sentence, \u0026quot;[0-9]\u0026quot;,\u0026quot;\u0026quot;)) %\u0026gt;% mutate(lineNumber = row_number()) %\u0026gt;% select(lineNumber, everything())\r}\rdfSentences \u0026lt;- getSentences(dfContent)\rknitr::kable(dfSentences[sample(nrow(dfSentences), 10),])\r\r\rlineNumber\rsentence\r\r\r\r55991\rThat with the King here resteth in his tent. .\r\r16097\rYou are a merry man, sir. fare you well.\r\r9717\rHe did ask favour.\r\r100749\rTo the health of our general. .\r\r40908\rYou are, I think, assur’d I love you not.\r\r117828\rLaurence. and another Watchman.\r\r19243\rEre now denied the asker, and now again,.\r\r145024\rI swear to do this, though a present death.\r\r70942\rStocking his messenger.\r\r122957\rFor bringing wood in slowly.\r\r\r\r\rCreating ngrams\r“ngrams are a contiguous sequence of n items from a given sequence of text…” - Wikipedia\nIn this case, ngrams would be a contiguous sequence of n words. In the simplest form, an ngram of size 1 is a single word.\rAn ngram of size 2 or more is a set of words that appear one after the other in a sequence.\nFor instance, given a sentence - how are you doing?\n\rThere are 4 Unigrams: how, are, you, doing\rThere are 3 Bigrams: how are, are you, you doing\rThere are 2 Trigrams: how are you, are you doing\rThere is 1 Quadrigram: how are you doing\r\rSo, using the data frame of clean sentences that we parsed, the next step is to create ngrams.\rOnce again this is made simple using the unnest_tokens function from the tidytext package. And this is where grouping by line numbers is important because we want ngrams to be created by sequence of words, only within the same line. We do not want ngrams connected by a sentence break.\ngetNgrams \u0026lt;- function(dfSentences, n) {\rdfNgrams \u0026lt;- dfSentences %\u0026gt;% group_by(lineNumber) %\u0026gt;% unnest_tokens(word, sentence, token = \u0026quot;ngrams\u0026quot;, n = n) %\u0026gt;% ungroup() %\u0026gt;% count(word, sort = TRUE) %\u0026gt;% rename(freq = n)\r}\rAll Words\rdf1grams \u0026lt;- getNgrams(dfSentences, 1)\rdim(df1grams)\r[1] 26453 2\rwordsOfFreq1 \u0026lt;- df1grams %\u0026gt;% group_by(freq) %\u0026gt;% summarise(n = sum(freq)) %\u0026gt;% dplyr::filter(freq == 1) %\u0026gt;% .$n\rThere are 26453 words in the corpus, that shows how vast Shakespeare’s vocabulary was. Let’s see a wordcloud of top 100 words. No surprise, the most commonly used words are: the, and, i, to, of. Excluding these commonly used ‘stop words’, in the second wordcloud, we can see the words Shakespeare used most frequently. We can see a lot of references to royalty and nobility, present specially in Shakespearean tragedies. Some names are mentioned quite frequently - Richard, John, Caesar, Brutus and Henry. Some English counties or shires are mentioned quite often - Warwick, York and Gloucester.\n\rUnigrams\rLet’s see a plot of the distribution of words in corpus by usage.\nHere’s a strategy for building ngram tables:\n\rThere is a long tail of 10522 words in the corpus that appear exactly once. Replace all words appearing just once, by UNK as a placeholder for a word not known from the corpus. There are a couple of motivations behind this:\r\rIt reduces the size of ngram tables.\rMore importantly, when an unknown word is encountered, the model knows how to tackle it before predicting the next set of words.\r\rRe-summarise and rearrange the table by frequency.\rAssign an index to each word.\rConvert to data.table format. Some of the biggest benefits of this approach are:\r\rRows could be assigned and referenced by a key. This makes it extremely simple to look up a row value.\rLookup by a key is very fast compared to matching character strings.\rWhen we create tables for bigrams and higher order ngrams, we can replace the words by their corresponding integer indexes. It saves a lot of memory as compared to creating tibbles with character strings.\r\r\rvTopUnigrams \u0026lt;- df1grams %\u0026gt;% dplyr::filter(freq \u0026gt;= 2) %\u0026gt;% .$word\rdf1grams \u0026lt;- df1grams %\u0026gt;% mutate(word = ifelse(word %in% vTopUnigrams, word, \u0026quot;UNK\u0026quot;)) %\u0026gt;% group_by(word) %\u0026gt;% summarise(freq = sum(freq)) %\u0026gt;% arrange(desc(freq)) %\u0026gt;% mutate(index1 = row_number()) %\u0026gt;% select(index1, freq, word)\rdt1grams \u0026lt;- as.data.table(df1grams)\r# set key to word\rdt1grams \u0026lt;- dt1grams[,.(index1,freq),key=word]\r# create another table with key=index1\rdt1gramsByIndex \u0026lt;- dt1grams[,.(word,freq),key=index1]\r# print dimensions\rdim(dt1grams)\r[1] 15932 3\rHere are the top 10 unigrams. UNK replaces the long tail of words appearing once.\rFinally, we are left with 15932 unigrams in our table.\n\r\r\r\r\rindex1\r\r\rword\r\r\rfreq\r\r\r\r1\r\r1\r\rthe\r\r26821\r\r\r\r2\r\r2\r\rand\r\r25670\r\r\r\r3\r\r3\r\ri\r\r20473\r\r\r\r4\r\r4\r\rto\r\r19377\r\r\r\r5\r\r5\r\rof\r\r16954\r\r\r\r6\r\r6\r\ra\r\r14351\r\r\r\r7\r\r7\r\ryou\r\r13568\r\r\r\r8\r\r8\r\rmy\r\r12449\r\r\r\r9\r\r9\r\rin\r\r10881\r\r\r\r10\r\r10\r\rthat\r\r10869\r\r\r\r\r\n\rBigrams\rA data table of bigrams is created where the index of the first word is the lookup key. The second word cannot be UNK since it is a word used for making predictions from the bigram table.\ndf2grams \u0026lt;- getNgrams(dfSentences, 2) %\u0026gt;% separate(word, c(\u0026quot;word1\u0026quot;, \u0026quot;word2\u0026quot;), sep = \u0026quot; \u0026quot;) %\u0026gt;% mutate(word1 = ifelse(word1 %in% vTopUnigrams, word1, \u0026quot;UNK\u0026quot;)) %\u0026gt;% dplyr::filter(word2 %in% vTopUnigrams) %\u0026gt;% group_by(word1, word2) %\u0026gt;% summarise(freq = sum(freq)) %\u0026gt;% arrange(desc(freq))\rdt2grams \u0026lt;- as.data.table(df2grams)\rdt2grams$index1 \u0026lt;- dt1grams[dt2grams$word1]$index1\rdt2grams$index2 \u0026lt;- dt1grams[dt2grams$word2]$index1\rdt2grams \u0026lt;- dt2grams[,.(index1,index2,freq)]\rsetkey(dt2grams, index1)\r# print dimensions\rdim(dt2grams)\r[1] 254884 3\rHere are the top 10 bigrams:\n\r\r\r\r\rword1\r\r\rword2\r\r\rfreq\r\r\r\r1\r\r\r\r30512\r\r\r\r2\r\ri\r\ram\r\r1846\r\r\r\r3\r\rmy\r\rlord\r\r1640\r\r\r\r4\r\rin\r\rthe\r\r1611\r\r\r\r5\r\ri\r\rhave\r\r1603\r\r\r\r6\r\ri\r\rwill\r\r1553\r\r\r\r7\r\rof\r\rthe\r\r1417\r\r\r\r8\r\rto\r\rthe\r\r1380\r\r\r\r9\r\rit\r\ris\r\r1066\r\r\r\r10\r\rto\r\rbe\r\r973\r\r\r\r\r\n\rTrigrams\rSimilarly, a data table of trigrams is created where indexes of the first and second words form the lookup key.\rThe third word cannot be UNK since it is a word used for making predictions from the trigram table.\ndf3grams \u0026lt;- getNgrams(dfSentences, 3) %\u0026gt;% separate(word, c(\u0026quot;word1\u0026quot;, \u0026quot;word2\u0026quot;, \u0026quot;word3\u0026quot;), sep = \u0026quot; \u0026quot;) %\u0026gt;% mutate(word1 = ifelse(word1 %in% vTopUnigrams, word1, \u0026quot;UNK\u0026quot;)) %\u0026gt;% mutate(word2 = ifelse(word2 %in% vTopUnigrams, word2, \u0026quot;UNK\u0026quot;)) %\u0026gt;% dplyr::filter(word3 %in% vTopUnigrams) %\u0026gt;% group_by(word1, word2, word3) %\u0026gt;% summarise(freq = sum(freq)) %\u0026gt;% arrange(desc(freq))\rdt3grams \u0026lt;- as.data.table(df3grams)\rdt3grams$index1 \u0026lt;- dt1grams[dt3grams$word1]$index1\rdt3grams$index2 \u0026lt;- dt1grams[dt3grams$word2]$index1\rdt3grams$index3 \u0026lt;- dt1grams[dt3grams$word3]$index1\rdt3grams \u0026lt;- dt3grams[,.(index1,index2,index3,freq)]\rsetkey(dt3grams, index1,index2)\r# print dimensions\rdim(dt3grams)\r[1] 476098 4\rFrom the top 10 trigrams, we can get an idea how UNK will be useful in this context. the UNK of is one of the most frequently used word sequences in the trigram table, which implies that if we encounter an unknown word after ‘the’, then ‘of’ is the most likely word to be predicted from the trigram table.\n\r\r\r\r\rword1\r\r\rword2\r\r\rword3\r\r\rfreq\r\r\r\r1\r\r\r\r\r38526\r\r\r\r2\r\ri\r\rpray\r\ryou\r\r238\r\r\r\r3\r\ri\r\rwill\r\rnot\r\r213\r\r\r\r4\r\ri\r\rknow\r\rnot\r\r159\r\r\r\r5\r\ri\r\rdo\r\rnot\r\r157\r\r\r\r6\r\rthe\r\rUNK\r\rof\r\r155\r\r\r\r7\r\rthe\r\rduke\r\rof\r\r141\r\r\r\r8\r\ri\r\ram\r\rnot\r\r139\r\r\r\r9\r\ri\r\ram\r\ra\r\r138\r\r\r\r10\r\ri\r\rwould\r\rnot\r\r127\r\r\r\r\r\n\rQuadrigrams\rHere are the top 10 quadrigrams:\n\r\r\r\r\rword1\r\r\rword2\r\r\rword3\r\r\rword4\r\r\rfreq\r\r\r\r1\r\r\r\r\r\r47569\r\r\r\r2\r\rwith\r\rall\r\rmy\r\rheart\r\r46\r\r\r\r3\r\ri\r\rknow\r\rnot\r\rwhat\r\r38\r\r\r\r4\r\rgive\r\rme\r\ryour\r\rhand\r\r31\r\r\r\r5\r\ri\r\rdo\r\rbeseech\r\ryou\r\r31\r\r\r\r6\r\rgive\r\rme\r\rthy\r\rhand\r\r30\r\r\r\r7\r\rthe\r\rduke\r\rof\r\ryork\r\r29\r\r\r\r8\r\ri\r\rdo\r\rnot\r\rknow\r\r28\r\r\r\r9\r\ri\r\rwould\r\rnot\r\rhave\r\r27\r\r\r\r10\r\ray\r\rmy\r\rgood\r\rlord\r\r25\r\r\r\r\r\n\rPentagrams\rHere are the top 10 pentagrams:\n\r\r\r\r\rword1\r\r\rword2\r\r\rword3\r\r\rword4\r\r\rword5\r\r\rfreq\r\r\r\r1\r\r\r\r\r\r\r60534\r\r\r\r2\r\ri\r\ram\r\rglad\r\rto\r\rsee\r\r15\r\r\r\r3\r\ri\r\rthank\r\ryou\r\rfor\r\ryour\r\r12\r\r\r\r4\r\ri\r\rhad\r\rrather\r\rbe\r\ra\r\r9\r\r\r\r5\r\ram\r\rglad\r\rto\r\rsee\r\ryou\r\r8\r\r\r\r6\r\ras\r\ri\r\ram\r\ra\r\rgentleman\r\r8\r\r\r\r7\r\rfor\r\rmine\r\rown\r\rpart\r\ri\r\r8\r\r\r\r8\r\ri\r\rpray\r\ryou\r\rtell\r\rme\r\r8\r\r\r\r9\r\rknow\r\rnot\r\rwhat\r\rto\r\rsay\r\r8\r\r\r\r10\r\rand\r\rso\r\ri\r\rtake\r\rmy\r\r7\r\r\r\r\r\nThese ngram tables comprise all the information needed for the prediction model.\n\rMemory Usage Comparison\rAs mentioned above, storing words as integer indexes in a data table is far more efficient as compared to a data frame with character strings.\rFor instance, here’s how a pentagram data table looks like that contains just the integer indexes of the words:\n\r\r\r\r\rindex1\r\r\rindex2\r\r\rindex3\r\r\rindex4\r\r\rindex5\r\r\rfreq\r\r\r\r1\r\r1\r\r1\r\r1\r\r9049\r\r5\r\r1\r\r\r\r2\r\r1\r\r1\r\r9049\r\r5\r\r1\r\r1\r\r\r\r3\r\r1\r\r4\r\r2\r\r9384\r\r12918\r\r1\r\r\r\r4\r\r1\r\r4\r\r687\r\r31\r\r4758\r\r1\r\r\r\r5\r\r1\r\r5\r\r31\r\r6421\r\r29\r\r1\r\r\r\r6\r\r1\r\r9\r\r183\r\r2\r\r1\r\r1\r\r\r\r7\r\r1\r\r11\r\r1\r\r46\r\r15\r\r1\r\r\r\r8\r\r1\r\r11\r\r1\r\r57\r\r16\r\r1\r\r\r\r9\r\r1\r\r11\r\r1\r\r1267\r\r11402\r\r1\r\r\r\r10\r\r1\r\r11\r\r1\r\r1403\r\r10842\r\r1\r\r\r\r\r\nIn a comparison of memory needed for all ngram tables, we see a difference of a factor of 5 between the two approaches.\nprint(object.size(df1grams)+object.size(df2grams)+object.size(df3grams)+\robject.size(df4grams)+object.size(df5grams), units = \u0026quot;auto\u0026quot;)\r158 Mb\rprint(object.size(dt1grams)+object.size(dt1gramsByIndex)+object.size(dt2grams)+object.size(dt3grams)+\robject.size(dt4grams)+object.size(dt5grams), units = \u0026quot;auto\u0026quot;)\r30.4 Mb\r\r\rNgram With Backoff Prediction Algorithm\rSo, given the ngram data tables, here’s how the prediction algorithm works:\n\rSanitize the input text as done above\rParse the text into sentences\rDetermine the words in the last sentence of the input text. Use upto the last 4 words from the input text.\rThe word predictions are done starting from the ngram table containing the longest sequence of words, provided the input text contains atleast n-1 words. So, if the last 4 words from the input text match the first 4 words in the pentagram table, then the fifth word is chosen, that has the highest frequency of occurance after those 4 words. If more than one word prediction is desired, then the fifth words with the next highest frequency of occurance are also chosen.\rIf no matches or fewer than desired number of matches are found in the pentagram table, then the algorithm backoffs to the quadrigram table using the last 3 words typed, then to the trigram table using the last 2 words typed, then to the bigram table using the last word typed and finally to the unigram table, until the desired number of word predictions are returned.\r\rLet’s see how this works concretely with some test cases:\nTest Cases\rThe first test is to confirm whether the word predictions from the unigram table, match with what we expect.\rThe top 10 unigram table contains 3 words beginning with the letter ‘t’ - the(1), to(4), that(10). And sure enough, this is what we get from our prediction function.\npredictNextWords(\u0026quot;t\u0026quot;)\r[1] \u0026quot;Predictions from 1grams: the,to,that,this,thou\u0026quot;\r[1] \u0026quot;Top 3 Final Predictions: the,to,that\u0026quot;\rFrom the bigram table, we could see that i am, i have and i will are among the top 10. So, the next test is to confirm these.\npredictNextWords(\u0026quot;i \u0026quot;)\r[1] \u0026quot;Predictions from 2grams: am,have,will,do,would\u0026quot;\r[1] \u0026quot;Top 3 Final Predictions: am,have,will\u0026quot;\rNarrowing down to words that begin with letter ‘h’ gives us:\npredictNextWords(\u0026quot;i h\u0026quot;)\r[1] \u0026quot;Predictions from 2grams: have,had,hope,hear,heard\u0026quot;\r[1] \u0026quot;Top 3 Final Predictions: have,had,hope\u0026quot;\rLet’s predict some trigrams. i am not and i am a are among the top 10 trigrams, so that’s what we expect to see.\npredictNextWords(\u0026quot;i am \u0026quot;)\r[1] \u0026quot;Predictions from 3grams: not,a,sure,glad,the\u0026quot;\r[1] \u0026quot;Top 3 Final Predictions: not,a,sure\u0026quot;\rNow let’s examine the UNK feature. We know that the UNK of is number 5 most common sequence in the trigram table so we should expect to see ‘of’ as the first prediction if the input text has an unknown word after ‘the’. So let’s input some gibberish after ‘the’ to make sure this word doesn’t exist in our vocabulary:\npredictNextWords(\u0026quot;the sjdhsbhbfh \u0026quot;)\r[1] \u0026quot;Predictions from 3grams: of,and,in,that,the\u0026quot;\r[1] \u0026quot;Top 3 Final Predictions: of,and,in\u0026quot;\rNow that the known test cases are working well, let’s see the backoff algorithm in action:\npredictNextWords(\u0026quot;to be or not \u0026quot;)\r[1] \u0026quot;Predictions from 5grams: to\u0026quot;\r[1] \u0026quot;Predictions from 4grams: to\u0026quot;\r[1] \u0026quot;Predictions from 3grams: to,at,i,allow\u0026#39;d,arriv\u0026#39;d\u0026quot;\r[1] \u0026quot;Top 3 Final Predictions: to,at,i\u0026quot;\rpredictNextWords(\u0026quot;to be or not to \u0026quot;)\r[1] \u0026quot;Predictions from 5grams: be\u0026quot;\r[1] \u0026quot;Predictions from 4grams: be,crack\u0026quot;\r[1] \u0026quot;Predictions from 3grams: be,crack,the,have,me,do\u0026quot;\r[1] \u0026quot;Top 3 Final Predictions: be,crack,the\u0026quot;\rpredictNextWords(\u0026quot;be or not to \u0026quot;)\r[1] \u0026quot;Predictions from 5grams: be\u0026quot;\r[1] \u0026quot;Predictions from 4grams: be,crack\u0026quot;\r[1] \u0026quot;Predictions from 3grams: be,crack,the,have,me,do\u0026quot;\r[1] \u0026quot;Top 3 Final Predictions: be,crack,the\u0026quot;\rpredictNextWords(\u0026quot;to be or not to be \u0026quot;)\r[1] \u0026quot;Predictions from 5grams: that\u0026quot;\r[1] \u0026quot;Predictions from 4grams: that,found,gone,endur\u0026#39;d,endured,seen\u0026quot;\r[1] \u0026quot;Top 3 Final Predictions: that,found,gone\u0026quot;\rpredictNextWords(\u0026quot;to be or not to be, that \u0026quot;)\r[1] \u0026quot;Predictions from 5grams: is\u0026quot;\r[1] \u0026quot;Predictions from 4grams: is,which\u0026quot;\r[1] \u0026quot;Predictions from 3grams: is,which,you,he,i,thou,can\u0026quot;\r[1] \u0026quot;Top 3 Final Predictions: is,which,you\u0026quot;\rpredictNextWords(\u0026quot;to be or not to be, that is \u0026quot;)\r[1] \u0026quot;Predictions from 5grams: the\u0026quot;\r[1] \u0026quot;Predictions from 4grams: the\u0026quot;\r[1] \u0026quot;Predictions from 3grams: the,not,my,to,a\u0026quot;\r[1] \u0026quot;Top 3 Final Predictions: the,not,my\u0026quot;\rpredictNextWords(\u0026quot;to be or not to be, that is the \u0026quot;)\r[1] \u0026quot;Predictions from 5grams: question\u0026quot;\r[1] \u0026quot;Predictions from 4grams: question,very,way,humour,best,brief\u0026quot;\r[1] \u0026quot;Top 3 Final Predictions: question,very,way\u0026quot;\rpredictNextWords(\u0026quot;be that is the \u0026quot;)\r[1] \u0026quot;Predictions from 5grams: question\u0026quot;\r[1] \u0026quot;Predictions from 4grams: question,very,way,humour,best,brief\u0026quot;\r[1] \u0026quot;Top 3 Final Predictions: question,very,way\u0026quot;\rUsing one of the most well known speeches from Hamlet, we can see that the algorithm gets the first word prediction from the pentagram table. But since we are looking for the next 3 words, the algorithm backoffs to predictions from quadrigram and trigram tables, until it has atleast 3 words. In this case, the pentagram table gives us the best match that we are looking for. We can also observe that once we have atleast 4 words in our input text, the words prior to those do not matter for the next word predictions.\n\nLet’s see another example:\npredictNextWords(\u0026quot;What a lovely day \u0026quot;)\r[1] \u0026quot;Predictions from 5grams: \u0026quot;\r[1] \u0026quot;Predictions from 4grams: \u0026quot;\r[1] \u0026quot;Predictions from 3grams: \u0026quot;\r[1] \u0026quot;Predictions from 2grams: and,to,is,of,i\u0026quot;\r[1] \u0026quot;Top 3 Final Predictions: and,to,is\u0026quot;\rpredictNextWords(\u0026quot;a lovely day \u0026quot;)\r[1] \u0026quot;Predictions from 4grams: \u0026quot;\r[1] \u0026quot;Predictions from 3grams: \u0026quot;\r[1] \u0026quot;Predictions from 2grams: and,to,is,of,i\u0026quot;\r[1] \u0026quot;Top 3 Final Predictions: and,to,is\u0026quot;\rpredictNextWords(\u0026quot;lovely day \u0026quot;)\r[1] \u0026quot;Predictions from 3grams: \u0026quot;\r[1] \u0026quot;Predictions from 2grams: and,to,is,of,i\u0026quot;\r[1] \u0026quot;Top 3 Final Predictions: and,to,is\u0026quot;\rpredictNextWords(\u0026quot;day \u0026quot;)\r[1] \u0026quot;Predictions from 2grams: and,to,is,of,i\u0026quot;\r[1] \u0026quot;Top 3 Final Predictions: and,to,is\u0026quot;\rHere we see that even though there are enough words to start matching from the higher ngram tables, the matches are gotten only from the bigram table using ‘day’ as the first word.\n\nAnother one:\npredictNextWords(\u0026quot;Beware the ides of \u0026quot;)\r[1] \u0026quot;Predictions from 5grams: march\u0026quot;\r[1] \u0026quot;Predictions from 4grams: march\u0026quot;\r[1] \u0026quot;Predictions from 3grams: march\u0026quot;\r[1] \u0026quot;Predictions from 2grams: march,the,my,his,a,this\u0026quot;\r[1] \u0026quot;Top 3 Final Predictions: march,the,my\u0026quot;\rpredictNextWords(\u0026quot;Beware the ides of mar\u0026quot;)\r[1] \u0026quot;Predictions from 5grams: march\u0026quot;\r[1] \u0026quot;Predictions from 4grams: march\u0026quot;\r[1] \u0026quot;Predictions from 3grams: march\u0026quot;\r[1] \u0026quot;Predictions from 2grams: march,marriage,marcius,marcus,mars\u0026quot;\r[1] \u0026quot;Top 3 Final Predictions: march,marriage,marcius\u0026quot;\rpredictNextWords(\u0026quot;Beware the ides of march\u0026quot;)\r[1] \u0026quot;Predictions from 5grams: march\u0026quot;\r[1] \u0026quot;Predictions from 4grams: march\u0026quot;\r[1] \u0026quot;Predictions from 3grams: march\u0026quot;\r[1] \u0026quot;Predictions from 2grams: march,marching\u0026quot;\r[1] \u0026quot;Predictions from 1grams: march,marching,march\u0026#39;d,marches,marchioness\u0026quot;\r[1] \u0026quot;Top 3 Final Predictions: march,marching,march\u0026#39;d\u0026quot;\r\r\rShiny App\rFinally, here’s a Shiny app that demos this predictive typing model.\nThe R markdown file for this post is available here.\n\r","date":1492214400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1492214400,"objectID":"e9d1bc20169dab5be474f8478a686e1c","permalink":"/post/building-a-predictive-typing-model/","publishdate":"2017-04-15T00:00:00Z","relpermalink":"/post/building-a-predictive-typing-model/","section":"post","summary":"Everyone using a smartphone or a mobile device has used an onscreen smart keyboard that tries to predict the next set of words that the user might want to type. Typically, upto 3 words are predicted, which are displayed in a row at the top of the keyboard. Given that typing on a glass pane without tactile feedback, could be very frustrating at times, the smart keyboard goes a long way in alleviating these issues.","tags":["rstats","shinyapps","tidytext","data.table","models","predictive"],"title":"Building a Predictive Typing Model","type":"post"},{"authors":null,"categories":["datascience"],"content":"\r\rWho’re the best batsmen in cricket today? Is there a way to define a set of objective criteria that provides an unbiased conclusion? In my last post, I selected a list of top batsmen in the history of cricket, by simply filtering all batsmen who have had a career batting average of 50 or more in any format. Prior to that I eliminated newcomers, specialist bowlers and unsuccessful players by another set of filters.\nI got a list of 42 batsmen who have consistently performed well throughout their career. Now the question is how could we select the best of the best who have had the most well-rounded performances. A post on fivethirtyeight has been a strong inspiration to arrive at the answers below.\nBest batsmen at present\rTo select current best batsmen we filter the list of top batsmen to include only the batsmen who are actively playing in all the 3 formats of the game.\nThis leaves us with only 6 batsmen who are currently playing in all the formats. They are AB de Villiers, HM Amla, JE Root, KS Williamson, SPD Smith, V Kohli. No surprises there as these batsmen are widely considered to be the best among the active players today. But instead of subjectively handpicking them, we arrived at this conclusion by applying some reasonable filters.\nBut how well-rounded are these players? To determine this I’ve defined the following criteria and metrics:\n\r\r\r\rCriteria\rMetric\rLabel\r\r\r\rConsistency of scoring runs\rCareer batting average\rSCORING CONSISTENCY\r\rAbility to score quickly\rStrike Rate\rSTRIKE RATE\r\rAbility to score big (number of 100s + number of 50s scored)\rBig scores per innings\rBIG SCORING RATE\r\rOverall contribution to the matches played\rTeam Win-Loss Ratio\rWIN RATIO\r\rDirect contribution to matches won\rBatting average in winning matches\rWIN CONTRIBUTION\r\r\r\rEvery metric defined here is a ratio instead of an absolute number. So it is useful to evaluate relative ability.\nAmong the 42 top batsmen selected before, I calculate percentile scores of these metrics for each batsman grouped by the format of the game. This means we have a score ranging from 0-100 for each metric, where each batsman in the list has been scored relative to the best batsman in each criteria.\nLet’s see the percentile scores for the 6 champion batsmen in our final list.\nPercentile Scores\r\r\r\r\r\rFormat\r\r\rCountry\r\r\rPlayer\r\r\rPercAverage\r\r\rPercStrikeRate\r\r\rPercInningsPerBigScore\r\r\rPercWinRatio\r\r\rPercAverageInWins\r\r\r\r1\r\rTests\r\rSouth Africa\r\rAB de Villiers\r\r20\r\r57\r\r27\r\r61\r\r47\r\r\r\r2\r\rODIs\r\rSouth Africa\r\rAB de Villiers\r\r100\r\r100\r\r86\r\r72\r\r93\r\r\r\r3\r\rT20Is\r\rSouth Africa\r\rAB de Villiers\r\r36\r\r73\r\r31\r\r88\r\r32\r\r\r\r4\r\rTests\r\rSouth Africa\r\rHM Amla\r\r8\r\r40\r\r22\r\r74\r\r30\r\r\r\r5\r\rODIs\r\rSouth Africa\r\rHM Amla\r\r83\r\r90\r\r93\r\r79\r\r83\r\r\r\r6\r\rT20Is\r\rSouth Africa\r\rHM Amla\r\r59\r\r62\r\r62\r\r44\r\r50\r\r\r\r7\r\rTests\r\rEngland\r\rJE Root\r\r54\r\r70\r\r81\r\r20\r\r86\r\r\r\r8\r\rODIs\r\rEngland\r\rJE Root\r\r76\r\r73\r\r89\r\r15\r\r86\r\r\r\r9\r\rT20Is\r\rEngland\r\rJE Root\r\r89\r\r67\r\r70\r\r32\r\r75\r\r\r\r10\r\rTests\r\rNew Zealand\r\rKS Williamson\r\r32\r\r44\r\r76\r\r15\r\r76\r\r\r\r11\r\rODIs\r\rNew Zealand\r\rKS Williamson\r\r69\r\r69\r\r82\r\r29\r\r75\r\r\r\r12\r\rT20Is\r\rNew Zealand\r\rKS Williamson\r\r71\r\r50\r\r77\r\r75\r\r63\r\r\r\r13\r\rTests\r\rAustralia\r\rSPD Smith\r\r98\r\r83\r\r86\r\r37\r\r91\r\r\r\r14\r\rODIs\r\rAustralia\r\rSPD Smith\r\r56\r\r83\r\r52\r\r75\r\r58\r\r\r\r15\r\rT20Is\r\rAustralia\r\rSPD Smith\r\r18\r\r45\r\r8\r\r57\r\r19\r\r\r\r16\r\rTests\r\rIndia\r\rV Kohli\r\r10\r\r79\r\r8\r\r64\r\r18\r\r\r\r17\r\rODIs\r\rIndia\r\rV Kohli\r\r94\r\r97\r\r97\r\r65\r\r97\r\r\r\r18\r\rT20Is\r\rIndia\r\rV Kohli\r\r100\r\r84\r\r93\r\r94\r\r94\r\r\r\rHere we can see how each champion scored relative to the rest of the elite group. AB de Villiers has the best career batting average and strike rate in ODIs so he gets a score of 100 in SCORING CONSISTENCY \u0026amp; STRIKE RATE. All other batsmen are scored relative to AB in these 2 criteria. With his astounding batting average of 99.94 in Tests, Sir Don Bradman gets a score of 100 in SCORING CONSISTENCY in Tests. And being at number 5 in all time highest batting averages in Tests, Steve Smith gets a high score of 91 in this criteria, relative to Sir Don.\nThis is all well and good, but how do we see the forest from trees? It is very difficult to make sense of high dimensional data in a table or in a single bar plot. This is where the visualization technique shown in the fivethirtyeight article shines. We build a wagon wheel with segments denoting each metric and within each segment, the data is binned by quartiles.\nHere’s how a wagon wheel would look like for each batsman. Each segment would be filled depending upon the percentile score in each criteria.\nSo let’s see how the wagon wheels of our champions look like:\nWe see that Steve Smith is the most well-rounded batsman in Tests. His WIN RATIO is lacking relative to the illustrious company he’s in, but overall his performance in Tests is phenomenal. Joe Root comes second but others have a bit more catching up to do.\nIn One Day Internationals, Hashim Amla dominates in all spheres with AB de Villiers and Virat Kohli not far behind. Joe Root seems to contribute in most matches won by England but it appears the English team loses a fair bit in ODIs. Hence the WIN RATIO of Root is a bit lacking.\nVirat Kohli is the undisputed king in the T20 Internationals format with Kane Williamson in second place. Steve Smith’s performance so far leaves much to be desired in this format.\nThe beauty of this visualization technique is that several dimensions in the data are compressed in a single facetted plot, in a visually appealing way. A glance at the plot reveals the relative best, both overall and in each metric.\n\r","date":1487203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487203200,"objectID":"8cf62ef4dd5606507db80e9429e6f539","permalink":"/post/best-batsmen-in-cricket-at-present/","publishdate":"2017-02-16T00:00:00Z","relpermalink":"/post/best-batsmen-in-cricket-at-present/","section":"post","summary":"Who’re the best batsmen in cricket today? Is there a way to define a set of objective criteria that provides an unbiased conclusion? In my last post, I selected a list of top batsmen in the history of cricket, by simply filtering all batsmen who have had a career batting average of 50 or more in any format. Prior to that I eliminated newcomers, specialist bowlers and unsuccessful players by another set of filters.","tags":["rstats","dataviz","cricket"],"title":"Best batsmen in cricket at present","type":"post"},{"authors":null,"categories":["datascience"],"content":"\r\rIn a previous post, I used data from Statsguru and looked at a brief history of cricket with respect to debut years and career spans of players. In this post, I use detailed player statistics from the same dataset to select top batsmen who have played this game.\nDistribution of matches played by all players\rLet’s take a look at the summary statistics of number of matches played by all players in each format of the game.\nNumber of matches played by all players in each format\rFormat: Tests\rMin. 1st Qu. Median Mean 3rd Qu. Max. 1.00 2.00 7.00 17.27 21.00 200.00 --------------------------------------------------------------------------- Format: ODIs\rMin. 1st Qu. Median Mean 3rd Qu. Max. 1.00 4.00 16.00 42.56 53.00 463.00 --------------------------------------------------------------------------- Format: T20Is\rMin. 1st Qu. Median Mean 3rd Qu. Max. 1.0 2.0 7.0 13.9 18.5 98.0 \rThe distribution appears to be heavily skewed towards the right. In simple terms, there are a handful of players who go on to play 100+ Tests and 200+ ODIs. A majority of the players have played very few matches. The mean and median are hardly indicative of players who established themselves in their teams.\n\rDistribution of innings to matches played by all players\rA cricket team has 11 players, out of which 5 or 6 are specialist batsmen, 1 or 2 are all-rounders (could bat and bowl well) and the rest are specialist bowlers. The wicketkeeper also has to be a good batsman. The batsmen play up in the order in a match and get to bat before the bowlers do.\nHere I will introduce another measure to distinguish batsmen from bowlers - ratio of innings played relative to number of matches played. In many matches a bowler doesn’t get a chance to bat, unless the top order batsmen and all-rounders fail, i.e. are out. Hence the ratio of innings to matches of specialist bowlers will be low.\nHere are the summary statistics of this ratio. The ratio is greater than 1 in Tests because each player could play upto 2 innings. The peaks in each plot are indicative of batsmen who have batted in the top 3 and have gotten a chance to bat in nearly every innings of every match they played.\nRatio of innings to matches of all players in each format\rFormat: Tests\rMin. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s 0.250 1.429 1.667 1.623 1.923 2.000 19 --------------------------------------------------------------------------- Format: ODIs\rMin. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s 0.1250 0.5714 0.8421 0.7670 1.0000 1.0000 62 --------------------------------------------------------------------------- Format: T20Is\rMin. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s 0.05556 0.50000 0.85714 0.73084 1.00000 1.00000 73 \r\rDistinguish batsmen from bowlers\rTo identify true batsmen in the dataset, I use 2 filters:\n\rNumber of matches played should be above the 75th percentile\rRatio of innings to matches played should be above the 25th percentile\r\rThe first filter eliminates newcomers and players who have had very short-lived careers. The second filter eliminates specialist bowlers from the dataset.\nLet’s take a look at the distribution of these metrics for these batsmen. These numbers appear more reasonable.\n\rCareer averages of batsmen\rLet’s take a look at the career batting averages of batsmen in each format. We see a few low averaging players left in the dataset who are potentially specialist bowlers. But without resorting to hand-picking, the two empirical filters chosen earlier did a good job of eliminating specialist bowlers from the dataset.\nCareer batting averages of batsmen in each format\rFormat: Tests\rMin. 1st Qu. Median Mean 3rd Qu. Max. 4.84 24.84 33.59 33.11 42.13 99.94 --------------------------------------------------------------------------- Format: ODIs\rMin. 1st Qu. Median Mean 3rd Qu. Max. 6.63 23.11 29.71 29.55 35.88 55.03 --------------------------------------------------------------------------- Format: T20Is\rMin. 1st Qu. Median Mean 3rd Qu. Max. 4.10 18.15 21.91 23.27 28.44 53.40 \r\rTop batsmen\rThe hallmark of a top batsman is consistency in scoring runs. From the distribution of career averages, it could be seen that very few batsmen have managed to score at an average of 50 or more runs in their career. So keeping things simple, this would be my sole criteria for selecting top batsmen amongst the rest. All batsmen who have averaged 50 or more in any format would be included in my list of top batsmen.\nThis gives us a unique list of 42 elite batsmen who are legendary in their achievements.\nLet’s take a look how they compare by various stats.\n\nCareer Span\rThe career span of these elite batsmen is plotted in the order in which they debuted in their international career. Career spans of batsmen who are still actively playing in any format are labeled with blue color.\nSummary of career spans in years of top batsmen\rMin. 1st Qu. Median Mean 3rd Qu. Max. 5.00 11.00 16.00 14.71 18.75 24.00 \rSachin Tendulkar and George Headley have had the longest career spans of 24 years each. But the international career of Headley along with that of almost all of the early era greats were interrupted by world wars. So even though they resumed their careers after the end of the wars, they were able to play relatively few matches (more on this below).\nGraeme Pollock has had the shortest career of 7 years amongst all retired players. Being a South African, his career was cut short when ICC suspended South Africa from competing internationally in 1970 because of apartheid.\n\n\rNumber of matches played\rAmong the many records in Tendulkar’s name, the number of Tests and ODIs played are at the forefront. He played a single T20I, because he, Dravid and other senior members of the then Indian team used to withdraw their names for team selection in T20Is. His records of playing in 200 Tests and 463 ODIs are a testament to his brilliance, his passion for the game and his fitness over the years. Among the active players only AB de Villiers and Hashim Amla are at the halfway mark relatively to Tendulkar’s mark in Tests. But both AB and Amla are already well into their prime years. Younis Khan is on the verge of riding into the sunset. The rest of the pack including Virat Kohli, Kane Williamson, Steve Smith have a looong way to catch up.\nIt could be observed how few matches did the early era greats play relatively. Not only were few teams competing at the time, but also interruptions by world wars.\nMichael Bevan established himself as an ODI specialist and played only 18 tests. Cheteshwar Pujara appears to have barely made the cutoffs for this list. He is a Tests specialist.\n\n\rCareer runs scored\rOnce again Tendulkar’s records here are at the top. It is highly unlikely that any other batsman could ever come close in the next 5-10 years.\n\n\rCareer batting averages\rFirst and foremost is Sir Don Bradman’s career batting average record of 99.94 in Tests. He is widely regarded to be the greatest batsman of all time. This is one record that’s impossible to beat. The second batsman on this list is Graeme Pollock at 60.97. Steve Smith is the only active batsman who is anywhere close to second, a phenomenal achievement in its own right.\nSecondly, it becomes clear why each batsman was selected for this list, a batting average of 50 or more in any single format. Though most batsmen have a Tests average above 50, Jonathan Trott, MS Dhoni and Michael Bevan made it by virtue of their superlative averages in the ODIs.\nVirat Kohli is the only batsman who has averaged above 50 in all the 3 formats.\n\n\rHundreds scored in career\rOnce again Tendulkar leads in the standings with 51 hundreds in Tests and 49 hundreds in ODIs. Sir Don’s record Test average of 99.94, Tendulkar’s records of number of matches played, runs scored and hundreds scored in career are some of the records that are going to be next to impossible to beat.\nSurprisingly none of the batsmen on this list have scored a hundred in T20Is. There are a few other batsmen who have scored hundreds in T20Is but none of them were consistent enough to make it to this list.\n\n\rHighest scores in an innings\rHere we see the great Brian Lara at the top of the Tests table with a highest score of 400 not out. I remember Sir Garfield Sobers’ score of 365 not out was one of the longest standing records in Test cricket until Lara surpassed it. Only Matthew Hayden was able to come any closer to Lara.\nIn ODIs, it was apt that the champion, Sachin Tendulkar was the first one to score a double hundred. A few other batsmen have scored 200+ scores in ODIs since, but none have had the consistency that Tendulkar had.\nIn T20Is, as mentioned above none of these batsman scored a hundred. Though a few have come close. I expect some of the active players to reach this milestone in the near future.\n\n\r\r","date":1486771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1486771200,"objectID":"ffb04d9aaa82881451c924e94cdb2ba4","permalink":"/post/top-batsmen-in-cricket/","publishdate":"2017-02-11T00:00:00Z","relpermalink":"/post/top-batsmen-in-cricket/","section":"post","summary":"In a previous post, I used data from Statsguru and looked at a brief history of cricket with respect to debut years and career spans of players. In this post, I use detailed player statistics from the same dataset to select top batsmen who have played this game.\nDistribution of matches played by all players\rLet’s take a look at the summary statistics of number of matches played by all players in each format of the game.","tags":["rstats","dataviz","cricket"],"title":"Top batsmen in cricket","type":"post"},{"authors":null,"categories":["datascience"],"content":"\rSince early childhood, I have been a big fan of cricket. Some of my earliest childhood memories are watching the game along with my loved ones in India. As much as I can recall, the first series I saw was between India and the West Indies during the early 80s, on a black and white TV set. Watching Sunil Gavaskar play the fearsome West Indian fast bowlers with aplomb, made me his fan…until a kid named Sachin Tendulkar arrived in 1989.\nAs the saying goes…In India, cricket is religion and Tendulkar is God! Even after his retirement in 2013, Tendulkar is still the most loved sportsperson or celebrity in India. A billion people passionately follow the game and voice their opinion on matters of team selection, players’ performances, umpiring…you name it. Armchair analysts abound, especially if the Indian cricket team loses a consequential match or a series.\nI was excited to find that detailed records and stats on all cricket players were available on Statsguru. The database covers all formats of the game:\n\rTests - Test matches (80 overs a day, each team playing upto 2 innings in 5 days)\rODIs - One-Day Internationals (50 overs an innings)\rT20Is - Twenty20 Internationals (20 overs an innings)\r\rIn this post, I’ve tried to examine a brief history of the game using the debut years and career spans of players.\nHere’s a sample of batting data:\nBatting records of players who have represented Australia in ODIs\r\r\r\r\r\rPlayer\r\r\rSpan\r\r\rMat\r\r\rInns\r\r\rNO\r\r\rRuns\r\r\rHS\r\r\rAve\r\r\rBF\r\r\rSR\r\r\r100\r\r\r50\r\r\r0\r\r\r\r1\r\rRT Ponting\r\r1995-2012\r\r374\r\r364\r\r39\r\r13589\r\r164\r\r41.81\r\r16944\r\r80.19\r\r29\r\r82\r\r20\r\r\r\r2\r\rAC Gilchrist\r\r1996-2008\r\r286\r\r278\r\r11\r\r9595\r\r172\r\r35.93\r\r9902\r\r96.89\r\r16\r\r55\r\r19\r\r\r\r3\r\rME Waugh\r\r1988-2002\r\r244\r\r236\r\r20\r\r8500\r\r173\r\r39.35\r\r11053\r\r76.9\r\r18\r\r50\r\r16\r\r\r\r4\r\rMJ Clarke\r\r2003-2015\r\r245\r\r223\r\r44\r\r7981\r\r130\r\r44.58\r\r10104\r\r78.98\r\r8\r\r58\r\r10\r\r\r\r5\r\rSR Waugh\r\r1986-2002\r\r325\r\r288\r\r58\r\r7569\r\r120*\r\r32.90\r\r9971\r\r75.91\r\r3\r\r45\r\r15\r\r\r\r6\r\rMG Bevan\r\r1994-2004\r\r232\r\r196\r\r67\r\r6912\r\r108*\r\r53.58\r\r9320\r\r74.16\r\r6\r\r46\r\r5\r\r\r\rHere’s a sample of bowling data:\nBowling records of players who have represented Australia in ODIs\r\r\r\r\r\rPlayer\r\r\rSpan\r\r\rMat\r\r\rInns\r\r\rBalls\r\r\rRuns\r\r\rWkts\r\r\rBBI\r\r\rAve\r\r\rEcon\r\r\rSR\r\r\r4\r\r\r5\r\r\r\r1\r\rB Lee\r\r2000-2012\r\r221\r\r217\r\r11185\r\r8877\r\r380\r\r5/22\r\r23.36\r\r4.76\r\r29.4\r\r14\r\r9\r\r\r\r2\r\rGD McGrath\r\r1993-2007\r\r249\r\r247\r\r12928\r\r8354\r\r380\r\r7/15\r\r21.98\r\r3.87\r\r34.0\r\r9\r\r7\r\r\r\r3\r\rSK Warne\r\r1993-2003\r\r193\r\r190\r\r10600\r\r7514\r\r291\r\r5/33\r\r25.82\r\r4.25\r\r36.4\r\r12\r\r1\r\r\r\r4\r\rMG Johnson\r\r2005-2015\r\r153\r\r150\r\r7489\r\r6038\r\r239\r\r6/31\r\r25.26\r\r4.83\r\r31.3\r\r9\r\r3\r\r\r\r5\r\rCJ McDermott\r\r1985-1996\r\r138\r\r138\r\r7461\r\r5018\r\r203\r\r5/44\r\r24.71\r\r4.03\r\r36.7\r\r4\r\r1\r\r\r\r6\r\rSR Waugh\r\r1986-2002\r\r325\r\r207\r\r8883\r\r6761\r\r195\r\r4/33\r\r34.67\r\r4.56\r\r45.5\r\r3\r\r0\r\r\r\rCurrently there are 10 international teams which have full-membership of the International Cricket Council - Australia, Bangladesh, England, India, New Zealand, Pakistan, South Africa, Sri Lanka, West Indies and Zimbabwe. I downloaded batting and bowling data for all of these 10 teams in all 3 formats. The dataset is rich with features and appears to cover the entire history of international cricket.\nMajor props to the folks at Statsguru for maintaining a clean dataset. Both batting and bowling datasets have the same number of players, as should be the case. So there wasn’t much data wrangling needed.\nHere are what the abbreviations mean:\n\r\r\r\rTerm\rDescription\r\r\r\rSpan\rSeparate into debut year and last year of career\r\rMat\rMatches played in career\r\rInns\rInnings played in career\r\rNO\rNumber of times not out in career\r\rHS\rHighest batting score in career\r\rAve\rCareer Average, batting: Runs/(Innings - NotOuts), bowling: Runs/Wickets\r\rBF\rBalls Faced\r\rSR\rStrike Rate, batting: Runs/Balls Faced, bowling: Balls/Wickets\r\rBBI\rBest bowling performance in an innings\r\rBalls\rNumber of balls bowled in career\r\rWkts\rNumber of wickets in career\r\rEcon\rBowling economy rate: Runs per over (Overs bowled = Balls bowled/6)\r\r100\rNumber of hundreds scored in career\r\r50\rNumber of fifties scored in career\r\r4\rNumber of times the bowler took 4 wickets in an innings\r\r5\rNumber of times the bowler took 5 or more wickets in an innings\r\r\r\rWhen did each country first play internationally?\rFrom the earliest debut years of players, it could be seen when each format was started and when each country started playing in a format. Australia and England have been the pioneering nations in each format. The earliest Tests were played between Australia and England in 1877. South Africa joined the fray in 1889. For the next 40 years, these were the only 3 international teams, until West Indies, India and New Zealand started playing in quick succession. In 1970 South Africa was suspended from playing international cricket by the ICC because of prevailing apartheid in the nation. As a result, the international career of some very promising South Africans like Graeme Pollock and Barry Richards ended abruptly. Later on some other players emigrated to England and Australia to get a chance to play internationally. This ban was lifted by the ICC in 1991. The newly formed South African team under Clive Rice was given a warm welcome in India, when they visited to play ODIs. South Africa resumed playing tests in 1992.\nSri Lanka, Zimbabwe and Bangladesh were Associate members with ODI status before they were elected as full members to be eligible to play Tests.\n\n\rNumber of players who have represented their country internationally\rEven though Australia and England started playing Tests together in 1877, there have been far more players representing the England team in Tests, when looking at the entire history. This suggests a lot more turnover in the English team.\n\n\rNumber of players who debuted internationally by year\rThis plot shows the number of distinct players debuting each year. So, for instance, if a player debuted in Tests, ODIs and T20Is team in the same year, then that is counted as 1. Few things could be noted here:\n\rThe gaps in Australia, England and India facets show the years interrupted by the 2 world wars.\rAs noted earlier, there’s a big gap in the South Africa facet, when the country was suspended from 1970-1991.\rThe gaps in Bangladesh, Sri Lanka and Zimbabwe are because each country was an associate member of the ICC and was only competing in ICC ODI tournaments until the country got its full member status.\rThere’s been a steady increase in the number of players debuting in Australia, England, India, Pakistan and West Indies, in the last decade or so. This suggests a trend of having specialist players in the team for each format. Top teams like Australia, England and India have adopted this trend. Some players are selected as Test match specialists, while the others are selected to only play in ODIs and T20Is.\r\r\rCareer spans of players\rTo examine the career spans of players, I filtered the dataset to only keep the players who haven’t been active in 2016-2017. Then I eliminated quick failures whose career didn’t last for more than 2 years. This meant the players left in the dataset were given enough opportunities to succeed at the international level. Then I examined the median career span by debut year.\nIt appears a median player used to play between 6 to 10 years before retiring. Although there was more dispersion in Tests, it was clearly the norm in ODIs. 1908 was an extreme outlier year in Tests because only 1 player qualified the inclusion criteria. This was Sir Jack Hobbs of England, an early master of the game who played for 23 years. 1909 was another extreme outlier year where only 3 players were included. These were the legends Warren Bardsley of Australia (18 years), Frank Woolley of England (26 years) and Bill Whitty of Australia (4 years).\nAn outlier in the ODIs plot corresponds to the year 1989. This was the year when some modern day legends made their debut. Most notably Sachin Tendulkar from India who played for 24 years. Saeed Anwar, Waqar Younis \u0026amp; Mushtaq Ahmed from Pakistan who each played for 15 years and Sanath Jayasuriya from Sri Lanka who played for 23 years. Alec Stewart and Nasser Hussain from England also debuted in 1989. They had checkered records in ODIs but overall their careers lasted for 15 years.\nAn astonishing trend appears to have started with the players debuting after the early 90s. Career spans of players peaked with those debuting during early 90s. Since then the median career span started dropping year by year. This could be seen across all formats. Although the median career span would change for more recent years as active players playing from early 2000s retire in future, nevertheless this trend is persistent from early 90s.\n\rMatches played by debut years\rThe same downwards trends from early 90s could also be observed in the median number of matches played by debut year. This isn’t surprising as the number of matches played is directly correlated with career span. An upward trend from early 30s could be observed in Test matches played. This is explained by the addition of West Indies, New Zealand and India as full members of the ICC from late 20s. Every international player started playing more matches as the number of competing teams doubled to 6.\nA big upward trend could be seen in ODIs from its inception. Until the beginning of ODIs, cricket was only played in Tests format that was characteristically very laid back. Both teams got a chance to bat upto 2 innings alternately over 5 days. More often than not the matches would end in a draw with no win/loss result. The introduction of 60 overs a side ODIs (later reduced to 50 overs a side) format was to be played in a single day. The team batting first would set a target score. Then the team batting second would chase this target. This ensured guaranteed results although there have been few occasions where the ODIs have resulted in ties at the same score. Naturally this format was more palatable to spectators and TV audiences that contributed to a rapid growth in its popularity. The ODIs format is still the most popular format till date.\nIn the next couple of posts I will analyze batting performances.\n\n\r","date":1486512000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1486512000,"objectID":"a6ec72abd1414e8bd615facdda824e07","permalink":"/post/a-brief-history-of-cricket/","publishdate":"2017-02-08T00:00:00Z","relpermalink":"/post/a-brief-history-of-cricket/","section":"post","summary":"Since early childhood, I have been a big fan of cricket. Some of my earliest childhood memories are watching the game along with my loved ones in India. As much as I can recall, the first series I saw was between India and the West Indies during the early 80s, on a black and white TV set. Watching Sunil Gavaskar play the fearsome West Indian fast bowlers with aplomb, made me his fan…until a kid named Sachin Tendulkar arrived in 1989.","tags":["rstats","dataviz","cricket"],"title":"A brief history of cricket","type":"post"},{"authors":null,"categories":["case studies"],"content":"\rIn a previous post in this series, we did an exploratory data analysis of the Ames Housing dataset.\nIn this post, we will build linear and non-linear models and see how well they predict the SalePrice of properties.\nEvaluation Criteria\rRoot-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed SalePrice will be our evaluation criteria. Taking the log ensures that errors in predicting expensive and cheap houses will affect the result equally.\n\rSteps for Building Models\rHere are the steps for building models and determining the best hyperparameter combinations by K-fold cross validation:\n\rPartition the training dataset into model training and validation sets. Use stratified sampling such that each partition has a similar distribution of the target variable - SalePrice.\rDefine linear and non-linear models.\rFor each model, create a grid of hyperparameter combinations that are equally spaced.\rFor each hyperparameter combination, fit a model on the training set and make predictions on the validation set. Repeat the process for all folds.\rDetermine root mean squared errors (RMSE) and choose the best hyperparameter combination that corresponds to the minimum RMSE.\rTrain each model with its best hyperparameter combination on the entire training set.\rCalculate RMSE of the each finalized model on the testing set.\rFinally, choose the best model that gives the least RMSE.\r\r\rPartitioning Training Data\rWe split the training data into 4 folds. Within each fold, 75% of the data is used for training models and 25% for validating the predicted values against the actual values.\nLet’s look at the distribution of the target variable across all folds:\nBy using stratified sampling, we ensure that the training and validation distributions of the target variable are similar.\n\rLinear Models\rOrdinary Least Squares Regression\rBefore creating any new features or indulging in more complex modelling methods, we will cross validate a simple linear model on the training data to establish a benchmark. If more complex approaches do not have a significant improvement in the model validation metrics, then they are not worthwhile to be pursued.\nLinear Regression Model Specification (regression)\rComputational engine: lm \rWhat’s notable?\r\rAfter training a linear model on all predictors, we get an RMSE of 0.1468.\rThis is the simplest and fastest model with no hyperparameters to tune.\r\r\r\rRegularized Linear Model\rWe will use glmnet that uses LASSO and Ridge Regression with regularization. We will do a grid search of the following hyperparameters that minimize RMSE:\n\rpenalty: The total amount of regularization in the model.\rmixture: The proportion of L1 regularization in the model.\r\rLinear Regression Model Specification (regression)\rMain Arguments:\rpenalty = tune()\rmixture = tune()\rComputational engine: glmnet \rLet’s take a look at the top 10 RMSE values and hyperparameter combinations:\n# A tibble: 10 x 3\rpenalty mixture mean_rmse\r\u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 4.83e- 3 0.922 0.127\r2 3.79e- 2 0.0518 0.129\r3 1.36e- 3 0.659 0.132\r4 1.60e- 3 0.431 0.133\r5 3.50e- 3 0.177 0.133\r6 4.17e- 2 0.288 0.133\r7 5.67e- 4 0.970 0.133\r8 6.79e- 9 0.0193 0.138\r9 4.32e-10 0.337 0.138\r10 1.95e- 6 0.991 0.138\rWhat’s notable?\r\rAfter hyperparameter tuning with cross validation, glmnet gives the best RMSE of 0.127 with penalty = 0.0048 and mixture = 0.9216.\rIt is a significant improvement over Ordinary Least Squares regression that had an RMSE of 0.1468.\rglmnet cross validation takes under a minute to execute.\rBut the presence of outliers can significantly affect its performance.\r\rHere a plot of the glmnet hyperparameter grid along with the best hyperparameter combination:\n\r\r\rNon-linear Models\rNext, we will train a couple of tree-based algorithms, which are not very sensitive to outliers and skewed data.\nrandomForest\rIn each ensemble, we have 1000 trees and do a grid search of the following hyperparameters:\n\rmtry: The number of predictors to randomly sample at each split.\rmin_n: The minimum number of data points in a node required to further split the node.\r\rRandom Forest Model Specification (regression)\rMain Arguments:\rmtry = tune()\rtrees = 1000\rmin_n = tune()\rEngine-Specific Arguments:\robjective = reg:squarederror\rComputational engine: randomForest \rLet’s take a look at the top 10 RMSE values and hyperparameter combinations:\n# A tibble: 10 x 3\rmin_n mtry mean_rmse\r\u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r1 4 85 0.134\r2 3 140 0.135\r3 14 90 0.135\r4 6 45 0.136\r5 9 138 0.136\r6 13 158 0.137\r7 9 183 0.137\r8 19 56 0.138\r9 21 130 0.138\r10 5 218 0.138\rWhat’s notable?\r\rAfter cross validation, we get the best RMSE of 0.134 with mtry = 85 and min_n = 4.\rThis is no improvement in RMSE compared to glmnet and randomForest cross validation takes much longer to execute than glmnet.\r\rHere a plot of the randomForest hyperparameter grid along with the best hyperparameter combination:\n\r\rxgboost\rIn each ensemble we have 1000 trees and do a grid search of the following hyperparameters:\n\rmin_n: The minimum number of data points in a node required to further split the node.\rtree_depth: The maximum depth or the number of splits of the tree.\rlearn_rate: The rate at which the boosting algorithm adapts from one iteration to another.\r\rBoosted Tree Model Specification (regression)\rMain Arguments:\rtrees = 1000\rmin_n = tune()\rtree_depth = tune()\rlearn_rate = tune()\rEngine-Specific Arguments:\robjective = reg:squarederror\rComputational engine: xgboost \rLet’s take a look at the top 10 RMSE values and hyperparameter combinations:\n# A tibble: 10 x 4\rmin_n tree_depth learn_rate mean_rmse\r\u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 13 3 0.0309 0.124\r2 40 4 0.0350 0.126\r3 6 8 0.0469 0.126\r4 34 15 0.0172 0.127\r5 28 10 0.0336 0.128\r6 20 14 0.00348 0.389\r7 22 7 0.000953 4.46 8 3 2 0.000528 6.81 9 10 12 0.000401 7.73 10 34 3 0.0000802 10.6 \rWhat’s notable?\r\rAfter cross validation, we get the best RMSE of 0.124 with min_n = 13, tree_depth = 3 and learn_rate = 0.0309.\rGives the best RMSE compared to glmnet and randomForest.\rHowever, xgboost cross validation takes longer to execute than that of glmnet, but is faster than that of randomForest\r\r--\r% --\r% --\r--\r\r\r\rFinalizing Models\rFor each model, we found the combination of hyperparameters that minimize RMSE. Using those parameters, we can now train the same models on the entire training dataset. Finally, we can use the trained models to predict log(SalePrice) on the entire training set to see the actual v/s predicted log(SalePrice) results.\nWhat’s notable?\r\rBoth randomForest and xgboost models do a fantastic job of predicting log(SalePrice) with the tuned parameters, as the predictions lie close to the straight line drawn at 45 degrees.\rThe glmnet model shows a couple of outliers with Ids 524 and 1299 whose predicted values are far in excess of their actual values. Even properties whose SalePrice is at the lower end, show a wide dispersion in prediced values.\rBut the true performance can only be measured on unseen testing data.\r\r\r\rPerformance on Test Data\r# A tibble: 3 x 3\rmodel test_rmse cv_rmse\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 glmnet 0.129 0.127\r2 randomForest 0.139 0.134\r3 xgboost 0.128 0.124\rWhat’s notable?\r\rAll models have similar RMSE on the unseen testing set as their cross validated RMSE, which shows the cross validation process and hyperparameters worked very well.\rRecords with Ids 1537 and 2217 are outliers, as none of the models are able to predict close to actual values.\rLooking at the test RMSE, we could finalize xgboost as the model that generalizes very well on this dataset.\r\r\r\rFeature Importance\rEven though xgboost is not as easily interpretable as a linear model, we could use variable importance plots to determine the most important features selected by the model.\nLet’s take a look at the top 10 most important features of our finalized xgboost model:\n\rCorrelations of numerical features are plotted side-by-side. All features have a correlation of 0.5 or more with SalePrice.\rAll of the top 10 features make sense. To evaluate SalePrice, a buyer would definitely look at total square footage, overall quality, neighborhood, number of bathrooms, kitchen quality, age of property, etc.\rThis shows, our finalized model generalizes well and makes very reasonable choices in terms of features.\r\r\rNew Property Premium\rAmong the top 10 features by importance in our final model, most of the features like square footage, neighborhood and number of bathrooms remain the same throughout the life of the property. Quality and condition of property does change but their evaluation is mostly subjective. The only other feature that cannot be disputed to change over time is PropertyAge.\nSo, how would the predicted SalePrice differ if a property was newly constructed vis-a-vis the same property if it were constructed more than 30 years earlier, and all the times in between?\nWe could pick a couple of properties at random, change PropertyAge and see its impact on SalePrice.\nWe can see there’s a small premium for a newly constructed property v/s an older property of the same build, quality and condition. This premium isn’t very much in a place like Ames, IA but we’d reckon it would be much higher in a larger metropolitan city.\n\r","date":1482710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482710400,"objectID":"734444933708011522f708ea70bae2aa","permalink":"/casestudies/ames-housing-part2-models/","publishdate":"2016-12-26T00:00:00Z","relpermalink":"/casestudies/ames-housing-part2-models/","section":"casestudies","summary":"In a previous post in this series, we did an exploratory data analysis of the Ames Housing dataset.\nIn this post, we will build linear and non-linear models and see how well they predict the SalePrice of properties.\nEvaluation Criteria\rRoot-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed SalePrice will be our evaluation criteria. Taking the log ensures that errors in predicting expensive and cheap houses will affect the result equally.","tags":["rstats","ggplot2","models","regression","linear","non linear","glmnet","randomForest","xgboost"],"title":"Ames Housing - Part 2 - Building Models","type":"casestudies"},{"authors":null,"categories":["case studies"],"content":"\rIn this case study, we will use the Ames Housing dataset to explore regression techniques and predict the sale price of houses.\nData Summaries\rThe Ames Housing dataset contains the sale prices of properties in Ames, Iowa along with 80 other features. Each property has an Id associated with it.\rHere are the dimensions of the training and testing sets respectively:\n[1] \u0026quot;Dimensions of the training set\u0026quot;\r[1] 1460 81\r[1] \u0026quot;Dimensions of the testing set\u0026quot;\r[1] 1459 81\rNow, let’s combine training and testing into a single dataset and take a look at the count of missing values:\nWhat’s notable?\r\rThe combined dataset has 2919 property records.\rVery few properties have a pool, fence or an alley access to the property.\rVery few properties have a miscellaneous feature that has not been covered by other features.\rMore than a dozen features have atleast 1 missing value. Since we have a tiny dataset, we will try to impute the missing values.\r\r\r\rData Cleaning \u0026amp; Transformation\rWe will visualize features of the complete dataset and create a data cleaning pipeline.\nFixing Data Errors\rFirst, a few data integrity checks need to be done to ensure the quality of the data:\n\rYearRemodAdd should not be earlier than YearBuilt: 1 record to be fixed\rYrSold should not be earlier than YearRemodAdd: 3 records to be fixed\r\r# A tibble: 1 x 4\rId YearBuilt YearRemodAdd YrSold\r\u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 1877 2002 2001 2009\r# A tibble: 3 x 4\rId YearBuilt YearRemodAdd YrSold\r\u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 524 2007 2008 2007\r2 2296 2007 2008 2007\r3 2550 2008 2009 2007\r\rGarageYrBlt should not be earlier than YearBuilt: 18 records to be fixed\rGarageYrBlt should not be later than YrSold: 1 record to be fixed\r\r# A tibble: 18 x 4\rId YearBuilt GarageYrBlt YrSold\r\u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 30 1927 1920 2008\r2 94 1910 1900 2007\r3 325 1967 1961 2010\r4 601 2005 2003 2006\r5 737 1950 1949 2006\r6 1104 1959 1954 2006\r7 1377 1930 1925 2008\r8 1415 1923 1922 2008\r9 1419 1963 1962 2008\r10 1522 1959 1956 2010\r11 1577 2010 2009 2010\r12 1806 1935 1920 2009\r13 1841 1978 1960 2009\r14 1896 1941 1940 2009\r15 1898 1935 1926 2009\r16 2123 1945 1925 2008\r17 2264 2006 2005 2007\r18 2510 2006 2005 2007\r# A tibble: 1 x 4\rId YearBuilt GarageYrBlt YrSold\r\u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 2593 2006 2207 2007\r\rImputing Missing Values \u0026amp; New Features\rBasement Features --\rBasement Features\rThere is one property (Id = 2121) where all the basement features are NA. TotalBsmtSF is replaced by 0.\n\rNow there are 79 properties which have no basement (TotalBsmtSF = 0). All other basement features having NA values are changed to None.\n\rSince qualitative features do not have the same distribution across neighborhoods, any remaining NA values are imputed to be the most common value in that Neighborhood.\n\r\r# A tibble: 1 x 13\rId Neighborhood BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinSF1 BsmtFinType2 BsmtFinSF2 BsmtUnfSF TotalBsmtSF BsmtFullBath BsmtHalfBath\r\u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 2121 BrkSide \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; NA \u0026lt;NA\u0026gt; NA NA NA NA NA\r# A tibble: 79 x 13\rId Neighborhood BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinSF1 BsmtFinType2 BsmtFinSF2 BsmtUnfSF TotalBsmtSF BsmtFullBath BsmtHalfBath\r\u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 18 Sawyer \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 \u0026lt;NA\u0026gt; 0 0 0 0 0\r2 40 Edwards \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 \u0026lt;NA\u0026gt; 0 0 0 0 0\r3 91 NAmes \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 \u0026lt;NA\u0026gt; 0 0 0 0 0\r4 103 SawyerW \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 \u0026lt;NA\u0026gt; 0 0 0 0 0\r5 157 NAmes \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 \u0026lt;NA\u0026gt; 0 0 0 0 0\r6 183 Edwards \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 \u0026lt;NA\u0026gt; 0 0 0 0 0\r7 260 OldTown \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 \u0026lt;NA\u0026gt; 0 0 0 0 0\r8 343 NAmes \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 \u0026lt;NA\u0026gt; 0 0 0 0 0\r9 363 Edwards \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 \u0026lt;NA\u0026gt; 0 0 0 0 0\r10 372 ClearCr \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 \u0026lt;NA\u0026gt; 0 0 0 0 0\r# ... with 69 more rows\r# A tibble: 9 x 13\rId Neighborhood BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinSF1 BsmtFinType2 BsmtFinSF2 BsmtUnfSF TotalBsmtSF BsmtFullBath BsmtHalfBath\r\u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 333 NridgHt Gd TA No GLQ 1124 \u0026lt;NA\u0026gt; 479 1603 3206 1 0\r2 949 CollgCr Gd TA \u0026lt;NA\u0026gt; Unf 0 Unf 0 936 936 0 0\r3 1488 Somerst Gd TA \u0026lt;NA\u0026gt; Unf 0 Unf 0 1595 1595 0 0\r4 2041 Veenker Gd \u0026lt;NA\u0026gt; Mn GLQ 1044 Rec 382 0 1426 1 0\r5 2186 Edwards TA \u0026lt;NA\u0026gt; No BLQ 1033 Unf 0 94 1127 0 1\r6 2218 IDOTRR \u0026lt;NA\u0026gt; Fa No Unf 0 Unf 0 173 173 0 0\r7 2219 IDOTRR \u0026lt;NA\u0026gt; TA No Unf 0 Unf 0 356 356 0 0\r8 2349 Somerst Gd TA \u0026lt;NA\u0026gt; Unf 0 Unf 0 725 725 0 0\r9 2525 CollgCr TA \u0026lt;NA\u0026gt; Av ALQ 755 Unf 0 240 995 0 0\rHistograms of numerical basement features and their correlations with SalePrice are plotted below.\nIt could be verified that: TotalBsmtSF = BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF\nAdditionally, new features are generated where:\n\rBsmtBath = BsmtFullBath + 0.5 * BsmtHalfBath\rHasBsmt = TotalBsmtSF \u0026gt; 0\r\r\rMost properties have a basement.\rColumn plots show that BsmtFinType2 and BsmtCond values are dominated by a single category.\r\r\rBathroom Features\r\rA new feature is generated to determine the total number of bathrooms: TotalBath = FullBath + HalfBath + BsmtBath\r\r\rFireplace Features\rThere are 1420 properties that have no fireplaces. FireplaceQu is changed to None.\r\r# A tibble: 1,420 x 4\rId Neighborhood Fireplaces FireplaceQu\r\u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; 1 1 CollgCr 0 \u0026lt;NA\u0026gt; 2 6 Mitchel 0 \u0026lt;NA\u0026gt; 3 11 Sawyer 0 \u0026lt;NA\u0026gt; 4 13 Sawyer 0 \u0026lt;NA\u0026gt; 5 16 BrkSide 0 \u0026lt;NA\u0026gt; 6 18 Sawyer 0 \u0026lt;NA\u0026gt; 7 19 SawyerW 0 \u0026lt;NA\u0026gt; 8 20 NAmes 0 \u0026lt;NA\u0026gt; 9 27 NAmes 0 \u0026lt;NA\u0026gt; 10 30 BrkSide 0 \u0026lt;NA\u0026gt; # ... with 1,410 more rows\r\rA new feature is generated where: HasFireplace = Fireplaces \u0026gt; 0\rA significant number of properties have fireplaces.\r\r\rGarage Features\rGarageYrBlt where NA is set to YearBuilt.\n\rThere are 157 properties where the property has no garage. In these records, GarageType, GarageFinish, GarageQual and GarageCond are recorded as None.\n\rSince qualitative features do not have the same distribution across neighborhoods, any remaining NA values are imputed to be the most common or median value in the Neighborhood by GarageType.\n\r\r# A tibble: 157 x 9\rId Neighborhood GarageType GarageYrBlt GarageFinish GarageCars GarageArea GarageQual GarageCond\r\u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; 1 40 Edwards \u0026lt;NA\u0026gt; 1955 \u0026lt;NA\u0026gt; 0 0 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 2 49 OldTown \u0026lt;NA\u0026gt; 1920 \u0026lt;NA\u0026gt; 0 0 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 3 79 Sawyer \u0026lt;NA\u0026gt; 1968 \u0026lt;NA\u0026gt; 0 0 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 4 89 IDOTRR \u0026lt;NA\u0026gt; 1915 \u0026lt;NA\u0026gt; 0 0 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 5 90 CollgCr \u0026lt;NA\u0026gt; 1994 \u0026lt;NA\u0026gt; 0 0 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 6 100 NAmes \u0026lt;NA\u0026gt; 1959 \u0026lt;NA\u0026gt; 0 0 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 7 109 IDOTRR \u0026lt;NA\u0026gt; 1919 \u0026lt;NA\u0026gt; 0 0 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 8 126 IDOTRR \u0026lt;NA\u0026gt; 1935 \u0026lt;NA\u0026gt; 0 0 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 9 128 OldTown \u0026lt;NA\u0026gt; 1930 \u0026lt;NA\u0026gt; 0 0 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 10 141 NAmes \u0026lt;NA\u0026gt; 1971 \u0026lt;NA\u0026gt; 0 0 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; # ... with 147 more rows\r# A tibble: 2 x 9\rId Neighborhood GarageType GarageYrBlt GarageFinish GarageCars GarageArea GarageQual GarageCond\r\u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; 1 2127 OldTown Detchd 1910 \u0026lt;NA\u0026gt; 1 360 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 2 2577 IDOTRR Detchd 1923 \u0026lt;NA\u0026gt; NA NA \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \r\rGarageArea and GarageCars have almost similar correlation with SalePrice.\rA new feature is generated where: HasGarage = GarageArea \u0026gt; 0\rMost properties have a garage.\rColumn plots show that GarageQual and GarageCond values are dominated by a single category.\r\r\rMasonry Features\rThere is one property (Id = 2611) where MasVnrArea = 198 but MasVnrType = NA. Impute MasVnrType to be most common value in the neighborhood where MasVnrArea \u0026gt; 0.\n\rImpute NA values in MasVnrType to be the most common values by Neighborhood and YearRemodAdd.\n\rImpute NA values in MasVnrArea to be the median values by Neighborhood and MasVnrType.\n\r\r# A tibble: 1 x 4\rId Neighborhood MasVnrType MasVnrArea\r\u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r1 2611 Mitchel \u0026lt;NA\u0026gt; 198\r# A tibble: 23 x 4\rId Neighborhood MasVnrType MasVnrArea\r\u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r1 235 Gilbert \u0026lt;NA\u0026gt; NA\r2 530 Crawfor \u0026lt;NA\u0026gt; NA\r3 651 Somerst \u0026lt;NA\u0026gt; NA\r4 937 SawyerW \u0026lt;NA\u0026gt; NA\r5 974 Somerst \u0026lt;NA\u0026gt; NA\r6 978 Somerst \u0026lt;NA\u0026gt; NA\r7 1244 NridgHt \u0026lt;NA\u0026gt; NA\r8 1279 CollgCr \u0026lt;NA\u0026gt; NA\r9 1692 Gilbert \u0026lt;NA\u0026gt; NA\r10 1707 Somerst \u0026lt;NA\u0026gt; NA\r# ... with 13 more rows\r# A tibble: 23 x 4\rId Neighborhood MasVnrType MasVnrArea\r\u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r1 235 Gilbert None NA\r2 530 Crawfor None NA\r3 651 Somerst None NA\r4 937 SawyerW None NA\r5 974 Somerst Stone NA\r6 978 Somerst None NA\r7 1244 NridgHt Stone NA\r8 1279 CollgCr BrkFace NA\r9 1692 Gilbert None NA\r10 1707 Somerst Stone NA\r# ... with 13 more rows\r\rA new feature is generated where: HasMasVnr = MasVnrArea \u0026gt; 0\rA significant number of properties have masonry.\r\r\rPool Features\rChange values in PoolQC to None if the property has no pool\n\rImpute NA values in remaining PoolQC to the most common value in the Neighborhood in the properties that have a pool.\n\r\r# A tibble: 2,906 x 4\rId Neighborhood PoolArea PoolQC\r\u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; 1 1 CollgCr 0 \u0026lt;NA\u0026gt; 2 2 Veenker 0 \u0026lt;NA\u0026gt; 3 3 CollgCr 0 \u0026lt;NA\u0026gt; 4 4 Crawfor 0 \u0026lt;NA\u0026gt; 5 5 NoRidge 0 \u0026lt;NA\u0026gt; 6 6 Mitchel 0 \u0026lt;NA\u0026gt; 7 7 Somerst 0 \u0026lt;NA\u0026gt; 8 8 NWAmes 0 \u0026lt;NA\u0026gt; 9 9 OldTown 0 \u0026lt;NA\u0026gt; 10 10 BrkSide 0 \u0026lt;NA\u0026gt; # ... with 2,896 more rows\r# A tibble: 3 x 4\rId Neighborhood PoolArea PoolQC\r\u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; 1 2421 NAmes 368 \u0026lt;NA\u0026gt; 2 2504 SawyerW 444 \u0026lt;NA\u0026gt; 3 2600 Mitchel 561 \u0026lt;NA\u0026gt; \r\rA new feature is generated where: HasPool = PoolArea \u0026gt; 0\rMost properties do not have a pool.\r\r\rPorch Features\r\rNew features are generated for:\r\rTotal porch area: PorchSF = OpenPorchSF + EnclosedPorch + 3SsnPorch + ScreenPorch\rWhether property has a porch: HasPorch = PorchSF \u0026gt; 0\r\r\r\rBuilt Area Features\r\rA new feature is added to determine the total square footage of built area: TotalSF = GrLivArea + TotalBsmtSF\r\r\rConstruction Year Features\r\rNew features are generated for:\r\rVintage of year built: 1945 or earlier, 1946-1999, 2000 or later\rAge of property from when it was built to the time it was sold: PropertyAge = YrSold - YearRemodAdd\rIndicate if the property is new or newly renovated: IsNew = YearRemodAdd == YrSold\rIndicate if the property has been remodelled: IsRemodAdd = YearRemodAdd \u0026gt; YearBuilt\r\r\r\rNeighborhood Features\rType of Neighborhood: There are 25 neighborhoods in the dataset.\rAs it is said, real estate is all about location, location, location. Clearly some neighborhoods command higher prices than others.\nNeighborhoods could be grouped together in fewer categories depending upon how they are ranked by their median SalePrice:\n\rType1: StoneBr, NridgHt, NoRidge\rType2: Veenker, Timber, Somerst\rType3: Crawfor, CollgCr, ClearCr, Blmngtn, Gilbert, NWAmes, SawyerW\rType4: Mitchel, NPkVill, NAmes, SWISU, Sawyer, Blueste, BrkSide, Edwards, OldTown\rType5: IDOTRR, BrDale, MeadowV\r\r\rOther Missing Features\rIn MiscFeature, Alley and Fence NA values are recoded as None.\n\rIn Utilities, Functional, SaleType NA values are imputed as the most common value of each feature.\n\rIn LotFrontage NA values are imputed as the median values in the Neighborhood.\n\rIn MSZoning, KitchenQual, Exterior1st, Exterior2nd, Electrical NA values are imputed as the most common value in the Neighborhood.\n\r\r\r\rLabel Encoding\rA quick look at the data description shows many features have categories that follow a specific order. These features are:\n\rLotShape: Reg, IR1, IR2, IR3\rLandSlope: Gtl, Mod, Sev\rExterQual: Ex, Gd, TA, Fa, Po\rExterCond: Ex, Gd, TA, Fa, Po\rBsmtQual: Ex, Gd, TA, Fa, Po, None\rBsmtCond: Ex, Gd, TA, Fa, Po, None\rBsmtExposure: Gd, Av, Mn, No, None\rBsmtFinType1: GLQ, ALQ, BLQ, Rec, LwQ, Unf, None\rBsmtFinType2: GLQ, ALQ, BLQ, Rec, LwQ, Unf, None\rHeatingQC: Ex, Gd, TA, Fa, Po\rCentralAir: Y, N\rKitchenQual: Ex, Gd, TA, Fa, Po\rFunctional: Typ, Min1, Min2, Mod, Maj1, Maj2, Sev, Sal\rFireplaceQu: Ex, Gd, TA, Fa, Po, None\rGarageFinish: Fin, RFn, Unf, None\rGarageQual: Ex, Gd, TA, Fa, Po, None\rGarageCond: Ex, Gd, TA, Fa, Po, None\rStreet: Grvl, Pave\rPavedDrive: Y, P, N\r\rMost of these features have a common order Ex, Gd, TA, Fa, Po, except some are missing None as a category. These features could be ordered with a common set of categories from Ex, Gd, TA, Fa, Po, None.\nSome categorical features are already ordered by an integer number. These features are:\n\rOverallQual: 10 to 1\rOverallCond: 10 to 1\r\rMoSold is cyclical and should be recoded as a factor.\nYrSold has only 5 values from 2006-2010 and should also be recoded as a factor.\nCategorical features where several categories have less than 10 observations are lumped into a single category named Other.\n\rFeatures to Drop\rHighly Correlated Features\rSome features could be dropped from further analysis because either they are too correlated or replaced by a similar feature.\n [1] \u0026quot;BsmtFullBath\u0026quot; \u0026quot;GarageCars\u0026quot; \u0026quot;GarageYrBlt\u0026quot; \u0026quot;GrLivArea\u0026quot; \u0026quot;PoolArea\u0026quot; \u0026quot;YearBuilt\u0026quot; \u0026quot;YearRemodAdd\u0026quot; \u0026quot;Neighborhood\u0026quot; \u0026quot;OpenPorchSF\u0026quot; [10] \u0026quot;EnclosedPorch\u0026quot; \u0026quot;3SsnPorch\u0026quot; \u0026quot;ScreenPorch\u0026quot; \r\rSkewed Categorical Features\rAny feature where more than 95% of the records have the same category probably doesn’t have any predictive value. An extreme case is Utilities which has only 2 categories - AllPub and NoSeWa in the dataset. Only 1 record has NoSeWa and the rest of the records have AllPub. Therefore, features like these do not have any predictive value.\n\r\r\rFinalized Data\r[1] \u0026quot;Dimensions of the finalized dataset\u0026quot;\r[1] 2919 73\rExcluding Id, there are 72 features in the finalized dataset.\rThere are 26 numerical, 26 ordinal and 20 nominal features.\r\rUnivariate Analysis\rLet us look at each feature in the dataset in detail.\nNumerical Features\rFirst let’s plot all the features that are measured as area in square feet:\nWhat’s notable?\r\rAll area features have outliers.\rMany features are heavily skewed so they need to be normalized before fitting models.\r\rNow let’s see other numerical features:\n\rWhat’s notable?\r\rMost of the properties have been built less than 20 years prior to their sale.\r\rLet’s plot the distribution of SalePrice in log scale:\n\rWhat’s notable?\r\rWe see long tailed distribution on both sides.\rThere are 11 properties below USD 50,000 and 17 above USD 500,000.\rLinear models are very sensitive to the presence of outliers.\r\r\r\r\rCategorical Features\rOrdinal Features\rWhat’s notable?\r\rCategorical imbalances exist in many features where 1 or 2 categories are dominant. This poses a big challenge for using these features as predictors, as categories with fewer counts tend to be underrepresented in the data.\r\r\r\rNominal Features.\rWhat’s notable?\r\rCategorical imbalances exist in many features where 1 or 2 categories are dominant.\rMost of the properties are sold during the summer months, and the least during the winter months.\rThe effect of housing market crisis are visible in the data, as the fewest properties were sold in 2010.\r\r\r\r\rBivariate Analysis\rNumerical-Numerical\rLet’s examine the relationship of SalePrice with other numerical features:\nWhat’s notable?\r\rFrom the scatterplot of TotalSF v/s SalePrice, it is very clear there are high leverage points where the target SalePrice is unusually low relative to the area in sq. ft. These points have an outsized impact on the slope of the regression line, which otherwise would be higher.\rThe same set of points impact TotalBsmtSF.\rThe Ids of these records are 524,1299,2550. Out of these 524 and 1299 are in the training set.\r\r\r\rCorrelations with SalePrice\rWe isolate the features that have an absolute correlation of 0.1 or more with SalePrice.\nWhat’s notable?\r\rThe top 5 features are TotalSF, GarageArea, TotalBath, TotalBsmtSF, 1stFlrSF. Quite reasonably, a buyer would look at these features to evaluate a property and its SalePrice.\rIt is somewhat counterintuitive that PropertyAge shows a strong negative correlation with SalePrice. It means properties that were more recently built, sell for higher prices than older properties.\r\r\r\rNumerical-Categorical (Ordinal)\rWhat’s notable?\r\rWe can spot clear trends in SalePrice v/s the order of the categories in almost all of these features.\rOverall quality and external quality show some of the strongest trends.\r\r\r\rNumerical-Categorical (Nominal)\rLet’s examine SalePrice with respect to the nominal features in the dataset. None of these features have a natural order, but we can identify trends within categories by sorting with the median SalePrice. The SalePrice axis is truncated to exclude outliers.\nWhat’s notable?\r\rGarageType: Builtin and attached garages are more preferred than detached or other types of garages.\rFrom MSSubClass categories, it is evident that 1946 or newer houses are higher priced than older houses.\r\r\r\r\rMultivariate Analysis\rWe will check variation of some related features with SalePrice.\nNumerical-Numerical-Categorical\rWe have determined TotalSF and GarageArea have among the strongest correlations with SalePrice. Let’s see how they vary by NeighborhoodType and GarageType respectively:\n\rFor the same total area, there are neighborhoods where SalePrice is higher than others.\rProperties with no garage are distinctly separated.\rProperties with built-in or attached garages tend to have higher SalePrice for the same GarageArea.\rTherefore, NeighborhoodType and GarageType explain some variance in SalePrice.\r\r\rCategorical-Categorical-Numerical\rWe want to see if there is any interaction of SalePrice with a combination of categorical features, that could provide any additional explanatory power for prediction:\n\rIt is evident that some neighborhoods have higher OverallQual and therefore command higher price. However in Type4 neighborhoods, we can see a clear variation in SalePrice by quality of property.\rIt is less clear if GarageType has a major impact by itself. Even though built-in and attached garages seem to be preferred, most of the variation can be explained by NeighborhoodType itself.\rLow density and floating village residential properties tend to be higher priced in both single and multi-storied properties built after 1946.\r\r\r\r","date":1482624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482624000,"objectID":"a771855254b168195f5a972534761a5d","permalink":"/casestudies/ames-housing-part1-eda/","publishdate":"2016-12-25T00:00:00Z","relpermalink":"/casestudies/ames-housing-part1-eda/","section":"casestudies","summary":"In this case study, we will use the Ames Housing dataset to explore regression techniques and predict the sale price of houses.\nData Summaries\rThe Ames Housing dataset contains the sale prices of properties in Ames, Iowa along with 80 other features. Each property has an Id associated with it.\rHere are the dimensions of the training and testing sets respectively:\n[1] \u0026quot;Dimensions of the training set\u0026quot;\r[1] 1460 81\r[1] \u0026quot;Dimensions of the testing set\u0026quot;\r[1] 1459 81\rNow, let’s combine training and testing into a single dataset and take a look at the count of missing values:","tags":["rstats","ggplot2","eda"],"title":"Ames Housing - Part 1 - Exploratory Data Analysis","type":"casestudies"},{"authors":null,"categories":["case studies"],"content":"\rOther posts in this series:\r\rDiamonds - Part 1 - In the rough - An Exploratory Data Analysis\rDiamonds - Part 2 - A cut above - Building Linear Models\r\rIn a couple of previous posts, we tried to understand what attributes of diamonds are important to determine their prices. We showed that carat, clarity and color are the most important predictors of price. We arrived at this conclusion after doing a detailed exploratory data analysis. Finally we fit linear models to predict prices and determined the best model from the metrics.\nIn this post, we will use non-linear regression models to predict diamond prices and compare them with those from linear models.\n\rTraining Non-linear Models\rWe’ll follow some of the same steps as we did for linear models, while transforming some predictors:\n\rPartition the dataset into training and testing sets in the proportion 75% and 25% respectively.\rStratify the partitioning by clarity, so both training and testing sets have the same distributions of this feature.\rclarity, color and cut have ordered categories from lowest to highest grades. The randomForest method requires no change in representing this data before training the models, however xgboost and keras methods require all the predictors to be in numerical form. Two methods could be used for transforming the categorical data:\rUse one-hot encoding to convert categorical data to sparse data with 0s and 1s. This way, each category in clarity, color and cut is converted to a new predictor in binary form. A disadvantage of this method is that it treates ordered categorical data the same as unordered categorical data, so the ordinality is lost in transformation. However, non-linear models should be able to infer the ordinality as our training sample is sufficiently large.\rRepresent the ordinal categories from lowest to highest grades in integer form. However, this creates a linear gradation from one category to another, which may not be a suitable choice here.\r\rCenter and scale all values in the training set and build a matrix of predictors.\rFit a non-linear model with the training set.\rMake predictions on the testing set and determine model metrics.\rWrap all the steps above inside a function in which the model formula, and a seed could be passed that randomizes the partition of training and testing sets.\rRun multiple iterations of models with different seeds, and compute their average metrics, that would reflect results on unseen data.\r\rHere are the average metrics for all the models trained with keras, randomForest and xgboost regression methods:\n\r\r\rmae\r\r\rrmse\r\r\rrsq\r\r\r\r\rkeras\r\rrandomForest\r\rxgboost\r\r\rkeras\r\rrandomForest\r\rxgboost\r\r\rkeras\r\rrandomForest\r\rxgboost\r\r\r\r\r\rprice ~ .\r\r360.55\r\r262.35\r\r280.49\r\r\r989.71\r\r529.28\r\r540.76\r\r\r0.93\r\r0.98\r\r0.98\r\r\r\rprice ~ carat\r\r860.29\r\r816.1\r\r815.76\r\r\r1499.2\r\r1427.25\r\r1427.35\r\r\r0.86\r\r0.87\r\r0.87\r\r\r\rprice ~ carat + clarity\r\r590.32\r\r548.67\r\r544.48\r\r\r1040.69\r\r1006.61\r\r992.46\r\r\r0.93\r\r0.94\r\r0.94\r\r\r\rprice ~ carat + clarity + color\r\r358.85\r\r305.17\r\r306.86\r\r\r645.4\r\r571.73\r\r575.3\r\r\r0.97\r\r0.98\r\r0.98\r\r\r\rprice ~ carat + clarity + color + cut\r\r347.99\r\r285.96\r\r282.38\r\r\r626.78\r\r545.02\r\r541.63\r\r\r0.98\r\r0.98\r\r0.98\r\r\r\r\rLooking at the r-squared terms, it is remarkable how well all the models have been able to infer the complex relationship between price and carat. To fit linear models, we needed to transform price to logarithmic terms and take the cube root of carat. The neural network as well as the decision tree based models do this all on their own. The root mean squared error is in $ terms so it is easier to interpret. Considering the mean and standard deviation of price in the dataset is about $4000, the root mean squared errors of the models are very low.\nExploratory data analysis adds value here, as the models with carat, clarity and color give excellent results. Including cut in the models does not provide any significant benefits and results in overfitted models.\nEven the base models with all predictors: price ~ . (where some of them are confounders), do a very good job of explaning the variance. Decision tree and neural network models are unaffected by multi-collinearity. We can use local model interpretations to determine the most important predictors from these models.\n\rLocal Interpretable Model-agnostic Explanations\rLIME is a method for explaining black-box machine learning models. It can help visualize and explain individual predictions. It makes the assumption that every complex model is linear on a local scale. So it is possible to fit a simple model around a single observation that will behave how the global model behaves at that locality. The simple model can be used to explain the predictions of the more complex model locally.\nThe generalized algorithm LIME applies is:\n\rGiven an observation, permute it to create replicated feature data with slight value modifications.\rCompute similarity distance measure between original observation and permuted observations.\rApply selected machine learning model to predict outcomes of permuted data.\rSelect m number of features to best describe predicted outcomes.\rFit a simple model to the permuted data, explaining the complex model outcome with m features from the permuted data weighted by its similarity to the original observation .\rUse the resulting feature weights to explain local behavior.\r\rHere we will select 5 features that best describe the predicted outcomes for 6 random observations from the testing set.\nThe features by importance that best explain the predictions in these 6 random samples are carat, clarity, color, x and y.\nWe know that x and y are co-linear with carat, which is why it is good practice to remove any redundant features from the training data before applying any machine learning algorithm. We find the model with the best metrics turns out to be the one using carat, clarity and color.\n\rActual v/s Predicted\rFinally, here are the scatterplots of actual v/s predicted price from the best model on the testing set, using the 3 regression methods:\nThe scatterplots are shown with both linear and logarithmic axes. Even though the results from all the 3 methods have roughly similar r-squared and rmse values, we can see predicted prices from keras have more dispersion than the two decision-tree methods at the higher end. The decision-tree based methods appear do a better job of predicting prices at the lower end with lesser dispersion.\nAs in the case with linear models, the variance in predicted diamond prices increases with price. But unlike linear models, the non-linear models do not produce extreme outliers in predicted prices. So, not only do non-linear methods do a fantastic job in inferring the relationships between price and its predictors, they also predict prices within a reasonable range.\n\rSummary\r\rAll the 3 non-linear regression methods can infer the complex relationship between price, carat and other predictors, without the need for feature engineering.\rExploratory Data Analysis is useful in removing the redundant features from the training dataset, resulting in both faster execution, as well as much better metrics.\rIn terms of time taken to train the models, keras neural network models execute the fastest by virtue of being able to use GPUs.\rAmong the decision-tree based methods, xgboost models train much faster than randomForest models.\rMultiple CPUs can be used to run randomForest and xgboost methods. RAM is the only limiting constraint, when trained on a local machine.\r\r\r","date":1482364800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482364800,"objectID":"32e601a2326d5d5823f45dd4ba0630f5","permalink":"/casestudies/diamonds-part3-non-linear-models/","publishdate":"2016-12-22T00:00:00Z","relpermalink":"/casestudies/diamonds-part3-non-linear-models/","section":"casestudies","summary":"Other posts in this series:\r\rDiamonds - Part 1 - In the rough - An Exploratory Data Analysis\rDiamonds - Part 2 - A cut above - Building Linear Models\r\rIn a couple of previous posts, we tried to understand what attributes of diamonds are important to determine their prices. We showed that carat, clarity and color are the most important predictors of price. We arrived at this conclusion after doing a detailed exploratory data analysis.","tags":["rstats","ggplot2","models","regression","non linear","neural net","keras","randomForest","xgboost"],"title":"Diamonds - Part 3 - A polished gem - Building Non-linear Models","type":"casestudies"},{"authors":null,"categories":["case studies"],"content":"\rIn a previous post in this series, we did an exploratory data analysis of the diamonds dataset and found that carat, x, y, z were strongly correlated with price. To some extent, clarity also appeared to provide some predictive ability.\nIn this post, we will build linear models and see how well they predict the price of diamonds.\nBefore we do any transformations, feature engineering or feature selections for our model, let’s see what kind of results we get from a base linear model, that uses all the features to predict price:\n\rCall:\rlm(formula = price ~ ., data = diamonds)\rResiduals:\rMin 1Q Median 3Q Max -21376 -592 -183 376 10694 Coefficients:\rEstimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 5753.76 396.63 14.51 \u0026lt; 0.0000000000000002 ***\rcarat 11256.98 48.63 231.49 \u0026lt; 0.0000000000000002 ***\rcut.L 584.46 22.48 26.00 \u0026lt; 0.0000000000000002 ***\rcut.Q -301.91 17.99 -16.78 \u0026lt; 0.0000000000000002 ***\rcut.C 148.03 15.48 9.56 \u0026lt; 0.0000000000000002 ***\rcut^4 -20.79 12.38 -1.68 0.0929 . color.L -1952.16 17.34 -112.57 \u0026lt; 0.0000000000000002 ***\rcolor.Q -672.05 15.78 -42.60 \u0026lt; 0.0000000000000002 ***\rcolor.C -165.28 14.72 -11.22 \u0026lt; 0.0000000000000002 ***\rcolor^4 38.20 13.53 2.82 0.0047 ** color^5 -95.79 12.78 -7.50 0.000000000000066 ***\rcolor^6 -48.47 11.61 -4.17 0.000030090737193 ***\rclarity.L 4097.43 30.26 135.41 \u0026lt; 0.0000000000000002 ***\rclarity.Q -1925.00 28.23 -68.20 \u0026lt; 0.0000000000000002 ***\rclarity.C 982.20 24.15 40.67 \u0026lt; 0.0000000000000002 ***\rclarity^4 -364.92 19.29 -18.92 \u0026lt; 0.0000000000000002 ***\rclarity^5 233.56 15.75 14.83 \u0026lt; 0.0000000000000002 ***\rclarity^6 6.88 13.72 0.50 0.6157 clarity^7 90.64 12.10 7.49 0.000000000000071 ***\rdepth -63.81 4.53 -14.07 \u0026lt; 0.0000000000000002 ***\rtable -26.47 2.91 -9.09 \u0026lt; 0.0000000000000002 ***\rx -1008.26 32.90 -30.65 \u0026lt; 0.0000000000000002 ***\ry 9.61 19.33 0.50 0.6192 z -50.12 33.49 -1.50 0.1345 ---\rSignif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rResidual standard error: 1130 on 53916 degrees of freedom\rMultiple R-squared: 0.92, Adjusted R-squared: 0.92 F-statistic: 2.69e+04 on 23 and 53916 DF, p-value: \u0026lt;0.0000000000000002\r# A tibble: 3 x 3\r.metric .estimator .estimate\r\u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r1 rmse standard 1130. 2 rsq standard 0.920\r3 mae standard 740. \rThe model summary shows it is an overfitted model. Among other things, we know that depth and table have no impact on price, yet these are shown to be highly significant. Root Mean Squared Error (rmse) and other metrics are also shown above.\nLet’s make a plot of actual v/s predicted prices to visualize how well this base model performs.\nIf the predictions are good, the points should lie close to a straight line drawn at 45 degrees. We can see this base model does a poor job of predicting prices. Worst of all, the model predicts negative prices on the lower end.\rIt shows that price has to be log transformed to avoid these absurdities.\nFeature Engineering\rWe know the price of a diamond is strongly correlated with its size. All things equal, the larger the diamond, the greater its price.\nAs a first approximation, we can assume a diamond is a cuboid with dimensions x, y and z. Then, we can compute its volume as x * y * z.\rAs these 3 dimensions are highly correlated, we can compute a geometrical average dimension by taking the cube root of volume, and retain a linear relationship with log(price).\nAnother way to calculate an average dimension is by using high school chemistry. Mass, volume and density are related to each other by the equation:\n$ density = mass/volume $\nWe can find out that 1 carat = 0.2 gms. Dividing by the density of diamond (3.51 gms/cc) would give us its volume in cc, which could be converted to a geometrical average dimension by taking the cube root.\nEven though both methods yield similar results, we could see that the density method results in a narrower range. But which method would be more robust?\rKeep in mind there are 20 z values that are 0. In 7 of these records both x and y are 0 too, which means these values were not recorded reliably.\n# A tibble: 20 x 10\rcarat cut color clarity depth table price x y z\r\u0026lt;dbl\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 1 Premium G SI2 59.1 59 3142 6.55 6.48 0\r2 1.01 Premium H I1 58.1 59 3167 6.66 6.6 0\r3 1.1 Premium G SI2 63 59 3696 6.5 6.47 0\r4 1.01 Premium F SI2 59.2 58 3837 6.5 6.47 0\r5 1.5 Good G I1 64 61 4731 7.15 7.04 0\r6 1.07 Ideal F SI2 61.6 56 4954 0 6.62 0\r7 1 Very Good H VS2 63.3 53 5139 0 0 0\r8 1.15 Ideal G VS2 59.2 56 5564 6.88 6.83 0\r9 1.14 Fair G VS1 57.5 67 6381 0 0 0\r10 2.18 Premium H SI2 59.4 61 12631 8.49 8.45 0\r11 1.56 Ideal G VS2 62.2 54 12800 0 0 0\r12 2.25 Premium I SI1 61.3 58 15397 8.52 8.42 0\r13 1.2 Premium D VVS1 62.1 59 15686 0 0 0\r14 2.2 Premium H SI1 61.2 59 17265 8.42 8.37 0\r15 2.25 Premium H SI2 62.8 59 18034 0 0 0\r16 2.02 Premium H VS2 62.7 53 18207 8.02 7.95 0\r17 2.8 Good G SI2 63.8 58 18788 8.9 8.85 0\r18 0.71 Good F SI2 64.1 60 2130 0 0 0\r19 0.71 Good F SI2 64.1 60 2130 0 0 0\r20 1.12 Premium G I1 60.4 59 2383 6.71 6.67 0\rIn all of these records, the carat values were recorded reliably and are probably more accurate than the dimensions.\rHence, we might prefer the density method of generating this feature.\nFurthermore, since density is a constant, dividing by a constant to calculate volume isn’t really necessary. Instead, a cube root transformation could be applied to carat itself for the purposes of predictive modelling that would result in a linear relationship between \\(log(price)\\) and \\(carat^{1/3}\\).\rIt is the reason why we’re fitting a linear model because the model is linear in its parameters.\n\rTraining Linear Models\rHere are the steps for building linear models and computing metrics:\n\rPartition the dataset into training and testing sets in the proportion 75% and 25% respectively.\rSince clarity is one of the main predictors, stratify the partitioning by clarity, so both training and testing sets have the same distributions of this feature.\rFit a linear model with the training set.\rMake predictions on the testing set and determine model metrics.\rWrap all the steps above inside a function in which the model formula and a seed could be passed. Since the seed determines the random partitioning, it helps to minimize vagaries in partitioning the training and testing sets before fitting models.\rRun multiple iterations of a model with different seeds, and compute its average metrics, that would reflect the results on unseen data.\r\rHere’s a sample split of training and testing set, stratified by clarity. As we can see, the training and testing sets have similar distributions.\ndfTrain$clarity n missing distinct 40457 0 8 lowest : I1 SI2 SI1 VS2 VS1 , highest: VS2 VS1 VVS2 VVS1 IF Value I1 SI2 SI1 VS2 VS1 VVS2 VVS1 IF\rFrequency 552 6895 9826 9222 6125 3780 2722 1335\rProportion 0.014 0.170 0.243 0.228 0.151 0.093 0.067 0.033\rdfTest$clarity n missing distinct 13483 0 8 lowest : I1 SI2 SI1 VS2 VS1 , highest: VS2 VS1 VVS2 VVS1 IF Value I1 SI2 SI1 VS2 VS1 VVS2 VVS1 IF\rFrequency 189 2299 3239 3036 2046 1286 933 455\rProportion 0.014 0.171 0.240 0.225 0.152 0.095 0.069 0.034\rAfter running 5 iterations of each model with a different seed, here are the average metrics:\n# A tibble: 5 x 4\rmodel rmse rsq mae\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 log(price) ~ . 11055. 0.670 570.\r2 log(price) ~ I(carat^(1/3)) 2893. 0.687 1039.\r3 log(price) ~ I(carat^(1/3)) + clarity 2312. 0.807 881.\r4 log(price) ~ I(carat^(1/3)) + clarity + color 1870. 0.870 631.\r5 log(price) ~ I(carat^(1/3)) + clarity + color + cut 1848. 0.875 625.\rThe first model with all predictors is an overfitted one.\nThe model with carat, clarity and color provides the best combination of root mean squared error and r-squared, that explains the most variance.\rThis is our final model.\rIncluding cut in the model has diminishing benefits, and tends to overfit the data.\nHere’s the summary of our final model:\n\rCall:\rlm(formula = model_formula, data = dfTrain)\rResiduals:\rMin 1Q Median 3Q Max -1.6022 -0.1034 0.0145 0.1066 1.7941 Coefficients:\rEstimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 2.147009 0.004993 429.99 \u0026lt; 0.0000000000000002 ***\rI(carat^(1/3)) 6.246412 0.005365 1164.27 \u0026lt; 0.0000000000000002 ***\rclarity.L 0.922295 0.005036 183.15 \u0026lt; 0.0000000000000002 ***\rclarity.Q -0.295539 0.004734 -62.43 \u0026lt; 0.0000000000000002 ***\rclarity.C 0.166979 0.004068 41.05 \u0026lt; 0.0000000000000002 ***\rclarity^4 -0.068591 0.003260 -21.04 \u0026lt; 0.0000000000000002 ***\rclarity^5 0.032833 0.002669 12.30 \u0026lt; 0.0000000000000002 ***\rclarity^6 -0.001904 0.002325 -0.82 0.41288 clarity^7 0.025508 0.002049 12.45 \u0026lt; 0.0000000000000002 ***\rcolor.L -0.488882 0.002927 -167.05 \u0026lt; 0.0000000000000002 ***\rcolor.Q -0.117319 0.002680 -43.78 \u0026lt; 0.0000000000000002 ***\rcolor.C -0.012230 0.002497 -4.90 0.00000098 ***\rcolor^4 0.019007 0.002288 8.31 \u0026lt; 0.0000000000000002 ***\rcolor^5 -0.008110 0.002159 -3.76 0.00017 ***\rcolor^6 -0.000396 0.001967 -0.20 0.84055 ---\rSignif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rResidual standard error: 0.166 on 40442 degrees of freedom\rMultiple R-squared: 0.973, Adjusted R-squared: 0.973 F-statistic: 1.05e+05 on 14 and 40442 DF, p-value: \u0026lt;0.0000000000000002\rHere’s a scatterplot of actual v/s predicted log(price) from our final model on the testing set:\nThe points lie close to the 45 degress line. However, on the high end, there are many outliers where actual and predicted values have very high variance.\rNevertheless, this is as good as it gets.\n\r","date":1482278400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482278400,"objectID":"109484543c340fea538c54b7fa5b7274","permalink":"/casestudies/diamonds-part2-linear-models/","publishdate":"2016-12-21T00:00:00Z","relpermalink":"/casestudies/diamonds-part2-linear-models/","section":"casestudies","summary":"In a previous post in this series, we did an exploratory data analysis of the diamonds dataset and found that carat, x, y, z were strongly correlated with price. To some extent, clarity also appeared to provide some predictive ability.\nIn this post, we will build linear models and see how well they predict the price of diamonds.\nBefore we do any transformations, feature engineering or feature selections for our model, let’s see what kind of results we get from a base linear model, that uses all the features to predict price:","tags":["rstats","ggplot2","models","regression","linear"],"title":"Diamonds - Part 2 - A cut above - Building Linear Models","type":"casestudies"},{"authors":null,"categories":["case studies"],"content":"\rIn this case study, we will explore the diamonds dataset, then build linear and non-linear regression models to predict the price of diamonds.\nData Description\rThe diamonds dataset contains the prices in 2008 USD terms, and other attributes of almost 54,000 diamonds.\n\r\rAttribute\rDescription\r\r\r\rprice\rprice in 2008 USD\r\rcarat\rweight of a diamond (1 carat = 0.2 gms)\r\rcut\rquality of the cut (Fair, Good, Very Good, Premium, Ideal)\r\rcolor\rdiamond color from D (best) to J (worst)\r\rclarity\ra measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\r\rx\rlength in mm\r\ry\rwidth in mm\r\rz\rdepth in mm\r\rdepth\rtotal depth percentage = z/mean(x, y)\r\rtable\rwidth of the top of diamond relative to widest point\r\r\r\r\r\r\rData Summaries\rA preliminary visual summary of the whole dataset shows all the features and their types. There are no missing values (NAs) in this dataset.\nLet’s examine each feature numerically:\ndfInput 10 Variables 53940 Observations\r----------------------------------------------------------------------------------------------------------------------------------------------------------------\rprice n missing distinct Info Mean Gmd .05 .10 .25 .50 .75 .90 .95 53940 0 11602 1 3933 4012 544 646 950 2401 5324 9821 13107 lowest : 326 327 334 335 336, highest: 18803 18804 18806 18818 18823\r----------------------------------------------------------------------------------------------------------------------------------------------------------------\rcarat n missing distinct Info Mean Gmd .05 .10 .25 .50 .75 .90 .95 53940 0 273 0.999 0.7979 0.5122 0.30 0.31 0.40 0.70 1.04 1.51 1.70 lowest : 0.20 0.21 0.22 0.23 0.24, highest: 4.00 4.01 4.13 4.50 5.01\r----------------------------------------------------------------------------------------------------------------------------------------------------------------\rcut n missing distinct 53940 0 5 lowest : Fair Good Very Good Premium Ideal , highest: Fair Good Very Good Premium Ideal Value Fair Good Very Good Premium Ideal\rFrequency 1610 4906 12082 13791 21551\rProportion 0.030 0.091 0.224 0.256 0.400\r----------------------------------------------------------------------------------------------------------------------------------------------------------------\rcolor n missing distinct 53940 0 7 lowest : J I H G F, highest: H G F E D\rValue J I H G F E D\rFrequency 2808 5422 8304 11292 9542 9797 6775\rProportion 0.052 0.101 0.154 0.209 0.177 0.182 0.126\r----------------------------------------------------------------------------------------------------------------------------------------------------------------\rclarity n missing distinct 53940 0 8 lowest : I1 SI2 SI1 VS2 VS1 , highest: VS2 VS1 VVS2 VVS1 IF Value I1 SI2 SI1 VS2 VS1 VVS2 VVS1 IF\rFrequency 741 9194 13065 12258 8171 5066 3655 1790\rProportion 0.014 0.170 0.242 0.227 0.151 0.094 0.068 0.033\r----------------------------------------------------------------------------------------------------------------------------------------------------------------\rdepth n missing distinct Info Mean Gmd .05 .10 .25 .50 .75 .90 .95 53940 0 184 0.999 61.75 1.515 59.3 60.0 61.0 61.8 62.5 63.3 63.8 lowest : 43.0 44.0 50.8 51.0 52.2, highest: 72.2 72.9 73.6 78.2 79.0\r----------------------------------------------------------------------------------------------------------------------------------------------------------------\rtable n missing distinct Info Mean Gmd .05 .10 .25 .50 .75 .90 .95 53940 0 127 0.98 57.46 2.448 54 55 56 57 59 60 61 lowest : 43.0 44.0 49.0 50.0 50.1, highest: 71.0 73.0 76.0 79.0 95.0\r----------------------------------------------------------------------------------------------------------------------------------------------------------------\rx n missing distinct Info Mean Gmd .05 .10 .25 .50 .75 .90 .95 53940 0 554 1 5.731 1.276 4.29 4.36 4.71 5.70 6.54 7.31 7.66 lowest : 0.00 3.73 3.74 3.76 3.77, highest: 10.01 10.02 10.14 10.23 10.74\r----------------------------------------------------------------------------------------------------------------------------------------------------------------\ry n missing distinct Info Mean Gmd .05 .10 .25 .50 .75 .90 .95 53940 0 552 1 5.735 1.269 4.30 4.36 4.72 5.71 6.54 7.30 7.65 lowest : 0.00 3.68 3.71 3.72 3.73, highest: 10.10 10.16 10.54 31.80 58.90\rValue 0.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 10.5 32.0 59.0\rFrequency 7 5 1731 12305 7817 5994 6742 9260 4298 3402 1635 652 69 14 6 1 1 1\rProportion 0.000 0.000 0.032 0.228 0.145 0.111 0.125 0.172 0.080 0.063 0.030 0.012 0.001 0.000 0.000 0.000 0.000 0.000\rFor the frequency table, variable is rounded to the nearest 0.5\r----------------------------------------------------------------------------------------------------------------------------------------------------------------\rz n missing distinct Info Mean Gmd .05 .10 .25 .50 .75 .90 .95 53940 0 375 1 3.539 0.7901 2.65 2.69 2.91 3.53 4.04 4.52 4.73 lowest : 0.00 1.07 1.41 1.53 2.06, highest: 6.43 6.72 6.98 8.06 31.80\rValue 0.0 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 8.0 32.0\rFrequency 20 1 2 3 8807 13809 9474 13682 5525 2352 237 20 5 1 1 1\rProportion 0.000 0.000 0.000 0.000 0.163 0.256 0.176 0.254 0.102 0.044 0.004 0.000 0.000 0.000 0.000 0.000\rFor the frequency table, variable is rounded to the nearest 0.5\r----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\rprice: The average price of a diamond in this dataset is ~ USD 4000. There are many outliers on the high end.\rcarat: The average carat weight is ~ 0.8. About 75% of the diamonds are under 1 carat. The top 5 values show presence of many outliers on the high end.\rcut: About 40% of the diamonds are of Ideal cut. Only 3% are Fair cut. So there is a lot of imbalance in the categories.\rcolor: Most of the diamonds are rated E to H color. Relatively fewer are rated J color.\rclarity: Most of the diamonds are rated SI2 to VS1 clarity. About 1% are rated the worst I1 clarity, where as only ~ 3% are rated IF.\rdepth: Most of the depth values are between 60 and 64. There are outliers on both low end and high end.\rtable: Most of the table values are between 54 and 65. There are outliers on both ends.\rx: Denotes the dimension along the x-axis. Most values are between 4 and 8. There are some 0 values too which means they were not recorded.\ry: Denotes the dimension along the y-axis. Most values are between 3.5 and 8. There are 7 records where the values are 0.\rz: Denotes the dimension along the z-axis. Most values are between 2.5 and 8.5. There are 20 records where the values are 0.\r\r\rUnivariate Analysis\rLet us look at each feature in the dataset in detail.\nNumerical Features\rThe plots show presence of outliers within each feature. Let’s exclude the outliers and plot them again.\nExcluding outliers, the range of values are more reasonable. We can see that carat and price are heavily right skewed.\nLet’s plot the distribution of price in log scale:\nTwo peaks in the log transformed plot show a bimodal distribution of prices. This implies two price points of diamonds are most popular among customers -\rone at just below USD 1000 and the other around USD 5000. Intriguingly, there are no diamonds in the dataset that are around USD 1500. Hence, a big gap is visible around that price.\n\rCategorical Features\rThe categorical imbalance in cut and clarity can be clearly noticed.\n\r\rBivariate Analysis\rLet’s examine the relationship of price with other features.\nNumerical-numerical\rFirst and foremost, let’s do a correlation analysis to see how price is correlated with other numerical features:\nWe can see that price is very strongly correlated with carat, x, y, and z dimensions. If a predictive linear regression model is built,\rsome of these features would act as confounders. table and depth have almost no correlation with price so they are not so interesting for\rpredictive modelling.\nNow let’s see the scatter plots:\nAfter removing outliers, it could be noted that price increases exponentially with carat, as well as x, y and z dimensions. So price should be plotted with a log tranformation. Let’s do that:\nNow, the relationship between log(price) appears to be linear with x, y and z. But, not so much with carat. Variance in price tends to\rincrease both by carat and its dimensions. Log transforming carat wouldn’t help because carat does not have a wide range.\rWe will find ways to deal with this when we do Feature Engineering.\n\rNumerical-Categorical\rLet’s examine price with respect to the categorical features in the dataset:\nThe boxplots above are plotted with truncated price axis for better visualization of trends. All the boxplots are counter-intuitive - median prices tend to decline as we move from lowest grade to highest grade in terms of cut, color and clarity. This is very odd.\n\rThe median price declines monotonically from Fair cut to Ideal cut.\rIn terms of color, the median price decreases from J (worst) to G (mid-grade), then increases and finally decreases for D (best).\rThe median price increases when clarity improves from I1 to SI2, and then decreases monotonically to IF grade.\r\r\r\rMultivariate Analysis\rSo far, we have determined carat, x, y, and z have the strongest relationship with price. Different grades of cut, color and clarity also seem to have some impact on median price. So let’s make some scatter plots to see these relationships:\nNumerical-Numerical-Categorical\rAlthough there is a lot of overlap, but there is a clear trend of price increasing with clarity, at a given carat weight. The same pattern could also be observed in the plot with increasing grades of color, though not to the same extent. There is no evidence of any relationship between price and carat with cut.\nWe can conclude both color and clarity explain some variance in price at a given carat weight.\nTo be sure of any interaction between table and depth, with color and clarity, let’s plot these:\nThere is no pattern in the interaction of price v/s depth and table values when plotted by color and clarity. So, these features do not have any predictive ability to determine price.\n\rCategorical-Categorical-Numerical\rWe want to see if there is any interaction of clarity with cut and color, that could provide any additional explanatory power to predict price:\nThe second heatmap appears to be more interesting. From bottom left to top right, with increasing grades of color and clarity, price tends to decrease on average. Once again, this runs counter to our intuition; after all prices of diamonds with the best color and clarity should be the highest. Nevertheless this counter-trend persists in the dataset.\nWith respect to cut and clarity, the mean prices do not show any discernable pattern.\n\r\rSummary\rTo summarize, here’s what we found interesting in this dataset, after doing an exploratory data analysis:\n\rprice is heavily right-skewed, and when log tranformed, has a bimodal distribution which implies there is demand in 2 different price ranges.\rcarat about 75% of the diamonds are below 1 carat. The variance in price increases with carat weight.\rcut is imbalanced with about 40% of the diamonds rated Ideal.\rcolor is imbalanced with about 5% of the diamonds rated J.\rclarity is imbalanced at the extremes, with only 1.5% of the diamonds rated I1 and 3.3% of the diamonds rated IF.\rprice is strongly correlated with carat and x, y, z dimensions of the diamonds. table and depth have almost no correlation with price.\rBoth clarity and color appear to explain some variance in price for a given carat weight.\r\r\r","date":1482192000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482192000,"objectID":"2d0610bfb55620bf0b219f1c3f9d09ae","permalink":"/casestudies/diamonds-part1-eda/","publishdate":"2016-12-20T00:00:00Z","relpermalink":"/casestudies/diamonds-part1-eda/","section":"casestudies","summary":"In this case study, we will explore the diamonds dataset, then build linear and non-linear regression models to predict the price of diamonds.\nData Description\rThe diamonds dataset contains the prices in 2008 USD terms, and other attributes of almost 54,000 diamonds.\n\r\rAttribute\rDescription\r\r\r\rprice\rprice in 2008 USD\r\rcarat\rweight of a diamond (1 carat = 0.2 gms)\r\rcut\rquality of the cut (Fair, Good, Very Good, Premium, Ideal)\r\rcolor\rdiamond color from D (best) to J (worst)\r\rclarity\ra measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\r\rx\rlength in mm\r\ry\rwidth in mm\r\rz\rdepth in mm\r\rdepth\rtotal depth percentage = z/mean(x, y)\r\rtable\rwidth of the top of diamond relative to widest point\r\r\r\r\r\r\rData Summaries\rA preliminary visual summary of the whole dataset shows all the features and their types.","tags":["rstats","ggplot2","eda"],"title":"Diamonds - Part 1 - In the rough - An Exploratory Data Analysis","type":"casestudies"},{"authors":null,"categories":["investing"],"content":"\rIt is important to understand the building blocks of systematic investing strategies before learning how to build them. Here is a schematic from the book, Inside the Black Box - The Simple Truth about Quantitative Trading by Rishi Narang, that provides a good way to visualize these building blocks and how they fit together in a system.\n\rThe Black Box Revealed\n\r\rThe author had aptly titled it - The Black Box Revealed, since that is how most people perceive it to be from the outside. From this schematic, it is easier to understand and visualize how a system is built with a systematic approach.\nHere are the components of the system:\n\rData - Encapsulates the processes of raw data collection, cleaning and preparation in tidy form to be used by the models.\rAlpha Model - Describes the core strategy, product universe (stocks, ETFs, futures) and the rules to be implemented.\rRisk Model - Describes position sizes or how capital/risk is allocated. This is just as much important as the alpha model itself.\rTransaction Cost Model - Describes the transaction costs by product. These may be theoretical numbers while backtesting the strategy using historical data and later adjusted based on live execution data.\rPortfolio Construction Model - Describes how a portfolio is constructed after putting together positions by product across one or more rules of the core strategy.\rExecution Model - Encapsulates a live trading system. Describes the tactical implementation of the core strategy.\rResearch - Sound research forms the underpinning of all models and is driven by quantitative and statistical analysis.\r\rThe interactions shown are typical, although there could be more depending upon the implementation. More than one model could also be combined together, depending upon how the strategy is formulated. Nonetheless, for most practitioners looking to build a complete system from scratch, this is a pretty good blueprint.\n","date":1469145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1469145600,"objectID":"fd112e2ef06c1881da9eae390dbe51e1","permalink":"/investing/building-blocks-of-investment-strategies/","publishdate":"2016-07-22T00:00:00Z","relpermalink":"/investing/building-blocks-of-investment-strategies/","section":"investing","summary":"It is important to understand the building blocks of systematic investing strategies before learning how to build them. Here is a schematic from the book, Inside the Black Box - The Simple Truth about Quantitative Trading by Rishi Narang, that provides a good way to visualize these building blocks and how they fit together in a system.\n\rThe Black Box Revealed\n\r\rThe author had aptly titled it - The Black Box Revealed, since that is how most people perceive it to be from the outside.","tags":["models","schematic"],"title":"Building blocks of systematic investment strategies","type":"investing"}]