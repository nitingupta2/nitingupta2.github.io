<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>rstats | Nitin Gupta</title>
    <link>/tags/rstats/</link>
      <atom:link href="/tags/rstats/index.xml" rel="self" type="application/rss+xml" />
    <description>rstats</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Nitin Gupta. All Rights Reserved.</copyright><lastBuildDate>Mon, 15 Oct 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>rstats</title>
      <link>/tags/rstats/</link>
    </image>
    
    <item>
      <title>How Much Do Swimmers Improve in a Season?</title>
      <link>/post/how-much-do-swimmers-improve-in-a-season/</link>
      <pubDate>Mon, 15 Oct 2018 00:00:00 +0000</pubDate>
      <guid>/post/how-much-do-swimmers-improve-in-a-season/</guid>
      <description>


&lt;hr&gt;
&lt;p&gt;In my &lt;a href=&#34;../a-deep-dive-in-analyzing-swimming-data/&#34;&gt;previous post&lt;/a&gt;, we’ve seen how kids improve year-by-year in swimming. Certainly kids get stronger and faster as they grow. But how much impact does practice and coaching have? As I mentioned in my previous post, coaches do not keep a record of attendance during practice. There are daily hour long practice sessions during the summers. But even though it isn’t known how regular the kids are in attending practice, the number of meets that a kid participates in during the season, can be a good proxy. Afterall, it’s highly unlikely that a kid participates in meets while mostly skipping practice.&lt;/p&gt;
&lt;p&gt;To judge improvement in a stroke, we’d like to see whether a kid got enough opportunities to practice as well as participate in meets. The summer league lasts for about 2 months. So, a reasonable start would be to take the data of kids whose first and last meet participations in a stroke were spaced at least 4 weeks apart.&lt;/p&gt;
&lt;p&gt;Let’s see the distribution of the number of meets those kids participate in a season:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/how-much-do-swimmers-improve-in-a-season/index_files/figure-html/number_of_meets-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Seems like, for every stroke, most kids participate in 4 or more meets in a season. So we’ll select it as a threshold to judge improvement. Then, for each kid who participates in 4 or more meets in a season, the improvement will be the difference between the first and the last swim times of the season.&lt;/p&gt;
&lt;div id=&#34;estimating-improvement&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimating Improvement&lt;/h2&gt;
&lt;p&gt;Here are the distributions of improvement in swim times by age:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/how-much-do-swimmers-improve-in-a-season/index_files/figure-html/improvement_by_age-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see wide variance among 6 to 10 year olds. But as the kids grow older, the variance reduces.&lt;/p&gt;
&lt;p&gt;In Breaststroke and Butterfly, there aren’t any 6 year old boys who satisfy the criteria for inclusion. So, instead of looking at improvement by age, it would be better to categorize it by age groups. We could classify kids into 3 age groups, younger kids (6-10), pre-teens and early teens (11-14) and mid to late teens (15-18).&lt;/p&gt;
&lt;p&gt;Let’s see a similar plot by age groups:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/how-much-do-swimmers-improve-in-a-season/index_files/figure-html/improvement_by_age_groups-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;is-improvement-significant&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Is Improvement Significant?&lt;/h2&gt;
&lt;p&gt;Now that we know the distribution of improvement in swim times, the question arises, is it statistically significant? In other words, does practice and coaching have a significant effect within a short season? It is hard to tell from the plot of distributions itself. Within each age group and stroke, we have a paired sample of swim times, where for the same swimmer, we know their first and last swim times of a season. By using statistical tests, we could quantify mean improvements and determine whether or not they are significant.&lt;/p&gt;
&lt;div id=&#34;frequentist-method&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Frequentist Method&lt;/h3&gt;
&lt;p&gt;In traditional statistics, a statistical test known as dependent-samples t test can be used to evaluate whether there is a significant difference between the means of first and last swim times of a season. The null hypothesis is that any change in mean swim times is due to random chance.&lt;/p&gt;
&lt;p&gt;Let’s run this test on the sample of Freestyle swim times of 15-18 year old boys.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
    Paired t-test

data:  first_time_of_season and last_time_of_season
t = 1.8, df = 31, p-value = 0.08
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.03343  0.52031
sample estimates:
mean of the differences 
                 0.2434 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mean improvement is about 0.24 seconds, but the 95% confidence interval includes 0, which means we cannot reject the null hypothesis.&lt;/p&gt;
&lt;p&gt;Let’s look at the mean improvement and 95% confidence intervals for all the age groups, plotted below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/how-much-do-swimmers-improve-in-a-season/index_files/figure-html/t_test-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at the plot, we could say girls of all age groups show a statistically significant improvement in swim times in all the strokes. But for boys, the picture is not so clear cut. Boys aged 6-10 and 11-14 show all around statistically significant improvement. But, as seen before, in Freestyle and also in Butterfly, boys aged 15-18 do not. In both the latter cases, confidence intervals include zero. Point estimates are shown with a hollow circle.&lt;/p&gt;
&lt;p&gt;But look closely at the p values. Even though the improvements in Backstroke and Breaststroke among 15-18 year old boys appear to be statistically significant, it is only because of the arbitrary choice of 95% confidence level, which has become a de facto standard in published research. With p values of 0.01 and 0.028, we wouldn’t reject the null hypothesis under a 99% confidence level.&lt;/p&gt;
&lt;p&gt;If this were a study to be published by a researcher in an academic journal, there would be a temptation to run multiple experiments with different filter criteria and attempt to produce final results according to their own bias; a.k.a. &lt;em&gt;p-hacking&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Under the null hypothesis, we’d draw the conclusion that 15-18 year old boys do not show any statistically significant improvement in a season. We might interpret that the hard work they put in practice is barely enough to maintain their time during a season. With a biased view, we might even question whether they’re putting in as much hard work as the girls of their age.&lt;/p&gt;
&lt;p&gt;But, would either of these conclusions be correct? Afterall, girls of all age groups and most boys show statistically significant improvement. So, why would 15-18 year old boys be any different?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian-method&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bayesian Method&lt;/h3&gt;
&lt;p&gt;This is where a Bayesian approach works best. Instead of determining whether the mean difference between the first and last swim times of the season is zero, which is uninformative, the Bayesian way is to determine how much have mean swim times improved, with associated probabilities.&lt;/p&gt;
&lt;p&gt;Let’s see the results from a Bayesian counterpart to the t test, on a sample of Freestyle swim times of 15-18 year old boys.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
    Bayesian estimation supersedes the t test (BEST) - paired samples

data: first_time_of_season and last_time_of_season, n = 32

  Estimates [95% credible interval]
mean paired difference: 0.24 [-0.026, 0.50]
sd of the paired differences: 0.70 [0.45, 0.96]

The mean difference is more than 0 by a probability of 0.96 
and less than 0 by a probability of 0.04 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instead of p-values, this Bayesian test provides an actual probability of improvement in swim times. We get estimates for mean and standard deviation of swim time improvements, along with credible or high density intervals. The probability that the mean improvement is more than zero is 95.9%, though the estimate is less precise and the 95% confidence interval includes zero.&lt;/p&gt;
&lt;p&gt;Let’s run the Bayesian test for all strokes and age groups in both genders:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/how-much-do-swimmers-improve-in-a-season/index_files/figure-html/bayes_t_test_plot-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at the distributions, we can see that 15-18 year old boys show a very small improvement in their Freestyle and Butterfly times, but the estimate is less precise and the confidence intervals include zero (those distributions are shaded with a light blue color). Nevertheless the probability of mean improvement being greater than zero is in the high 90s, which from a practical standpoint, is convincing enough.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;With a reasonable set of filters, we created sample datasets by age groups to determine whether kids show significant improvement in swim times within a short summer season. We applied Bayesian testing which turns out to be more informative than traditional statistical testing. For all intents and purposes, we can conclude that kids of all age groups show improvement in their swim times, even within a short summer season.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;The R markdown file with code for this post is available &lt;a href=&#34;https://github.com/nitingupta2/nitingupta2.github.io/blob/master/post/how-much-do-swimmers-improve-in-a-season/&#34;&gt;here&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Deep Dive in Analyzing Swimming Data</title>
      <link>/post/a-deep-dive-in-analyzing-swimming-data/</link>
      <pubDate>Sun, 30 Sep 2018 00:00:00 +0000</pubDate>
      <guid>/post/a-deep-dive-in-analyzing-swimming-data/</guid>
      <description>


&lt;hr&gt;
&lt;p&gt;This past summer, my daughter joined a swim team. Her team is among several teams that are part of an area league. All summer long, developmental and competitive meets are organized where teams compete on a one-to-one basis. All kids in a team swim in developmental meets. For competitive meets, top 3 or 4 kids are chosen by age groups. Meets are organized in community pools that are either 25 meters or 25 yards in length. The team schedules are packed with practices and events which are intense but thoroughly enjoyable.&lt;/p&gt;
&lt;p&gt;This is one sport where progress can be measured on almost daily basis. While most young kids take it up as a hobby, older kids seem to be very competitive and quite mindful of their swim times. Besides her own results this year, I was curious to know how my daughter did compared to a larger cohort of kids her own age. And what could we expect, if she keeps at it in the years to come?&lt;/p&gt;
&lt;p&gt;Luckily, I found past several years of her team’s results to analyze. Gathering this data, cleaning and transforming it for analysis was a huge challenge onto itself. Not something I intend to discuss in this post. But I digress.&lt;/p&gt;
&lt;p&gt;Let’s take a look at the number of kids by age and gender in this dataset.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a-deep-dive-in-analyzing-swimming-data/index_files/figure-html/boys_girls-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The pyramid plot above shows the number of competitors from ages 6 to 18 throughout the years. Seems like 6 to 8 are the most popular ages for kids to begin competitive swimming. There are some late joiners by age 10, after which we see a steady decline in the number of kids competing.&lt;/p&gt;
&lt;p&gt;Let’s take a look at the ages when swimmers compete in each stroke.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a-deep-dive-in-analyzing-swimming-data/index_files/figure-html/first_competed-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can observe a few things here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Everyone starts with Freestyle swimming. The participation rate is close to 100% for both genders. But towards the late teens, some boys seem to give up Freestyle to specialize in other strokes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Girls seem to learn Backstroke a little faster than boys, and a wide majority of girls keep swimming Backstroke until their late teens.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Girls catch up with learning Breaststroke much more quickly than boys, and a majority of them keep at it until their late teens, where as some boys choose to specialize in either Backstroke or Butterfly.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Learning Butterfly appears to be a much bigger challenge for both genders. Relatively fewer kids compete in it early on. Boys in their early teens appear to participate at a faster clip than girls. But towards the late teens, girls appear to participate more consistently.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;steep-learning-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Steep Learning Curve&lt;/h2&gt;
&lt;p&gt;There’s a steep learning curve in swimming. Each stroke has its own level of difficulty and challenges but once the basics are learned, rapid progress could be made. However, there’s a huge difference between knowing how to swim a stroke, and swimming it well. It requires both speed and precision.&lt;/p&gt;
&lt;p&gt;Swimming also happens to be one of the most unforgiving sports. In other sports, minor violations result in fouls or penalties; disqualifications or ejections are rare and usually result from violent or reckless behavior. But in swimming, the moment a competitor violates a rule, it results in disqualification. Even in summer leagues, the swim officials are trained and expected to follow USA Swimming standards. So, a 6 year old is to be judged the same way as an adult at USA Swimming events.&lt;/p&gt;
&lt;p&gt;To get an idea, here are some ways of getting disqualified:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Freestyle: As the name suggests, everything is legal except the following:
&lt;ul&gt;
&lt;li&gt;Walking on the bottom of the pool.&lt;/li&gt;
&lt;li&gt;Stopping and pushing off of the bottom of the pool.&lt;/li&gt;
&lt;li&gt;Pulling on a lane line for an assist.&lt;/li&gt;
&lt;li&gt;Not touching the wall before turning or at finish.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Backstroke
&lt;ul&gt;
&lt;li&gt;Not swimming on back off the wall.&lt;/li&gt;
&lt;li&gt;Delaying initiating a turn.&lt;/li&gt;
&lt;li&gt;Pulling on a lane line for an assist.&lt;/li&gt;
&lt;li&gt;Not touching the wall before turning or at finish.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Breaststroke
&lt;ul&gt;
&lt;li&gt;Pulling hands beyond hips.&lt;/li&gt;
&lt;li&gt;Non-simultaneous or single hand touch at the turn or finish.&lt;/li&gt;
&lt;li&gt;Doing alternating kicks (as in Freestyle) or dolphin kicks (as in Butterfly).&lt;/li&gt;
&lt;li&gt;Not being on breast after leaving the wall.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Butterfly
&lt;ul&gt;
&lt;li&gt;Arms underwater during the recovery phase of stroke.&lt;/li&gt;
&lt;li&gt;Non-simulatenous arm movements during strokes.&lt;/li&gt;
&lt;li&gt;Non-simultaneous or single hand touch at the turn or finish.&lt;/li&gt;
&lt;li&gt;Doing alternating kicks (as in Freestyle) or Breaststroke kicks.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;From what I observed throughout the season, in a majority of cases, rule violations are not deliberate. They happen inadvertently.&lt;/p&gt;
&lt;p&gt;Let’s take a look at the total number of disqualifications by age, gender and stroke in the dataset:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a-deep-dive-in-analyzing-swimming-data/index_files/figure-html/dqs-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The absolute numbers of DQs are a proxy for the level of difficulty of each stroke. Most kids learn each stroke in this order from Freestyle to Butterfly. However, as seen in the previous plot, the level of participation in Butterfly is far less as compared to other strokes. For instance, the level of participation in Freestyle and Backstroke is consistently at ~ 80% or above. About 80% of 7 year olds attempt to swim Breaststroke, where as only ~ 33% of 7 year old boys and ~ 48% of 7 year old girls attempt Butterfly in comparison. Hence the absolute numbers of disqualifications merely reflect the total number of unique competitors at that age.&lt;/p&gt;
&lt;p&gt;A better way to look at this is to compute the number of DQs per person, as shown below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a-deep-dive-in-analyzing-swimming-data/index_files/figure-html/dqs_per_person-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, we can see the number of disqualifications per person reduce as the kids get better with age.
In Freestyle, Backstroke and Breaststroke races, kids upto 8 years swim only 1 lap. In Butterfly, kids upto 10 years swim only 1 lap.
In all other age brackets, kids swim 2 laps, which means they turn after touching the opposite end of the pool. Learning to execute a legal turn without compromising pace is both an art and a science. We can see its challenges in Backstroke at age 9 and in Butterfly at age 11, when the kids start swimming 2 laps in a race. The number of disqualifications per person increase after decreasing upto the prior year when the kids are swimming only 1 lap.&lt;/p&gt;
&lt;p&gt;Consistently swimming a legal backstroke appears to remain a challenge until the mid-teens. It is indicative of the failures in executing legal turns. Backstroke could be thought of as swimming Freestyle on the back. Arm and leg movements have no constraints as long as the body is on the back. But the biggest challenge is the swimmers cannot see the wall in the direction they are swimming. Their eyes are always facing up. They train to spot a row of flags hung 5 meters from the wall over the pool and count the number of strokes they need to reach the wall from that point. While some inexperienced swimmers touch the wall with their hands and push off again on the back, it compromises their speed. The quickest way to turn is to flip on the belly while approaching a turn and push off against the wall with the feet so that the body regains its position on the back. This needs to be done in one smooth series of motions. As soon as the body flips on the belly, only a single or double hand pull is allowed to initiate the turn. If the swimmer does multiple strokes, they’re disqualified. If the swimmer turns too soon, it’s likely they’ll miss touching the wall. No sculling is allowed to reach the wall. Therefore, again a swimmer gets disqualified. We can see these mistakes happen well into the teenage years.&lt;/p&gt;
&lt;p&gt;For every boy competing in Breaststroke at age 6, there has been more than 1 disqualification, which means 6 year old boys face multiple disqualifications in a season. Their level of participation is at 40%. It shows how challenging it is to learn Breaststroke. 6 year old girls appear to do a bit better. The good thing is that kids are persistent in learning Breaststroke from an early age. So, not only does the level of participation increase with age, but also the number of disqualifications per person decrease dramatically. But again, at age 9 when the kids start swimming 2 laps, the rate of improvement slows down. Other than the Breaststroke technique, kids have to remember to touch the wall with both hands simultaneously and then turn back on their breast. Most kids become very good with practice, but even a slight lapse of concentration results in disqualification.&lt;/p&gt;
&lt;p&gt;Similarly, Butterfly is a bigger challenge in the early years. With every stroke, the arms have to break the surface of the water during recovery. Many young swimmers find it hard to have the upper body strength to do it consistently. As the kids grow, their strength improves and it becomes less hard.
Once again, we see a surge of DQs at age 11, when the kids start swimming 2 laps. As with Breaststroke, kids have to remember to touch the wall with both hands simultaneously and then turn back without doing any flutter or alternate kicks.&lt;/p&gt;
&lt;p&gt;Overall, girls seem to improve more consistently than boys.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;swim-times&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Swim Times&lt;/h2&gt;
&lt;p&gt;Now, here is what I was most curious about before doing this analysis. Shown below are the distributions of swim times by age, gender and stroke. I have excluded the races in yards to focus only on the races in meters, which is the international standard.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a-deep-dive-in-analyzing-swimming-data/index_files/figure-html/distribution_swim_times-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To provide a reference point, I have added the current 50 Short Course Meters world records for each stroke and gender. As expected, the distributions of swim times are heavily skewed towards the right. In the plots, I have truncated the outliers exceeding 80 seconds.&lt;/p&gt;
&lt;p&gt;On average, we see consistent improvement with age across the board. In nearly every plot, we see multi-modal distributions with multiple peaks that separate the best swimmers from merely good and the rest of the pack. As the kids progress to swimming 50 meters, i.e. 2 laps of the 25m length, the distributions during the early years once again become wider with heavier skews and fatter right tails. Not only does the difference in swim speed, but also the time taken to turn contributes to a large variance in swim times.&lt;/p&gt;
&lt;p&gt;The variance reduces in later years and the distributions tend to become more normal. I reckon this is due to two reasons:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Kids get stronger with age and better with practice.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A survivorship bias comes in play. As we’ve seen in the first plot, the number of competitors decrease with age. Basically the kids who have consistently done well are the ones who continue to swim beyond the early years. There might be some kids who swim just for fun or to hang out with their friends. But chances are, those who cannot compete effectively in any stroke would drop out.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Nevertheless, we see higher variance in strokes that are more technical, i.e. Backstroke, Breaststroke and Butterfly, as compared to Freestyle.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mean-times&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mean Times&lt;/h2&gt;
&lt;p&gt;Looking at the plots of distributions, we can observe a very interesting thing as the kids grow older. Note, all the plots are drawn to the same scale. But the distributions of the boys’ swim times curve more to the left than those of girls. It is evident that on average, boys continue to improve well into the late teens, where as the rate of improvement of girls slows down. Why does that happen?&lt;/p&gt;
&lt;p&gt;Let’s see the mean times to swim 50 Short Course Meters by age.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a-deep-dive-in-analyzing-swimming-data/index_files/figure-html/mean_swim_times-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What’s really interesting is that on average, boys and girls are doing just about the same until age 12. Thereafter, boys start pulling apart. Why? Do teenage boys work harder than teenage girls to improve? Hardly the case. On the contrary, we have seen evidence that girls learn and improve more consistently than boys. So what’s happening here? I am no expert in biology, but this is the about the age where puberty begins. Among other things, boys tend to grow taller and more muscular than girls at this stage. No wonder they start swimming faster than girls!
This might be among the few instances where the impact of puberty on both sexes, could be measurably observed.&lt;/p&gt;
&lt;p&gt;In both genders, the trajectory of improvement is non-linear with age. Kids show dramatic improvement until they reach their teens. Thereafter, the rate of improvement slows down. There’s a physical limit to how fast a human could swim, so we could expect the curves to reach their minima at some point during the 20s - 30s when the swimmers reach the pinnacle of physical strength and performance. When the swimmers are past their prime, we could expect them to start slowing down.&lt;/p&gt;
&lt;p&gt;USA Swimming has race data for swimmers ranging from ages 5 to 50. I sampled data for 50 Short Course Meters Freestyle races to explore.&lt;/p&gt;
&lt;p&gt;Here are the curves drawn using USA Swimming data:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a-deep-dive-in-analyzing-swimming-data/index_files/figure-html/mean_swim_times_USA_Swimming-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The USA Swimming sample data produces a similar set of curves. The effects of puberty on average swim times are exhibited in this dataset too. Boys and girls have nearly the same average swim times until age 12, but from age 13 onwards, boys start swimming faster than girls.&lt;/p&gt;
&lt;p&gt;Another interesting observation is that after age 18, the rate of improvement accelerates once again for both male and female swimmers. Both curves show a big shift towards the left. Yet again, survivorship bias appears to come into play. These aren’t recreational swimmers. Rather they are kids who continue to swim in college teams, and then compete in national and international events. Superior coaching and facilities at the top level, probably are big factors contributing to this dramatic improvement.&lt;/p&gt;
&lt;p&gt;Judging by the averages, male swimmers reach their prime in their early 20s and maintain their peak form until their early 30s. In comparison, female swimmers do not hit their prime until their late 20s, and have a much shorter peak span than males.&lt;/p&gt;
&lt;p&gt;The average times start deteriorating by the mid 30s for both males and females. Unfortunately, the data is very sparse for swimmers 35 years and older. To reduce noise, I excluded the data for every age where there are fewer than 10 records. So it’s hard to see the trajectory well into the 40s.&lt;/p&gt;
&lt;p&gt;Here are plots of all swim times in this dataset with best fitting curves for both genders:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a-deep-dive-in-analyzing-swimming-data/index_files/figure-html/improvement_curves-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;From this study, we gain some interesting insights into the world of competitive swimming from the early years:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Most kids join swim teams between 6 and 8. More and more kids drop out during the teenage years.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;After Freestyle, kids learn to swim Backstroke before they learn Breaststroke and Butterfly. Butterfly remains a challenging stroke to learn well into the teens.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Persistence pays off and kids who don’t give up improve by leaps and bounds from 6 to 10.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Boys and girls improve at the same rate until the onset of puberty. Then boys start gaining a big advantage.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On average, boys reach peak performance in the early 20s itself. On the flip side, they require all the hard work just to maintain it until the 30s. Girls reach peak performance only by the late 20s. On the brighter side, this is a big motivation to continue working hard well into adulthood.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;The R markdown file with code for this post is available &lt;a href=&#34;https://github.com/nitingupta2/nitingupta2.github.io/blob/master/post/a-deep-dive-in-analyzing-swimming-data/&#34;&gt;here&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How would you bet? Lessons from a Biased Coin Flipping Experiment</title>
      <link>/post/biased-coin-flipping-experiment/</link>
      <pubDate>Thu, 03 Aug 2017 00:00:00 +0000</pubDate>
      <guid>/post/biased-coin-flipping-experiment/</guid>
      <description>


&lt;p&gt;Recently I listened to a podcast featuring Victor Haghani of &lt;a href=&#34;https://elmfunds.com/&#34;&gt;Elm Partners&lt;/a&gt;, who described a fascinating coin-flipping experiment. The experiment was designed to be played for 30 minutes by participants in groups of 2-15 in university classrooms or office conference rooms, without consulting each other or the internet or other resources. To conduct the experiment, a &lt;a href=&#34;http://coinflipbet.herokuapp.com/&#34;&gt;custom web app&lt;/a&gt; was built for placing bets on a simulated coin with a 60% chance of coming up heads. Participants used their personal laptops or work computers to play. They were offered a stake of &lt;span class=&#34;math inline&#34;&gt;\(\$25\)&lt;/span&gt; to begin, with a promise of a check worth the final balance (subject to a caveat) in their game account. No out-of-pocket betting or any gimmicks. But they had to agree to remain in the room for 30 minutes.&lt;/p&gt;
&lt;p&gt;Even though the experiment in no longer active, the &lt;a href=&#34;http://coinflipbet.herokuapp.com/&#34;&gt;game URL&lt;/a&gt; is still active. Before reading any further, I strongly encourage readers to play the game and decide how they would bet and what outcomes they should expect? &lt;strong&gt;&lt;em&gt;SPOILERS AHEAD&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;div id=&#34;what-would-be-an-optimal-betting-strategy&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What would be an optimal betting strategy?&lt;/h3&gt;
&lt;p&gt;The details of the experiment are described in a &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2856963&#34;&gt;paper&lt;/a&gt; that would be very accessible to anyone with a rudimentary understanding of probability.&lt;/p&gt;
&lt;p&gt;The probability of heads is reported to be 60%, which means that if the game’s virtual coin is flipped sufficiently large number of times, the number of heads should converge to 60% of the total outcomes. Since each flip is independent of all prior flips, there is a positive expectancy in betting on heads only. The expected gain in betting an amount &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; on heads is &lt;span class=&#34;math inline&#34;&gt;\(0.6x - 0.4x = 0.2x\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;But how should one bet? Intuitively it is apparent that betting too large would be counter-productive because if we lose, the balance would deplete quickly. A string of consecutive losses would quickly lead to bankruptcy. On the other hand, betting too little isn’t going to be very productive either, even though we know the odds of betting on heads are favorable.&lt;/p&gt;
&lt;p&gt;So, somewhere along the spectrum between betting too little or too large, there is an optimal betting strategy. In 1955 John Kelly working at Bell Labs published a formula that showed how to maximize wealth while betting on games with favorable odds. Even though this formula has been known to professional gamblers for decades, to my knowledge, it is not a part of any high school or undergraduate curriculum on probability and statistics. I know this personally, having a background in science and engineering.&lt;/p&gt;
&lt;p&gt;When the Kelly formula is applied to a game like this with binary (heads/tails) outcomes, it suggests betting a constant fraction of the existing balance, denoted by &lt;span class=&#34;math inline&#34;&gt;\(2*p - 1\)&lt;/span&gt;, where p is the probability of the favorable outcome. Clearly, the outcome would be favorable only if p &amp;gt; 0.5.&lt;/p&gt;
&lt;p&gt;Given that the probability of heads in this game is 0.6, the optimal bet size is &lt;span class=&#34;math inline&#34;&gt;\(2*0.6-1 = 0.2\)&lt;/span&gt; or 20% of the existing balance at each flip. This optimality could be demonstrated by simulation.&lt;/p&gt;
&lt;p&gt;In the original experiment, 61 participants flipped virtual coins 7253 times. So during the course of a 30 min game,
a virtual coin was flipped ~ 120 times on average.&lt;/p&gt;
&lt;p&gt;I generated a sample set of 1000 games. In each game, a virtual coin is flipped 120 times with a 0.6 probability of getting heads.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/biased-coin-flipping-experiment/index_files/figure-html/final_balance_distributions-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Assuming there are no constraints in the game and the upside is unbounded, the boxplots show the range of outcomes resulting from a constant percentage bet on heads on each flip.&lt;/p&gt;
&lt;p&gt;Few things stand out from this plot:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Since the lowest balance could only be 0 and the maximum is unbounded, the median provides a better measure of the central tendency of the betting outcomes. The mean is highly skewed by extreme outcomes on the upside. The median provides a balance between those who got extremely lucky and those who weren’t so much. The median balance is the highest while maintaining a constant 20% bet, which is exactly what the Kelly formula suggests.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you played the game and didn’t discover the maximum payout, it should be very evident looking at this plot that the game can’t be offered practically without an upper limit. In this sample, the final balance reaches ~ &lt;span class=&#34;math inline&#34;&gt;\(\$55\)&lt;/span&gt; million in a game with 120 flips betting 50% on heads, even though this would be far from an ideal betting strategy. In theory, if someone got extremely lucky they could flip 120 heads in succession and bet 100% each time to get a final balance of &lt;span class=&#34;math inline&#34;&gt;\(25*1.2^{120} = \$79.4\)&lt;/span&gt; billion; although this would be extremely unlikely.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;While betting 40% of the balance, odds are low that you would make more than the starting balance of &lt;span class=&#34;math inline&#34;&gt;\(\$25\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;While betting 60% or more of the balance, you are almost guaranteed to end up below the starting balance of &lt;span class=&#34;math inline&#34;&gt;\(\$25\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;While betting 80% or more of the balance, you are almost guaranteed to go bankrupt&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The plots of median final balance and the percentage of games which lead to bankruptcy at each constant proportional bet are shown below, These plots make it clear why the Kelly formula provides the optimal betting strategy. The risk with respect to reward is well-balanced.&lt;/p&gt;
&lt;p&gt;The Kelly formula assumes log utility of wealth. With a constant proportional bet of 20% with a 60% chance of heads, the expected utility for each flip is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(0.6*log(1.2) + 0.4*log(0.8) = 0.020136\)&lt;/span&gt;, which means that each flip gives a dollar equivalent increase in utility of &lt;span class=&#34;math inline&#34;&gt;\(exp(0.020136)\)&lt;/span&gt; =1.0203401 or ~ 2%&lt;/p&gt;
&lt;p&gt;So, 120 flips starting at &lt;span class=&#34;math inline&#34;&gt;\(\$25\)&lt;/span&gt;, leads to &lt;span class=&#34;math inline&#34;&gt;\(\$25*(1.0203401^{120}) = \$280.1\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Alternatively, in 120 flips, if 60% or 72 were heads, the median outcome could be calculated by &lt;span class=&#34;math inline&#34;&gt;\(\$25*(1.2^{72})*(0.8^{48}) = \$280.1\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Both of these values correspond to the peak of the median final balance determined from simulation.
The median values for other constant proportional bets could be calculated in the same manner.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/biased-coin-flipping-experiment/index_files/figure-html/median_plots-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;!-- Furthermore, we can demonstrate that the optimal bet level is only dependent upon the favorability of odds. It is independent of the starting balance and the number of flips. --&gt;
&lt;!-- ```{r Kelly_criterion, cache=TRUE, results=&#39;hide&#39;} --&gt;
&lt;!-- dfParam &lt;- crossing(InitialBalance = c(25L, 50L), --&gt;
&lt;!--                     NumFlips = c(20L, 50L, 80L, 120L), --&gt;
&lt;!--                     ProbHeads = c(0.6, 0.65)) --&gt;
&lt;!-- ifileKelly &lt;- &#34;C:/Backups/Website/nitingupta2.github.io/content/post/final_balance_Kelly.rds&#34; --&gt;
&lt;!-- if(file.exists(ifileKelly)) { --&gt;
&lt;!--     dfFinalBalance &lt;- read_rds(ifileKelly) --&gt;
&lt;!-- } else { --&gt;
&lt;!--     dfFinalBalance &lt;- NULL --&gt;
&lt;!--     for(i in 1:nrow(dfParam)) { --&gt;
&lt;!--         cat(&#34;\nGetting final profits for param combination: &#34;, i) --&gt;
&lt;!--         initial_balance &lt;- dfParam[[&#34;InitialBalance&#34;]][i] --&gt;
&lt;!--         n_flips &lt;- dfParam[[&#34;NumFlips&#34;]][i] --&gt;
&lt;!--         p_heads &lt;- dfParam[[&#34;ProbHeads&#34;]][i] --&gt;
&lt;!--         set.seed(100) --&gt;
&lt;!--         lCoinFlips &lt;- rerun(n_games, getCoinFlips(n_flips, p_heads)) %&gt;% set_names(paste0(&#34;Game_&#34;, 1:n_games)) --&gt;
&lt;!--         lBalanceSeries &lt;- map(vBetFraction, .f = ~ getTotalBalanceSeries(lCoinFlips, initial_balance, .x)) %&gt;%  --&gt;
&lt;!--             set_names(paste0(&#34;Bet_&#34;, vBetFraction*100)) --&gt;
&lt;!--         dfFinalBalance &lt;- map_df(lBalanceSeries, .f = ~ .x[1L+n_flips,]) %&gt;%  --&gt;
&lt;!--             mutate(BetFraction = vBetFraction, --&gt;
&lt;!--                    InitialBalance = initial_balance, --&gt;
&lt;!--                    NumFlips = n_flips, --&gt;
&lt;!--                    ProbHeads = p_heads) %&gt;%  --&gt;
&lt;!--             bind_rows(dfFinalBalance) --&gt;
&lt;!--     } --&gt;
&lt;!--     dfFinalBalance &lt;- dfFinalBalance %&gt;% select(InitialBalance, NumFlips, ProbHeads, BetFraction, everything()) --&gt;
&lt;!--     write_rds(dfFinalBalance, ifileKelly) --&gt;
&lt;!-- } --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r plot_optimal, fig.align=&#39;center&#39;, fig.width=10} --&gt;
&lt;!-- dfMedianBalanceOpt &lt;- bind_cols(dfFinalBalance %&gt;% select(c(1:4)), --&gt;
&lt;!--                                 MedianBalance = dfFinalBalance %&gt;% select(starts_with(&#34;Game&#34;)) %&gt;% apply(1, median)) %&gt;%  --&gt;
&lt;!--     mutate(InitialBalanceText = paste0(&#34;Starting balance = $&#34;, InitialBalance)) %&gt;%  --&gt;
&lt;!--     mutate(ProbHeadsText = paste0(&#34;Probability of heads = &#34;, ProbHeads)) --&gt;
&lt;!-- dfAnnText &lt;- dfMedianBalanceOpt %&gt;%  --&gt;
&lt;!--     dplyr::filter(NumFlips == max(NumFlips)) %&gt;%  --&gt;
&lt;!--     group_by(InitialBalance, ProbHeads, NumFlips) %&gt;%  --&gt;
&lt;!--     mutate(OptimalBet = BetFraction,  --&gt;
&lt;!--            MaxBalance = max(MedianBalance)) %&gt;%  --&gt;
&lt;!--     ungroup() %&gt;%  --&gt;
&lt;!--     dplyr::filter(MedianBalance &gt; (MaxBalance - 1e-12)) %&gt;%  --&gt;
&lt;!--     mutate(AnnotateText = paste0(&#34;Optimal bet = &#34;,OptimalBet*100, &#34;%&#34;)) --&gt;
&lt;!-- ggplot(dfMedianBalanceOpt, aes(x = BetFraction, y = MedianBalance)) +  --&gt;
&lt;!--     geom_line(aes(color = factor(NumFlips)), size = 0.7) + --&gt;
&lt;!--     scale_x_continuous(breaks = seq(0, 1, 0.2), labels = scales::percent) + --&gt;
&lt;!--     scale_y_continuous(label = scales::dollar) + --&gt;
&lt;!--     scale_color_viridis(name = &#34;Number of coin flips&#34;, discrete = T, option = &#34;C&#34;, direction = -1) + --&gt;
&lt;!--     facet_grid(ProbHeadsText ~ InitialBalanceText, scales = &#34;free&#34;) + --&gt;
&lt;!--     geom_text(data = dfAnnText, aes(label = AnnotateText), x=Inf, y=Inf, vjust=1, hjust=1, size = 3.5) + --&gt;
&lt;!--     labs(x = &#34;Constant bet on heads on each flip&#34;, --&gt;
&lt;!--          y = &#34;Median final balance&#34;) + --&gt;
&lt;!--     theme(strip.text = element_text(face = &#34;bold&#34;)) --&gt;
&lt;!-- ``` --&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-the-game-play&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Simulating the game play&lt;/h3&gt;
&lt;p&gt;If you’ve read this far, you understand this game can’t be offered without an upper limit on the final payout. If you played the game and bet wisely or were just plain lucky, you would’ve discovered that the maximum payout is &lt;span class=&#34;math inline&#34;&gt;\(\$250\)&lt;/span&gt;. So the objective changes mid-game from growing to preserving existing balance. Once the maximum payout balance is reached, it is foolhardy to keep betting large sums no matter what betting strategy you have chosen.&lt;/p&gt;
&lt;p&gt;Secondly, the UI design of the app doesn’t have a mechanism to choose a constant percentage bet level. So, if you were following a systematic betting strategy like Kelly, then you would have to do mental calculations to determine the exact dollar amount. While this is not hard to do, I believe most people would just round up or down to the nearest dollar amount. Also it takes less time to input a whole dollar amount than dollars and cents. So even though the minimum bet size in the game is 1 cent, effectively it would be &lt;span class=&#34;math inline&#34;&gt;\(\$1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/biased-coin-flipping-experiment/index_files/figure-html/final_balance_distributions_constrained-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Under these constraints and assumptions, the plot shows bimodal distributions at all bet levels. During the game, the objective changes to maximizing the chances of ending a game above the maximum payout of &lt;span class=&#34;math inline&#34;&gt;\(\$250\)&lt;/span&gt;. Here we can see that the Kelly formula turned out to be a close approximation to optimal, even though the optimal level is determined only in hindsight. Fewer bankruptcies result at higher bet levels than in unconstrained games. I made the assumption that the participants who got lucky early on betting big, decided to bet just &lt;span class=&#34;math inline&#34;&gt;\(\$1\)&lt;/span&gt; on all subsequent bets after discovering the maximum payout.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/biased-coin-flipping-experiment/index_files/figure-html/maximum_payout_plots-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-did-the-participants-do-in-the-experiment&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How did the participants do in the experiment?&lt;/h3&gt;
&lt;p&gt;Not very well to say the least. There’s an excellent discussion in the paper, but here are the salient points:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Only 21% of the participants reached the maximum payout of &lt;span class=&#34;math inline&#34;&gt;\(\$250\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;51% of the participants neither went bust nor reached the maximum payout. Their average final balance was &lt;span class=&#34;math inline&#34;&gt;\(\$75\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;33% of the participants ended up below the starting balance of &lt;span class=&#34;math inline&#34;&gt;\(\$25\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;28% of the participants went bust and received no payout&lt;/li&gt;
&lt;li&gt;Average payout across all participants was &lt;span class=&#34;math inline&#34;&gt;\(\$91\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Only 5 among the 61 participants had heard of the Kelly criterion. Out of those, only 1 managed to barely double his stake while the other broke even after 100 flips&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;betting-patterns&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Betting Patterns&lt;/h3&gt;
&lt;p&gt;Betting patterns were found to be quite erratic.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;18 participants bet all-in on one flip&lt;/li&gt;
&lt;li&gt;Some bet too small and then too big&lt;/li&gt;
&lt;li&gt;Many participants adopted a &lt;a href=&#34;https://en.wikipedia.org/wiki/Martingale_%28betting_system%29&#34;&gt;Martingale strategy&lt;/a&gt; where the losing bets are doubled up to recover past losses&lt;/li&gt;
&lt;li&gt;Some bet small constant wagers to minimize chances of ruin and end up with a positive balance&lt;/li&gt;
&lt;li&gt;41 participants (67%!!) bet on tails at some point during the game play. 29 of them bet on tails 5 or more times&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Despite how terrible some of these betting strategies may have been, hitting the maximum payout limit might have saved some of them from ending up bankrupt. Well, assuming these participants were not foolish enough to keep betting high after knowing they couldn’t get more than &lt;span class=&#34;math inline&#34;&gt;\(\$250\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let’s see how some of these betting patterns work in simulation.&lt;/p&gt;
&lt;div id=&#34;all-in-betting&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;All-in Betting&lt;/h4&gt;
&lt;p&gt;18 out of 61 participants reportedly bet 100% on a single flip. Assuming they continued this betting pattern, majority of these participants would’ve lost everything after their second bet. Those who reached the maximum payout would’ve had to win first 4 flips in succession. Assuming they kept their wits beyond that point, only 13% of such participants would’ve ended up with a balance of &lt;span class=&#34;math inline&#34;&gt;\(\$250\)&lt;/span&gt; or more.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/biased-coin-flipping-experiment/index_files/figure-html/all_in_results-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;betting-too-small-and-then-too-big&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Betting too small and then too big&lt;/h4&gt;
&lt;p&gt;Some participants bet too small and then too big. It appears they were over-cautious at first, but when they built a sizeable balance, they threw caution to the wind. Some form of &lt;a href=&#34;https://en.wikipedia.org/wiki/Wealth_effect&#34;&gt;wealth effect&lt;/a&gt; or &lt;a href=&#34;http://www.investopedia.com/terms/h/house-money-effect.asp&#34;&gt;house money effect&lt;/a&gt; came into play.&lt;/p&gt;
&lt;p&gt;To simulate this strategy, I assumed such participants bet 5% until they accumulated a balance of &lt;span class=&#34;math inline&#34;&gt;\(\$100\)&lt;/span&gt;, after which they started betting 30%. If their balance fell below &lt;span class=&#34;math inline&#34;&gt;\(\$100\)&lt;/span&gt; again, they got spooked and adopted the 5% conservative bet again.&lt;/p&gt;
&lt;p&gt;The simulated results are quite interesting. Although none of the participants would’ve been bankrupted (assuming they adopted the ultra safe 5% bets when their balance was lower), only ~ 10% of them would’ve made &lt;span class=&#34;math inline&#34;&gt;\(\$250\)&lt;/span&gt; or more.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/biased-coin-flipping-experiment/index_files/figure-html/small_big_betting_results-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;martingale-betting&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Martingale betting&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Martingale_%28betting_system%29&#34;&gt;Martingale&lt;/a&gt; bettors keep doubling up their bets after every loss. For instance, if they bet &lt;span class=&#34;math inline&#34;&gt;\(\$2\)&lt;/span&gt; and lost, their next bet would be &lt;span class=&#34;math inline&#34;&gt;\(\$4\)&lt;/span&gt;, and the next one &lt;span class=&#34;math inline&#34;&gt;\(\$8\)&lt;/span&gt; if they lost again. Their idea is to recover the former loss(es) and make profits equal to the original stake. Most often this strategy starts with betting small and doubles up on the previous bet even in successive losses. This idea mostly stems from &lt;a href=&#34;https://en.wikipedia.org/wiki/Gambler%27s_fallacy&#34;&gt;gambler’s fallacy&lt;/a&gt;. So if a Martingale bettor was betting on heads but the coin flipped tails several times in succession, their belief is the next flip would be heads and they’ll recover everything they’ve lost thus far. After all mean reversion should come into play, right?.&lt;/p&gt;
&lt;p&gt;But mean reversion could take much longer to manifest. The coin has no memory of the past flips. Every flip is independent of the prior flips. Even with a biased coin like in this game, it could very well happen that it flips tails 20 times in succession and then flips heads 30 times in succession. The probability of heads converges to 0.6 only &lt;strong&gt;&lt;em&gt;in the long run&lt;/em&gt;&lt;/strong&gt;. After losing 20 bets in succession starting with &lt;span class=&#34;math inline&#34;&gt;\(\$1\)&lt;/span&gt;,
a Martingale bettor would need to bet ~ $1.05 million to recover past losses.&lt;/p&gt;
&lt;p&gt;To simulate this betting strategy, I assumed a minimum bet size of &lt;span class=&#34;math inline&#34;&gt;\(\$1\)&lt;/span&gt; as long as the balance is &lt;span class=&#34;math inline&#34;&gt;\(\$25\)&lt;/span&gt; or lower i.e. 4% to start. The minimum bet is readjusted proportionally as the balance increases or decreases in multiples of
&lt;span class=&#34;math inline&#34;&gt;\(\$25\)&lt;/span&gt;. So it goes up to &lt;span class=&#34;math inline&#34;&gt;\(\$2\)&lt;/span&gt; when the balance is above &lt;span class=&#34;math inline&#34;&gt;\(\$50\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\$4\)&lt;/span&gt; when the balance is above &lt;span class=&#34;math inline&#34;&gt;\(\$100\)&lt;/span&gt;. The previous losing bet is doubled up in succession.&lt;/p&gt;
&lt;p&gt;The simulated results show 42% of the bettors going bankrupt with this strategy.
Only 38% of the bettors would’ve ended up with &lt;span class=&#34;math inline&#34;&gt;\(\$250\)&lt;/span&gt; or more.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/biased-coin-flipping-experiment/index_files/figure-html/martingale_results-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;betting-on-tails&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Betting on tails&lt;/h4&gt;
&lt;p&gt;There were 41 participants who bet on tails at some point during the game. 29 of them bet on tails more than 5 times in the game. 13 of them bet on tails more than 25% of the time. These participants were more likely to make that bet after a string of consecutive heads. As the authors state in the paper &lt;em&gt;“…some combination of the illusion of control, law of small number bias, gambler’s fallacy or hot hand fallacy was at work”&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Let’s assume, these participants bet on heads, until a string of ‘X’ heads, after which they kept betting on tails until they got one, after which they bet on heads again repeating the same process. What would be the value of ‘X’ for the participants to bet on tails more than 25% of the time?
We could get a sense from the simulated game data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Heads_Threshold_2 Heads_Threshold_3 Heads_Threshold_4 Heads_Threshold_5 Heads_Threshold_6 
             0.35              0.21              0.12              0.07              0.04 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It turns out most of these participants were expecting tails after a string of 3 or more heads. Assuming they followed this heuristic, even while making the optimal 20% bet, the simulated results show ~ 7% of the participants going bankrupt and only 25% of them ending up with &lt;span class=&#34;math inline&#34;&gt;\(\$250\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/biased-coin-flipping-experiment/index_files/figure-html/betting_tails_results-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;comparison-of-betting-strategies&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comparison of Betting Strategies&lt;/h3&gt;
&lt;p&gt;The plot below shows the performance of the betting strategies modelled above, with respect to a game optimal strategy of betting 20% of the balance. Most of them end up with a much higher proportion of participants going bankrupt in the game. No strategy comes anywhere close to the game optimal strategy, when it comes to the number of participants reaching the maximum payout offered in the game.&lt;/p&gt;
&lt;p&gt;In fact, discovering the maximum payout of &lt;span class=&#34;math inline&#34;&gt;\(\$250\)&lt;/span&gt; would’ve helped some of the participants employing these wayward betting strategies into adopting more conservative approaches in the latter part of their game. But if the game was designed to payout a fraction of the final balance, some more interesting outcomes might emerge. For instance, the maximum balance in the game could be raised to 2500 and the actual payout could be a tenth of that
(&lt;span class=&#34;math inline&#34;&gt;\(\$250\)&lt;/span&gt; still). The evidence from simulated results suggests the outcomes in that case to be much worse.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/biased-coin-flipping-experiment/index_files/figure-html/betting_comparison-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-did-i-play-the-game&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How did I play the game?&lt;/h3&gt;
&lt;p&gt;To be sure, I was &lt;em&gt;not&lt;/em&gt; a part of the cohort of 61 participants who played this game in person and were paid for participation. The &lt;a href=&#34;http://coinflipbet.herokuapp.com/&#34;&gt;game URL&lt;/a&gt; is still active, even though there’s no actual payout offered any more. I found it after listening to the &lt;a href=&#34;https://chatwithtraders.com/ep-129-victor-haghani/&#34;&gt;podcast&lt;/a&gt; and reading the &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2856963&#34;&gt;paper&lt;/a&gt;. So, unfortunately I already knew the optimal strategy before trying. In my gameplay, I reached the maximum payout by the 16th minute. I ended up with a balance of &lt;span class=&#34;math inline&#34;&gt;\(\$270\)&lt;/span&gt;, flipping the virtual coin a total of 122 times.&lt;/p&gt;
&lt;p&gt;I did however, get some loved ones to play the game uninformed and then tell me their betting strategies. In a sample of n = 4, they reported everything from Martingale betting, betting 10 cents per flip, finding patterns in flip sequences and even betting on tails because of said patterns. In other words, I got a microcosm of results in the paper.&lt;/p&gt;
&lt;p&gt;I would have bet erratically myself, had I not known the optimal strategy. A smaller proportion than optimal at first and then perhaps too big.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Despite being offered favorable odds, most participants adopted strategies subject to their own biases and ended up with poor to suboptimal outcomes. Having a background in quantitative fields didn’t seem to help most of them. Through simulations we can explore outcomes of some of these biases, but actual gameplay involves much more complexity. Most participants would’ve been swayed by a combination of biases at different points during their gameplay.&lt;/p&gt;
&lt;p&gt;Even though betting a constant proportion is an optimal strategy for this game, and has been known since 1955, there persists a gap in education where even the most quantitatively oriented people like myself have no idea about it. Experiments like this have tremendous educational value and should be part of the curriculum in both high schools and colleges. I firmly agree with the thoughts echoed in the paper.&lt;/p&gt;
&lt;p&gt;In a broader context, this experiment shares similarities with investing. Often the same biases come into play while investing. Even though the outcomes in investing are continuous in nature, and uncertainities abound, a systematic strategy could spell the difference between attaining financial objectives or not. It shows the importance and benefits of sticking to a well-thought-out systematic plan that is devoid of all biases.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;The R markdown file for this post is available &lt;a href=&#34;https://github.com/nitingupta2/nitingupta2.github.io/blob/master/post/biased-coin-flipping-experiment/&#34;&gt;here&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Text Analysis of Amazon Shareholder Letters</title>
      <link>/post/text-analysis-of-amazon-shareholder-letters/</link>
      <pubDate>Fri, 28 Apr 2017 00:00:00 +0000</pubDate>
      <guid>/post/text-analysis-of-amazon-shareholder-letters/</guid>
      <description>


&lt;p&gt;Amazon is led by its charismatic founder and CEO, Jeff Bezos, who is widely hailed as one of the top visionaries of this era. At the end of the fiscal year in March, Amazon publishes a letter to shareholders written by Bezos, in which he summarises his thoughts, business and management philosophy. While reading these letters, among other things that strikes is Bezos’ clarity of thought, expressed in a concise and easy to comprehend way. It is a very rare quality possessed by very few business leaders. Few other names that come to mind are Warren Buffett, Charlie Munger, Elon Musk and late Steve Jobs.&lt;/p&gt;
&lt;p&gt;The first &lt;strong&gt;1997 Letter to Shareholders&lt;/strong&gt; was published at the conclusion of the first fiscal year of Amazon as a public company. In that letter, Jeff Bezos outlined his vision for Amazon and what kind of company he wanted it to be. Obsessing over customers and offering compelling value, is a core principle he laid out in that letter. Focussing on long term investment decisions, company growth and profitability, is how he envisioned Amazon to thrive. The remarkable thing is that in the last 20+ years, Amazon has executed right along these principles and has become, as of this post, the fourth largest company in the United States by market cap. In the first shareholder letter, Bezos said it is Day 1 for the internet and Amazon. He echoes the same thoughts in the most recent &lt;strong&gt;2016 Letter to Shareholders&lt;/strong&gt;, published earlier this month. In other words, he believes, Amazon still functions as a startup, though now it’s the biggest one. In every shareholder letter, Bezos appends the first shareholder letter of 1997, which shows how he has stayed true to his vision.&lt;/p&gt;
&lt;p&gt;In this post, I’ll do a text analysis of Amazon Shareholder Letters from 1997 to 2016.&lt;/p&gt;
&lt;p&gt;These letters are in pdf format and could be downloaded from Amazon’s &lt;a href=&#34;http://phx.corporate-ir.net/phoenix.zhtml?c=97664&amp;amp;p=irol-reportsannual&#34;&gt;investor relations website&lt;/a&gt;. I have put them in a zipped file &lt;a href=&#34;https://github.com/nitingupta2/nitingupta2.github.io/blob/master/post/text-analysis-of-amazon-shareholder-letters/&#34;&gt;here&lt;/a&gt;. The first step is to combine the text and remove the 1997 letter from every subsequent letter, and do some additional cleaning. Since ‘Day 1’ and ‘Day 2’ are often used phrases in the letters, I decided to alter the numbers 1 and 2 to &lt;em&gt;one&lt;/em&gt; and &lt;em&gt;two&lt;/em&gt; and remove all other numbers from the letters.&lt;/p&gt;
&lt;div id=&#34;how-does-bezos-start-these-letters&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How does Bezos start these letters?&lt;/h3&gt;
&lt;p&gt;We see that the letters are typically addressed to shareholders/shareowners. In 1998, they were addressed to shareholders, customers and employees. 2016 has been a marked departure in his approach. While the earlier letters were mixed with facts and opinion, the 2016 letter constructs a short narrative of why companies decline. He reminds employees about the pitfalls of ‘Day 2’ and beckons them to remain in ‘Day 1’ mode for the next couple of decades.&lt;/p&gt;
&lt;center&gt;
&lt;!-- html table generated in R 3.6.3 by xtable 1.8-4 package --&gt;
&lt;!-- Thu Jun 11 15:14:14 2020 --&gt;
&lt;table border=&#34;1&#34;&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
year
&lt;/th&gt;
&lt;th&gt;
first_line
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1997
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
To our shareholders.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
2
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1998
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
To our shareholders, customers, and employees.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
3
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1999
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
To our shareholders.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2000
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
To our shareholders.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2001
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
To our shareholders.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
6
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2002
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
To our shareholders.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
7
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2003
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
To our shareholders.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2004
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
To our shareholders.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
9
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2005
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
To our shareholders.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
10
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2006
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
To our shareholders.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
11
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2007
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
To our shareowners.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
12
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2008
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
To our shareowners.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
13
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2009
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
To our shareowners.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
14
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2010
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
To our shareowners.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
15
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2011
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
To our shareowners.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
16
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2012
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
To our shareowners.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
17
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2013
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
To our shareowners.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
18
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2014
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
To our shareowners.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
19
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2015
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
To our shareowners.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
20
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2016
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
“Jeff, what does Day two look like?”
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;words-per-letter&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Words per Letter&lt;/h3&gt;
&lt;p&gt;We can see that Bezos doesn’t write very long letters. The mean word count is around 1700 words. But from 2013-2015, his letters were more than twice as long.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/text-analysis-of-amazon-shareholder-letters/index_files/figure-html/words_per_letter-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;top-words&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Top Words&lt;/h3&gt;
&lt;p&gt;After excluding the commonly used ‘stop words’, a wordcloud of top 100 words is shown below. A remarkable thing to note is how much more &lt;em&gt;customers&lt;/em&gt; are emphasized in the letters. Besides the company name, other terms that gain prominence are &lt;em&gt;business, sales, service, experience, cash,&lt;/em&gt; and &lt;em&gt;shareholders&lt;/em&gt;. Also note the frequent mention of words related to some products and services - &lt;em&gt;Kindle, Prime, AWS, Marketplace, Fulfillment&lt;/em&gt;. Bezos has been talking about ‘Day 1’ right from the first to the latest letter, hence we find both &lt;em&gt;day&lt;/em&gt; and &lt;em&gt;one&lt;/em&gt; in the mix as well.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/text-analysis-of-amazon-shareholder-letters/index_files/figure-html/wordcloud-1.png&#34; width=&#34;528&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;words-commonly-associated-with-customers&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Words Commonly Associated with ‘customers’&lt;/h3&gt;
&lt;p&gt;I tokenized the text into bigrams (sequence of two words occuring together) and excluded all instances containing commonly used ‘stop words’. Since we know &lt;em&gt;customer&lt;/em&gt; or &lt;em&gt;customers&lt;/em&gt; are the most common words, let’s examine which words appear the most with them.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/text-analysis-of-amazon-shareholder-letters/index_files/figure-html/customer_network-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The arrows point towards the second word in the bigram. And n denotes the frequency of occurance. We can see that ‘customer experience’ has the strongest focus. It has been mentioned 47 times in all the letters, followed by ‘customer service’ and ‘customer satisfaction’.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dominant-themes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Dominant Themes&lt;/h3&gt;
&lt;p&gt;For each bigram and year pair, I calculate its tf-idf statistic, which measures how important a bigram is to a document in a collection of documents.&lt;/p&gt;
&lt;p&gt;Plotting the top 10 terms by this statistic every year, reveals what was on Bezos’ mind every year that was not so common with all other years.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/text-analysis-of-amazon-shareholder-letters/index_files/figure-html/unique_bigrams-1.png&#34; width=&#34;1152&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In most of the cases, the top 10 tf-idf terms neatly summarise the important points mentioned in the letters. What’s really interesting is, we could get a sense of almost all major products and services offered by Amazon and when they were introduced or became popular. Beginning in 1998 with the launch of music and video stores (it means CDs and DVDs for you millennials out there :), to Prime Instant Video in 2013 and so many more. 2004 was all about explaining why free cash flow is important than earnings. 2010 was focussed on advancing machine learning and building advanced technology capabilities.
The latest one from 2016 pertains to the Day 1 and Day 2 narrative and the quick decision making philosophy at a juggernaut like Amazon.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;network-of-common-bigrams&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Network of Common Bigrams&lt;/h3&gt;
&lt;p&gt;These are bigrams that appear in 5 or more letters. In a nutshell. they show what has been most important to Amazon.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/text-analysis-of-amazon-shareholder-letters/index_files/figure-html/bigram_network-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sentiment-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sentiment Analysis&lt;/h3&gt;
&lt;p&gt;For computing sentiment scores, I use 2 lexicons from the &lt;code&gt;tidytext&lt;/code&gt; package:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;bing&lt;/em&gt;&lt;/strong&gt; : A general purpose lexicon created by &lt;a href=&#34;https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html&#34;&gt;Bing Liu et. al.&lt;/a&gt;, containing a dictionary of words classified into 2 categories, &lt;em&gt;positive&lt;/em&gt; and &lt;em&gt;negative&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;loughran&lt;/em&gt;&lt;/strong&gt; : Developed by &lt;a href=&#34;https://doi.org/10.1111/j.1540-6261.2010.01625.x&#34;&gt;Tim Loughran and Bill McDonald&lt;/a&gt; based on analyses of financial reports, it contains a dictionary of words classified into 6 categories - &lt;em&gt;constraining, litiguous, negative, positive, superflous, uncertainity&lt;/em&gt;. (As of this writing, this lexicon is available only in the &lt;a href=&#34;http://github.com/juliasilge/tidytext&#34;&gt;dev version&lt;/a&gt; of &lt;code&gt;tidytext&lt;/code&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I restrict the usage to only positive and negative sentiment words from the &lt;strong&gt;&lt;em&gt;loughran&lt;/em&gt;&lt;/strong&gt; lexicon. Words not found in these lexicons are classified as neutral.&lt;/p&gt;
&lt;p&gt;Let’s look at the top 10 most common words which drive the positive and negative sentiment in these letters.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/text-analysis-of-amazon-shareholder-letters/index_files/figure-html/sentiment_words-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;bing&lt;/strong&gt; lexicon treats &lt;em&gt;free&lt;/em&gt; and &lt;em&gt;fulfillment&lt;/em&gt; as positive words, and &lt;em&gt;cloud&lt;/em&gt; as a negative word. But these are mostly used in business terms by Amazon. &lt;em&gt;free&lt;/em&gt; is used in the context of ‘free super saver shipping’ or ‘free shipping’, &lt;em&gt;fulfillment&lt;/em&gt; is used in terms of ‘fulfillment center(s)’ and &lt;em&gt;cloud&lt;/em&gt; in terms of ‘cloud service(s)’. None of these could be interpretted as positive or negative so it is best to remove these terms from our word dictionary before calculating the net sentiment scores.&lt;/p&gt;
&lt;p&gt;I define a net sentiment score as the sum of all positive terms, less the sum of all negative terms, divided by the sum of all positive, negative and neutral terms. The plot below shows the net sentiment scores by year, alongside the annual returns of Amazon stock. Amazon stock had a return of 966% in 1998, which distorts its plot, so I limited the annual returns plot to within 200% range.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/text-analysis-of-amazon-shareholder-letters/index_files/figure-html/sentiment-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can observe the sentiment in these letters has remained quite positive, even in the wake of Amazon stock dropping more than 80% after the dot-com bubble bust. Judging from the sentiment scores of these letters, Bezos doesn’t seem to be swayed by Amazon stock performance at all. The annual returns have almost 0 correlation to the yearly sentiment scores.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;The R markdown file for this post is available &lt;a href=&#34;https://github.com/nitingupta2/nitingupta2.github.io/blob/master/post/text-analysis-of-amazon-shareholder-letters/&#34;&gt;here&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Building a Predictive Typing Model</title>
      <link>/post/building-a-predictive-typing-model/</link>
      <pubDate>Sat, 15 Apr 2017 00:00:00 +0000</pubDate>
      <guid>/post/building-a-predictive-typing-model/</guid>
      <description>


&lt;p&gt;Everyone using a smartphone or a mobile device has used an onscreen smart keyboard that tries to predict the next set of words that the user might want to type. Typically, upto 3 words are predicted, which are displayed in a row at the top of the keyboard. Given that typing on a glass pane without tactile feedback, could be very frustrating at times, the smart keyboard goes a long way in alleviating these issues.&lt;/p&gt;
&lt;p&gt;But, how does predictive typing work? Basically, it’s a predictive typing model built using an ngram with &lt;a href=&#34;https://en.wikipedia.org/wiki/Katz%27s_back-off_model&#34;&gt;backoff prediction model&lt;/a&gt;. Makes complete sense, right? Well, it’s less complicated than it sounds, and actually not hard to understand, as we shall see further.&lt;/p&gt;
&lt;div id=&#34;corpus&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Corpus&lt;/h2&gt;
&lt;p&gt;To begin, we need a large corpus of text with a wide-ranging vocabulary. The first name that comes to mind is that of William Shakespeare. Shakespeare wrote a number of plays and poems that are classics in the English language. These plays and poems are available in the public domain and are easily accessible through a number of sources, (see &lt;a href=&#34;http://www.gutenberg.org/browse/authors/s#a65&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://shakespeare.mit.edu/&#34;&gt;here&lt;/a&gt;).
By some &lt;a href=&#34;http://www1.cmc.edu/pages/faculty/welliott/Shakespeare%20Vocabulary%20Chapter%20911.pdf&#34;&gt;accounts&lt;/a&gt;, Shakespeare’s vocabulary ranged from 15,000-30,000 words, which makes it excellent to use it for our corpus.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ropenscilabs/gutenbergr&#34;&gt;David Robinson’s&lt;/a&gt; &lt;code&gt;gutenbergr&lt;/code&gt; package makes it easy to download ‘The Complete Works Of Shakespeare’, from Project Gutenberg’s website. The downloaded dataset is a data frame with text appearing in rows along with the gutenberg id.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dfRaw &amp;lt;- gutenberg_download(100)
knitr::kable(head(dfRaw, 12))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;gutenberg_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;text&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Shakespeare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;em&gt;This Etext has certain copyright implications you should read!&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&amp;lt;&amp;lt;THIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;SHAKESPEARE IS COPYRIGHT 1990-1993 BY WORLD LIBRARY, INC., AND IS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PROVIDED BY PROJECT GUTENBERG ETEXT OF ILLINOIS BENEDICTINE COLLEGE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;WITH PERMISSION. ELECTRONIC AND MACHINE READABLE COPIES MAY BE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;DISTRIBUTED SO LONG AS SUCH COPIES (1) ARE FOR YOUR OR OTHERS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PERSONAL USE ONLY, AND (2) ARE NOT DISTRIBUTED OR USED&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;COMMERCIALLY. PROHIBITED COMMERCIAL DISTRIBUTION INCLUDES BY ANY&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;SERVICE THAT CHARGES FOR DOWNLOAD TIME OR FOR MEMBERSHIP.&amp;gt;&amp;gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(dfRaw[sample(nrow(dfRaw), 6),])&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;gutenberg_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;text&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Assure yourselves, will never be unkind.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Like one besotted on your sweet delights.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;And spurn in pieces posts of adamant;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Whereto, when they shall know what men are rich,&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TITUS. Marcus, even thou hast struck upon my crest,&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;data-preparation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Preparation&lt;/h2&gt;
&lt;p&gt;The copyright notice appears multiple times throughout the text. Fortunately, it doesn’t prevent us from using this data for non-commercial purposes. The first step is to merge the entire text and remove the copyright notices that are helpfully within &amp;lt;&amp;lt; … &amp;gt;&amp;gt;. Secondly, in plays, the characters and their relationships and titles are defined with ‘DRAMATIS PERSONAE’. These lines have to be removed too.&lt;/p&gt;
&lt;p&gt;After doing some quick cleaning, the text is reconverted to a data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;complete_works &amp;lt;- str_c(dfRaw$text, collapse = &amp;quot;.&amp;quot;)
complete_works &amp;lt;- str_split(complete_works, &amp;quot;&amp;lt;&amp;lt;[^&amp;gt;]*&amp;gt;&amp;gt;&amp;quot;)[[1]]
complete_works &amp;lt;- complete_works[!(str_detect(complete_works, &amp;quot;Dramatis Personae|DRAMATIS PERSONAE&amp;quot;))]
complete_works &amp;lt;- str_replace_all(complete_works, &amp;quot;\\[|\\]&amp;quot;, &amp;quot;.&amp;quot;)

dfContent &amp;lt;- as_tibble(complete_works) %&amp;gt;% slice(3:n())
rm(complete_works)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we parse the content into sentences. The &lt;code&gt;tidytext&lt;/code&gt; package by &lt;a href=&#34;http://varianceexplained.org/&#34;&gt;David Robinson&lt;/a&gt; and &lt;a href=&#34;http://juliasilge.com/&#34;&gt;Julia Silge&lt;/a&gt;, is most useful in doing this within the tidyverse framework.&lt;/p&gt;
&lt;p&gt;Here’s what the function does below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Replaces all ‘logical’ sentence break delimiters by a period for easier tokenization by sentence&lt;/li&gt;
&lt;li&gt;Removes extra periods and spaces&lt;/li&gt;
&lt;li&gt;Tokenizes into sentences&lt;/li&gt;
&lt;li&gt;Removes all sentences containing ACT or SCENE&lt;/li&gt;
&lt;li&gt;Removes all digits from sentences&lt;/li&gt;
&lt;li&gt;Finally adds line numbers to each row (more on the importance of this later…)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A sample of 10 sentences is shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;getSentences &amp;lt;- function(dfContent) {
    df &amp;lt;- dfContent %&amp;gt;% 
        mutate(value = str_replace_all(value, &amp;quot;;|:|!|\\?| \\-|\\- | \\- &amp;quot;, &amp;quot;\\. &amp;quot;)) %&amp;gt;% 
        mutate(value = str_replace_all(value, &amp;quot;[\\.]+&amp;quot;, &amp;quot;\\.&amp;quot;)) %&amp;gt;% 
        mutate(value = str_replace_all(value, &amp;quot;[[:space:]]+&amp;quot;, &amp;quot; &amp;quot;)) %&amp;gt;% 
        unnest_tokens(sentence, value, token = &amp;quot;sentences&amp;quot;, to_lower = FALSE) %&amp;gt;% 
        dplyr::filter(!str_detect(sentence, &amp;quot;Act|ACT|Scene|SCENE&amp;quot;)) %&amp;gt;% 
        mutate(sentence = str_replace_all(sentence, &amp;quot;[0-9]&amp;quot;,&amp;quot;&amp;quot;)) %&amp;gt;% 
        mutate(lineNumber = row_number()) %&amp;gt;% 
        select(lineNumber, everything())
}

dfSentences &amp;lt;- getSentences(dfContent)
knitr::kable(dfSentences[sample(nrow(dfSentences), 10),])&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;lineNumber&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;sentence&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;55991&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;That with the King here resteth in his tent. .&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;16097&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;You are a merry man, sir. fare you well.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;9717&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;He did ask favour.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100749&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;To the health of our general. .&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;40908&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;You are, I think, assur’d I love you not.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;117828&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Laurence. and another Watchman.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;19243&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Ere now denied the asker, and now again,.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;145024&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;I swear to do this, though a present death.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;70942&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Stocking his messenger.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;122957&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;For bringing wood in slowly.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-ngrams&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating ngrams&lt;/h2&gt;
&lt;p&gt;“ngrams are a contiguous sequence of n items from a given sequence of text…” - &lt;a href=&#34;https://en.wikipedia.org/wiki/N-gram&#34;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this case, ngrams would be a contiguous sequence of n words. In the simplest form, an ngram of size 1 is a single word.
An ngram of size 2 or more is a set of words that appear one after the other in a sequence.&lt;/p&gt;
&lt;p&gt;For instance, given a sentence - &lt;strong&gt;how are you doing?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are 4 Unigrams: how, are, you, doing&lt;/li&gt;
&lt;li&gt;There are 3 Bigrams: how are, are you, you doing&lt;/li&gt;
&lt;li&gt;There are 2 Trigrams: how are you, are you doing&lt;/li&gt;
&lt;li&gt;There is 1 Quadrigram: how are you doing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, using the data frame of clean sentences that we parsed, the next step is to create ngrams.
Once again this is made simple using the &lt;strong&gt;unnest_tokens&lt;/strong&gt; function from the &lt;code&gt;tidytext&lt;/code&gt; package. And this is where grouping by line numbers is important because we want ngrams to be created by sequence of words, only within the same line. We do not want ngrams connected by a sentence break.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;getNgrams &amp;lt;- function(dfSentences, n) {
    dfNgrams &amp;lt;- dfSentences %&amp;gt;% 
        group_by(lineNumber) %&amp;gt;% 
        unnest_tokens(word, sentence, token = &amp;quot;ngrams&amp;quot;, n = n) %&amp;gt;% 
        ungroup() %&amp;gt;% 
        count(word, sort = TRUE) %&amp;gt;% 
        rename(freq = n)
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;all-words&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;All Words&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1grams &amp;lt;- getNgrams(dfSentences, 1)
dim(df1grams)
[1] 26453     2
wordsOfFreq1 &amp;lt;- df1grams %&amp;gt;% group_by(freq) %&amp;gt;% summarise(n = sum(freq)) %&amp;gt;% dplyr::filter(freq == 1) %&amp;gt;% .$n&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are 26453 words in the corpus, that shows how vast Shakespeare’s vocabulary was. Let’s see a wordcloud of top 100 words. No surprise, the most commonly used words are: &lt;strong&gt;&lt;em&gt;the, and, i, to, of&lt;/em&gt;&lt;/strong&gt;. Excluding these commonly used ‘stop words’, in the second wordcloud, we can see the words Shakespeare used most frequently. We can see a lot of references to royalty and nobility, present specially in Shakespearean tragedies. Some names are mentioned quite frequently - Richard, John, Caesar, Brutus and Henry. Some English counties or &lt;a href=&#34;https://en.wikipedia.org/wiki/Shire&#34;&gt;shires&lt;/a&gt; are mentioned quite often - Warwick, York and Gloucester.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/building-a-predictive-typing-model/index_files/figure-html/wordcloud-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;unigrams&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Unigrams&lt;/h3&gt;
&lt;p&gt;Let’s see a plot of the distribution of words in corpus by usage.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/building-a-predictive-typing-model/index_files/figure-html/plot_freq-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here’s a strategy for building ngram tables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is a long tail of 10522 words in the corpus that appear exactly once. Replace all words appearing just once, by &lt;strong&gt;UNK&lt;/strong&gt; as a placeholder for a word not known from the corpus. There are a couple of motivations behind this:
&lt;ul&gt;
&lt;li&gt;It reduces the size of ngram tables.&lt;/li&gt;
&lt;li&gt;More importantly, when an unknown word is encountered, the model knows how to tackle it before predicting the next set of words.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Re-summarise and rearrange the table by frequency.&lt;/li&gt;
&lt;li&gt;Assign an index to each word.&lt;/li&gt;
&lt;li&gt;Convert to data.table format. Some of the biggest benefits of this approach are:
&lt;ul&gt;
&lt;li&gt;Rows could be assigned and referenced by a key. This makes it extremely simple to look up a row value.&lt;/li&gt;
&lt;li&gt;Lookup by a key is very fast compared to matching character strings.&lt;/li&gt;
&lt;li&gt;When we create tables for bigrams and higher order ngrams, we can replace the words by their corresponding integer indexes. It saves a lot of memory as compared to creating tibbles with character strings.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vTopUnigrams &amp;lt;- df1grams %&amp;gt;% dplyr::filter(freq &amp;gt;= 2) %&amp;gt;% .$word
df1grams &amp;lt;- df1grams %&amp;gt;% 
    mutate(word = ifelse(word %in% vTopUnigrams, word, &amp;quot;UNK&amp;quot;)) %&amp;gt;% 
    group_by(word) %&amp;gt;% 
    summarise(freq = sum(freq)) %&amp;gt;% 
    arrange(desc(freq)) %&amp;gt;% 
    mutate(index1 = row_number()) %&amp;gt;% 
    select(index1, freq, word)

dt1grams &amp;lt;- as.data.table(df1grams)
# set key to word
dt1grams &amp;lt;- dt1grams[,.(index1,freq),key=word]
# create another table with key=index1
dt1gramsByIndex &amp;lt;- dt1grams[,.(word,freq),key=index1]
# print dimensions
dim(dt1grams)
[1] 15932     3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the top 10 unigrams. &lt;strong&gt;UNK&lt;/strong&gt; replaces the long tail of words appearing once.
Finally, we are left with 15932 unigrams in our table.&lt;/p&gt;
&lt;center&gt;
&lt;!-- html table generated in R 3.6.3 by xtable 1.8-4 package --&gt;
&lt;!-- Thu Jun 11 14:58:51 2020 --&gt;
&lt;table border=&#34;1&#34;&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
index1
&lt;/th&gt;
&lt;th&gt;
word
&lt;/th&gt;
&lt;th&gt;
freq
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
the
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
26821
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
2
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
and
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
25670
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
3
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
3
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
20473
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
to
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
19377
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
of
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
16954
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
6
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
6
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
a
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
14351
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
7
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
7
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
you
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
13568
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
my
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
12449
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
9
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
9
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
in
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
10881
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
10
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
10
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
that
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
10869
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bigrams&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bigrams&lt;/h3&gt;
&lt;p&gt;A data table of bigrams is created where the index of the first word is the lookup key. The second word cannot be &lt;strong&gt;UNK&lt;/strong&gt; since it is a word used for making predictions from the bigram table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df2grams &amp;lt;- getNgrams(dfSentences, 2) %&amp;gt;% 
    separate(word, c(&amp;quot;word1&amp;quot;, &amp;quot;word2&amp;quot;), sep = &amp;quot; &amp;quot;) %&amp;gt;% 
    mutate(word1 = ifelse(word1 %in% vTopUnigrams, word1, &amp;quot;UNK&amp;quot;)) %&amp;gt;% 
    dplyr::filter(word2 %in% vTopUnigrams) %&amp;gt;% 
    group_by(word1, word2) %&amp;gt;% 
    summarise(freq = sum(freq)) %&amp;gt;% 
    arrange(desc(freq))

dt2grams &amp;lt;- as.data.table(df2grams)
dt2grams$index1 &amp;lt;- dt1grams[dt2grams$word1]$index1
dt2grams$index2 &amp;lt;- dt1grams[dt2grams$word2]$index1
dt2grams &amp;lt;- dt2grams[,.(index1,index2,freq)]
setkey(dt2grams, index1)

# print dimensions
dim(dt2grams)
[1] 254884      3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the top 10 bigrams:&lt;/p&gt;
&lt;center&gt;
&lt;!-- html table generated in R 3.6.3 by xtable 1.8-4 package --&gt;
&lt;!-- Thu Jun 11 14:59:03 2020 --&gt;
&lt;table border=&#34;1&#34;&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
word1
&lt;/th&gt;
&lt;th&gt;
word2
&lt;/th&gt;
&lt;th&gt;
freq
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
30512
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
2
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
am
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1846
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
3
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
my
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
lord
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1640
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
in
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
the
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1611
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
have
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1603
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
6
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
will
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1553
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
7
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
of
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
the
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1417
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
to
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
the
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1380
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
9
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
it
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
is
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1066
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
10
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
to
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
be
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
973
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trigrams&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Trigrams&lt;/h3&gt;
&lt;p&gt;Similarly, a data table of trigrams is created where indexes of the first and second words form the lookup key.
The third word cannot be &lt;strong&gt;UNK&lt;/strong&gt; since it is a word used for making predictions from the trigram table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df3grams &amp;lt;- getNgrams(dfSentences, 3) %&amp;gt;% 
    separate(word, c(&amp;quot;word1&amp;quot;, &amp;quot;word2&amp;quot;, &amp;quot;word3&amp;quot;), sep = &amp;quot; &amp;quot;) %&amp;gt;% 
    mutate(word1 = ifelse(word1 %in% vTopUnigrams, word1, &amp;quot;UNK&amp;quot;)) %&amp;gt;% 
    mutate(word2 = ifelse(word2 %in% vTopUnigrams, word2, &amp;quot;UNK&amp;quot;)) %&amp;gt;% 
    dplyr::filter(word3 %in% vTopUnigrams) %&amp;gt;% 
    group_by(word1, word2, word3) %&amp;gt;% 
    summarise(freq = sum(freq)) %&amp;gt;% 
    arrange(desc(freq))

dt3grams &amp;lt;- as.data.table(df3grams)
dt3grams$index1 &amp;lt;- dt1grams[dt3grams$word1]$index1
dt3grams$index2 &amp;lt;- dt1grams[dt3grams$word2]$index1
dt3grams$index3 &amp;lt;- dt1grams[dt3grams$word3]$index1
dt3grams &amp;lt;- dt3grams[,.(index1,index2,index3,freq)]
setkey(dt3grams, index1,index2)

# print dimensions
dim(dt3grams)
[1] 476098      4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the top 10 trigrams, we can get an idea how &lt;strong&gt;UNK&lt;/strong&gt; will be useful in this context. &lt;strong&gt;&lt;em&gt;the UNK of&lt;/em&gt;&lt;/strong&gt; is one of the most frequently used word sequences in the trigram table, which implies that if we encounter an unknown word after ‘the’, then ‘of’ is the most likely word to be predicted from the trigram table.&lt;/p&gt;
&lt;center&gt;
&lt;!-- html table generated in R 3.6.3 by xtable 1.8-4 package --&gt;
&lt;!-- Thu Jun 11 14:59:27 2020 --&gt;
&lt;table border=&#34;1&#34;&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
word1
&lt;/th&gt;
&lt;th&gt;
word2
&lt;/th&gt;
&lt;th&gt;
word3
&lt;/th&gt;
&lt;th&gt;
freq
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
38526
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
2
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
pray
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
you
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
238
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
3
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
will
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
not
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
213
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
know
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
not
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
159
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
do
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
not
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
157
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
6
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
the
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
UNK
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
of
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
155
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
7
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
the
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
duke
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
of
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
141
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
am
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
not
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
139
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
9
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
am
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
a
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
138
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
10
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
would
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
not
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
127
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quadrigrams&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Quadrigrams&lt;/h3&gt;
&lt;p&gt;Here are the top 10 quadrigrams:&lt;/p&gt;
&lt;center&gt;
&lt;!-- html table generated in R 3.6.3 by xtable 1.8-4 package --&gt;
&lt;!-- Thu Jun 11 14:59:54 2020 --&gt;
&lt;table border=&#34;1&#34;&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
word1
&lt;/th&gt;
&lt;th&gt;
word2
&lt;/th&gt;
&lt;th&gt;
word3
&lt;/th&gt;
&lt;th&gt;
word4
&lt;/th&gt;
&lt;th&gt;
freq
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
47569
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
2
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
with
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
all
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
my
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
heart
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
46
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
3
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
know
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
not
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
what
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
38
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
give
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
me
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
your
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
hand
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
31
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
do
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
beseech
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
you
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
31
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
6
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
give
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
me
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
thy
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
hand
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
30
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
7
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
the
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
duke
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
of
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
york
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
do
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
not
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
know
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
28
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
9
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
would
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
not
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
have
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
27
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
10
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
ay
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
my
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
good
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
lord
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
25
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pentagrams&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Pentagrams&lt;/h3&gt;
&lt;p&gt;Here are the top 10 pentagrams:&lt;/p&gt;
&lt;center&gt;
&lt;!-- html table generated in R 3.6.3 by xtable 1.8-4 package --&gt;
&lt;!-- Thu Jun 11 15:00:17 2020 --&gt;
&lt;table border=&#34;1&#34;&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
word1
&lt;/th&gt;
&lt;th&gt;
word2
&lt;/th&gt;
&lt;th&gt;
word3
&lt;/th&gt;
&lt;th&gt;
word4
&lt;/th&gt;
&lt;th&gt;
word5
&lt;/th&gt;
&lt;th&gt;
freq
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
60534
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
2
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
am
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
glad
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
to
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
see
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
15
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
3
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
thank
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
you
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
for
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
your
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
had
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
rather
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
be
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
a
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
9
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
am
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
glad
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
to
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
see
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
you
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
6
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
as
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
am
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
a
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
gentleman
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
7
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
for
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
mine
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
own
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
part
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
pray
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
you
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
tell
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
me
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
9
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
know
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
not
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
what
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
to
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
say
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
10
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
and
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
so
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
take
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
my
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;These ngram tables comprise all the information needed for the prediction model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;memory-usage-comparison&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Memory Usage Comparison&lt;/h3&gt;
&lt;p&gt;As mentioned above, storing words as integer indexes in a data table is far more efficient as compared to a data frame with character strings.
For instance, here’s how a pentagram data table looks like that contains just the integer indexes of the words:&lt;/p&gt;
&lt;center&gt;
&lt;!-- html table generated in R 3.6.3 by xtable 1.8-4 package --&gt;
&lt;!-- Thu Jun 11 15:00:17 2020 --&gt;
&lt;table border=&#34;1&#34;&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
index1
&lt;/th&gt;
&lt;th&gt;
index2
&lt;/th&gt;
&lt;th&gt;
index3
&lt;/th&gt;
&lt;th&gt;
index4
&lt;/th&gt;
&lt;th&gt;
index5
&lt;/th&gt;
&lt;th&gt;
freq
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
9049
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
2
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
9049
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
3
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
9384
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
12918
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
687
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
31
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
4758
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
31
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
6421
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
29
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
6
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
9
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
183
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
7
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
11
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
46
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
15
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
11
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
57
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
16
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
9
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
11
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1267
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
11402
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
10
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
11
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1403
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
10842
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;In a comparison of memory needed for all ngram tables, we see a difference of a factor of 5 between the two approaches.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(object.size(df1grams)+object.size(df2grams)+object.size(df3grams)+
          object.size(df4grams)+object.size(df5grams), units = &amp;quot;auto&amp;quot;)
158 Mb

print(object.size(dt1grams)+object.size(dt1gramsByIndex)+object.size(dt2grams)+object.size(dt3grams)+
          object.size(dt4grams)+object.size(dt5grams), units = &amp;quot;auto&amp;quot;)
30.4 Mb&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ngram-with-backoff-prediction-algorithm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ngram With Backoff Prediction Algorithm&lt;/h2&gt;
&lt;p&gt;So, given the ngram data tables, here’s how the prediction algorithm works:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sanitize the input text as done above&lt;/li&gt;
&lt;li&gt;Parse the text into sentences&lt;/li&gt;
&lt;li&gt;Determine the words in the last sentence of the input text. Use upto the last 4 words from the input text.&lt;/li&gt;
&lt;li&gt;The word predictions are done starting from the ngram table containing the longest sequence of words, provided the input text contains atleast n-1 words. So, if the last 4 words from the input text match the first 4 words in the pentagram table, then the fifth word is chosen, that has the highest frequency of occurance after those 4 words. If more than one word prediction is desired, then the fifth words with the next highest frequency of occurance are also chosen.&lt;/li&gt;
&lt;li&gt;If no matches or fewer than desired number of matches are found in the pentagram table, then the algorithm backoffs to the quadrigram table using the last 3 words typed, then to the trigram table using the last 2 words typed, then to the bigram table using the last word typed and finally to the unigram table, until the desired number of word predictions are returned.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s see how this works concretely with some test cases:&lt;/p&gt;
&lt;div id=&#34;test-cases&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Test Cases&lt;/h3&gt;
&lt;p&gt;The first test is to confirm whether the word predictions from the unigram table, match with what we expect.
The top 10 unigram table contains 3 words beginning with the letter ‘t’ - &lt;code&gt;the&lt;/code&gt;(1), &lt;code&gt;to&lt;/code&gt;(4), &lt;code&gt;that&lt;/code&gt;(10). And sure enough, this is what we get from our prediction function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictNextWords(&amp;quot;t&amp;quot;)
[1] &amp;quot;Predictions from 1grams: the,to,that,this,thou&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: the,to,that&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the bigram table, we could see that &lt;code&gt;i am&lt;/code&gt;, &lt;code&gt;i have&lt;/code&gt; and &lt;code&gt;i will&lt;/code&gt; are among the top 10. So, the next test is to confirm these.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictNextWords(&amp;quot;i &amp;quot;)
[1] &amp;quot;Predictions from 2grams: am,have,will,do,would&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: am,have,will&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Narrowing down to words that begin with letter ‘h’ gives us:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictNextWords(&amp;quot;i h&amp;quot;)
[1] &amp;quot;Predictions from 2grams: have,had,hope,hear,heard&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: have,had,hope&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s predict some trigrams. &lt;code&gt;i am not&lt;/code&gt; and &lt;code&gt;i am a&lt;/code&gt; are among the top 10 trigrams, so that’s what we expect to see.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictNextWords(&amp;quot;i am &amp;quot;)
[1] &amp;quot;Predictions from 3grams: not,a,sure,glad,the&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: not,a,sure&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s examine the &lt;strong&gt;UNK&lt;/strong&gt; feature. We know that &lt;code&gt;the UNK of&lt;/code&gt; is number 5 most common sequence in the trigram table so we should expect to see ‘of’ as the first prediction if the input text has an unknown word after ‘the’. So let’s input some gibberish after ‘the’ to make sure this word doesn’t exist in our vocabulary:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictNextWords(&amp;quot;the sjdhsbhbfh &amp;quot;)
[1] &amp;quot;Predictions from 3grams: of,and,in,that,the&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: of,and,in&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that the &lt;em&gt;known&lt;/em&gt; test cases are working well, let’s see the backoff algorithm in action:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictNextWords(&amp;quot;to be or not &amp;quot;)
[1] &amp;quot;Predictions from 5grams: to&amp;quot;
[1] &amp;quot;Predictions from 4grams: to&amp;quot;
[1] &amp;quot;Predictions from 3grams: to,at,i,allow&amp;#39;d,arriv&amp;#39;d&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: to,at,i&amp;quot;
predictNextWords(&amp;quot;to be or not to &amp;quot;)
[1] &amp;quot;Predictions from 5grams: be&amp;quot;
[1] &amp;quot;Predictions from 4grams: be,crack&amp;quot;
[1] &amp;quot;Predictions from 3grams: be,crack,the,have,me,do&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: be,crack,the&amp;quot;
predictNextWords(&amp;quot;be or not to &amp;quot;)
[1] &amp;quot;Predictions from 5grams: be&amp;quot;
[1] &amp;quot;Predictions from 4grams: be,crack&amp;quot;
[1] &amp;quot;Predictions from 3grams: be,crack,the,have,me,do&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: be,crack,the&amp;quot;
predictNextWords(&amp;quot;to be or not to be &amp;quot;)
[1] &amp;quot;Predictions from 5grams: that&amp;quot;
[1] &amp;quot;Predictions from 4grams: that,found,gone,endur&amp;#39;d,endured,seen&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: that,found,gone&amp;quot;
predictNextWords(&amp;quot;to be or not to be, that &amp;quot;)
[1] &amp;quot;Predictions from 5grams: is&amp;quot;
[1] &amp;quot;Predictions from 4grams: is,which&amp;quot;
[1] &amp;quot;Predictions from 3grams: is,which,you,he,i,thou,can&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: is,which,you&amp;quot;
predictNextWords(&amp;quot;to be or not to be, that is &amp;quot;)
[1] &amp;quot;Predictions from 5grams: the&amp;quot;
[1] &amp;quot;Predictions from 4grams: the&amp;quot;
[1] &amp;quot;Predictions from 3grams: the,not,my,to,a&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: the,not,my&amp;quot;
predictNextWords(&amp;quot;to be or not to be, that is the &amp;quot;)
[1] &amp;quot;Predictions from 5grams: question&amp;quot;
[1] &amp;quot;Predictions from 4grams: question,very,way,humour,best,brief&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: question,very,way&amp;quot;
predictNextWords(&amp;quot;be that is the &amp;quot;)
[1] &amp;quot;Predictions from 5grams: question&amp;quot;
[1] &amp;quot;Predictions from 4grams: question,very,way,humour,best,brief&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: question,very,way&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using one of the most well known speeches from Hamlet, we can see that the algorithm gets the first word prediction from the pentagram table. But since we are looking for the next 3 words, the algorithm backoffs to predictions from quadrigram and trigram tables, until it has atleast 3 words. In this case, the pentagram table gives us the best match that we are looking for. We can also observe that once we have atleast 4 words in our input text, the words prior to those do not matter for the next word predictions.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Let’s see another example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictNextWords(&amp;quot;What a lovely day &amp;quot;)
[1] &amp;quot;Predictions from 5grams: &amp;quot;
[1] &amp;quot;Predictions from 4grams: &amp;quot;
[1] &amp;quot;Predictions from 3grams: &amp;quot;
[1] &amp;quot;Predictions from 2grams: and,to,is,of,i&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: and,to,is&amp;quot;
predictNextWords(&amp;quot;a lovely day &amp;quot;)
[1] &amp;quot;Predictions from 4grams: &amp;quot;
[1] &amp;quot;Predictions from 3grams: &amp;quot;
[1] &amp;quot;Predictions from 2grams: and,to,is,of,i&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: and,to,is&amp;quot;
predictNextWords(&amp;quot;lovely day &amp;quot;)
[1] &amp;quot;Predictions from 3grams: &amp;quot;
[1] &amp;quot;Predictions from 2grams: and,to,is,of,i&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: and,to,is&amp;quot;
predictNextWords(&amp;quot;day &amp;quot;)
[1] &amp;quot;Predictions from 2grams: and,to,is,of,i&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: and,to,is&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we see that even though there are enough words to start matching from the higher ngram tables, the matches are gotten only from the bigram table using ‘day’ as the first word.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Another one:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictNextWords(&amp;quot;Beware the ides of &amp;quot;)
[1] &amp;quot;Predictions from 5grams: march&amp;quot;
[1] &amp;quot;Predictions from 4grams: march&amp;quot;
[1] &amp;quot;Predictions from 3grams: march&amp;quot;
[1] &amp;quot;Predictions from 2grams: march,the,my,his,a,this&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: march,the,my&amp;quot;
predictNextWords(&amp;quot;Beware the ides of mar&amp;quot;)
[1] &amp;quot;Predictions from 5grams: march&amp;quot;
[1] &amp;quot;Predictions from 4grams: march&amp;quot;
[1] &amp;quot;Predictions from 3grams: march&amp;quot;
[1] &amp;quot;Predictions from 2grams: march,marriage,marcius,marcus,mars&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: march,marriage,marcius&amp;quot;
predictNextWords(&amp;quot;Beware the ides of march&amp;quot;)
[1] &amp;quot;Predictions from 5grams: march&amp;quot;
[1] &amp;quot;Predictions from 4grams: march&amp;quot;
[1] &amp;quot;Predictions from 3grams: march&amp;quot;
[1] &amp;quot;Predictions from 2grams: march,marching&amp;quot;
[1] &amp;quot;Predictions from 1grams: march,marching,march&amp;#39;d,marches,marchioness&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: march,marching,march&amp;#39;d&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;shiny-app&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Shiny App&lt;/h2&gt;
&lt;p&gt;Finally, here’s a &lt;a href=&#34;https://nitingupta2.shinyapps.io/ptmapp&#34;&gt;Shiny app&lt;/a&gt; that demos this predictive typing model.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;The R markdown file for this post is available &lt;a href=&#34;https://github.com/nitingupta2/nitingupta2.github.io/blob/master/post/building-a-predictive-typing-model/&#34;&gt;here&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Best batsmen in cricket at present</title>
      <link>/post/best-batsmen-in-cricket-at-present/</link>
      <pubDate>Thu, 16 Feb 2017 00:00:00 +0000</pubDate>
      <guid>/post/best-batsmen-in-cricket-at-present/</guid>
      <description>


&lt;hr&gt;
&lt;p&gt;Who’re the best batsmen in cricket today? Is there a way to define a set of objective criteria that provides an unbiased conclusion? In my &lt;a href=&#34;../top-batsmen-in-cricket/&#34;&gt;last post&lt;/a&gt;, I selected a list of top batsmen in the history of cricket, by simply filtering all batsmen who have had a career batting average of 50 or more in any format. Prior to that I eliminated newcomers, specialist bowlers and unsuccessful players by another set of filters.&lt;/p&gt;
&lt;p&gt;I got a list of 42 batsmen who have consistently performed well throughout their career. Now the question is how could we select the best of the best who have had the most well-rounded performances. A post on &lt;a href=&#34;https://fivethirtyeight.com/features/kawhi-leonard-is-the-most-well-rounded-elite-shooter-since-larry-bird/&#34;&gt;fivethirtyeight&lt;/a&gt; has been a strong inspiration to arrive at the answers below.&lt;/p&gt;
&lt;div id=&#34;best-batsmen-at-present&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Best batsmen at present&lt;/h3&gt;
&lt;p&gt;To select current best batsmen we filter the list of top batsmen to include only the batsmen who are actively playing in all the 3 formats of the game.&lt;/p&gt;
&lt;p&gt;This leaves us with only 6 batsmen who are currently playing in all the formats. They are AB de Villiers, HM Amla, JE Root, KS Williamson, SPD Smith, V Kohli. No surprises there as these batsmen are widely considered to be the best among the active players today. But instead of subjectively handpicking them, we arrived at this conclusion by applying some reasonable filters.&lt;/p&gt;
&lt;p&gt;But how well-rounded are these players? To determine this I’ve defined the following criteria and metrics:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;51%&#34; /&gt;
&lt;col width=&#34;30%&#34; /&gt;
&lt;col width=&#34;17%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Criteria&lt;/th&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;Label&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Consistency of scoring runs&lt;/td&gt;
&lt;td&gt;Career batting average&lt;/td&gt;
&lt;td&gt;SCORING CONSISTENCY&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Ability to score quickly&lt;/td&gt;
&lt;td&gt;Strike Rate&lt;/td&gt;
&lt;td&gt;STRIKE RATE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Ability to score big (number of 100s + number of 50s scored)&lt;/td&gt;
&lt;td&gt;Big scores per innings&lt;/td&gt;
&lt;td&gt;BIG SCORING RATE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Overall contribution to the matches played&lt;/td&gt;
&lt;td&gt;Team Win-Loss Ratio&lt;/td&gt;
&lt;td&gt;WIN RATIO&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Direct contribution to matches won&lt;/td&gt;
&lt;td&gt;Batting average in winning matches&lt;/td&gt;
&lt;td&gt;WIN CONTRIBUTION&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Every metric defined here is a ratio instead of an absolute number. So it is useful to evaluate relative ability.&lt;/p&gt;
&lt;p&gt;Among the 42 top batsmen selected before, I calculate percentile scores of these metrics for each batsman grouped by the format of the game. This means we have a score ranging from 0-100 for each metric, where each batsman in the list has been scored relative to the best batsman in each criteria.&lt;/p&gt;
&lt;p&gt;Let’s see the percentile scores for the 6 champion batsmen in our final list.&lt;/p&gt;
&lt;!-- html table generated in R 3.6.3 by xtable 1.8-4 package --&gt;
&lt;!-- Thu Jun 11 14:56:45 2020 --&gt;
&lt;table border=&#34;1&#34;&gt;
&lt;caption align=&#34;bottom&#34;&gt;
Percentile Scores
&lt;/caption&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
Format
&lt;/th&gt;
&lt;th&gt;
Country
&lt;/th&gt;
&lt;th&gt;
Player
&lt;/th&gt;
&lt;th&gt;
PercAverage
&lt;/th&gt;
&lt;th&gt;
PercStrikeRate
&lt;/th&gt;
&lt;th&gt;
PercInningsPerBigScore
&lt;/th&gt;
&lt;th&gt;
PercWinRatio
&lt;/th&gt;
&lt;th&gt;
PercAverageInWins
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
Tests
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
South Africa
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
AB de Villiers
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
20
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
57
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
27
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
61
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
47
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
2
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
ODIs
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
South Africa
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
AB de Villiers
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
100
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
100
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
86
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
72
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
93
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
3
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
T20Is
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
South Africa
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
AB de Villiers
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
36
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
73
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
31
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
88
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
32
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
Tests
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
South Africa
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
HM Amla
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
40
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
22
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
74
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
30
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
ODIs
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
South Africa
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
HM Amla
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
83
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
90
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
93
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
79
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
83
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
6
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
T20Is
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
South Africa
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
HM Amla
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
59
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
62
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
62
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
44
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
50
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
7
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
Tests
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
England
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
JE Root
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
54
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
70
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
81
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
20
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
86
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
ODIs
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
England
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
JE Root
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
76
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
73
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
89
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
15
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
86
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
9
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
T20Is
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
England
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
JE Root
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
89
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
67
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
70
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
32
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
75
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
10
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
Tests
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
New Zealand
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
KS Williamson
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
32
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
44
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
76
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
15
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
76
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
11
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
ODIs
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
New Zealand
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
KS Williamson
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
69
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
69
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
82
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
29
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
75
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
12
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
T20Is
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
New Zealand
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
KS Williamson
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
71
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
50
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
77
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
75
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
63
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
13
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
Tests
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
Australia
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
SPD Smith
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
98
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
83
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
86
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
37
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
91
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
14
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
ODIs
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
Australia
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
SPD Smith
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
56
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
83
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
52
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
75
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
58
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
15
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
T20Is
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
Australia
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
SPD Smith
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
18
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
45
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
57
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
19
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
16
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
Tests
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
India
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
V Kohli
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
10
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
79
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
64
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
17
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
ODIs
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
India
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
V Kohli
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
94
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
97
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
97
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
65
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
97
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
18
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
T20Is
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
India
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
V Kohli
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
100
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
84
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
93
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
94
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
94
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Here we can see how each champion scored relative to the rest of the elite group. AB de Villiers has the best career batting average and strike rate in ODIs so he gets a score of 100 in SCORING CONSISTENCY &amp;amp; STRIKE RATE. All other batsmen are scored relative to AB in these 2 criteria. With his astounding batting average of 99.94 in Tests, Sir Don Bradman gets a score of 100 in SCORING CONSISTENCY in Tests. And being at number 5 in all time highest batting averages in Tests, Steve Smith gets a high score of 91 in this criteria, relative to Sir Don.&lt;/p&gt;
&lt;p&gt;This is all well and good, but how do we see the forest from trees? It is very difficult to make sense of high dimensional data in a table or in a single bar plot. This is where the visualization technique shown in the &lt;a href=&#34;https://fivethirtyeight.com/features/kawhi-leonard-is-the-most-well-rounded-elite-shooter-since-larry-bird/&#34;&gt;fivethirtyeight&lt;/a&gt; article shines. We build a wagon wheel with segments denoting each metric and within each segment, the data is binned by quartiles.&lt;/p&gt;
&lt;p&gt;Here’s how a wagon wheel would look like for each batsman. Each segment would be filled depending upon the percentile score in each criteria.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/best-batsmen-in-cricket-at-present/index_files/figure-html/plotWagonWheel-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So let’s see how the wagon wheels of our champions look like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/player_facets_Tests.png&#34; /&gt;
&lt;img src=&#34;/img/player_facets_ODIs.png&#34; /&gt;
&lt;img src=&#34;/img/player_facets_T20Is.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that Steve Smith is the most well-rounded batsman in Tests. His WIN RATIO is lacking relative to the illustrious company he’s in, but overall his performance in Tests is phenomenal. Joe Root comes second but others have a bit more catching up to do.&lt;/p&gt;
&lt;p&gt;In One Day Internationals, Hashim Amla dominates in all spheres with AB de Villiers and Virat Kohli not far behind. Joe Root seems to contribute in most matches won by England but it appears the English team loses a fair bit in ODIs. Hence the WIN RATIO of Root is a bit lacking.&lt;/p&gt;
&lt;p&gt;Virat Kohli is the undisputed king in the T20 Internationals format with Kane Williamson in second place. Steve Smith’s performance so far leaves much to be desired in this format.&lt;/p&gt;
&lt;p&gt;The beauty of this visualization technique is that several dimensions in the data are compressed in a single facetted plot, in a visually appealing way. A glance at the plot reveals the relative best, both overall and in each metric.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Top batsmen in cricket</title>
      <link>/post/top-batsmen-in-cricket/</link>
      <pubDate>Sat, 11 Feb 2017 00:00:00 +0000</pubDate>
      <guid>/post/top-batsmen-in-cricket/</guid>
      <description>


&lt;hr&gt;
&lt;p&gt;In a &lt;a href=&#34;../a-brief-history-of-cricket/&#34;&gt;previous post&lt;/a&gt;, I used data from &lt;a href=&#34;http://stats.espncricinfo.com/ci/engine/stats/index.html&#34;&gt;Statsguru&lt;/a&gt; and looked at a brief history of cricket with respect to debut years and career spans of players. In this post, I use detailed player statistics from the same dataset to select top batsmen who have played this game.&lt;/p&gt;
&lt;div id=&#34;distribution-of-matches-played-by-all-players&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Distribution of matches played by all players&lt;/h3&gt;
&lt;p&gt;Let’s take a look at the summary statistics of number of matches played by all players in each format of the game.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Number of matches played by all players in each format
Format: Tests
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   1.00    2.00    7.00   17.27   21.00  200.00 
--------------------------------------------------------------------------- 
Format: ODIs
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   1.00    4.00   16.00   42.56   53.00  463.00 
--------------------------------------------------------------------------- 
Format: T20Is
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
    1.0     2.0     7.0    13.9    18.5    98.0 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/top-batsmen-in-cricket/index_files/figure-html/matchesPlayed-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The distribution appears to be heavily skewed towards the right. In simple terms, there are a handful of players who go on to play 100+ Tests and 200+ ODIs. A majority of the players have played very few matches. The mean and median are hardly indicative of players who established themselves in their teams.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;distribution-of-innings-to-matches-played-by-all-players&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Distribution of innings to matches played by all players&lt;/h3&gt;
&lt;p&gt;A cricket team has 11 players, out of which 5 or 6 are specialist batsmen, 1 or 2 are all-rounders (could bat and bowl well) and the rest are specialist bowlers. The wicketkeeper also has to be a good batsman. The batsmen play up in the order in a match and get to bat before the bowlers do.&lt;/p&gt;
&lt;p&gt;Here I will introduce another measure to distinguish batsmen from bowlers - ratio of innings played relative to number of matches played. In many matches a bowler doesn’t get a chance to bat, unless the top order batsmen and all-rounders fail, i.e. are out. Hence the ratio of innings to matches of specialist bowlers will be low.&lt;/p&gt;
&lt;p&gt;Here are the summary statistics of this ratio. The ratio is greater than 1 in Tests because each player could play upto 2 innings. The peaks in each plot are indicative of batsmen who have batted in the top 3 and have gotten a chance to bat in nearly every innings of every match they played.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Ratio of innings to matches of all players in each format
Format: Tests
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&amp;#39;s 
  0.250   1.429   1.667   1.623   1.923   2.000      19 
--------------------------------------------------------------------------- 
Format: ODIs
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&amp;#39;s 
 0.1250  0.5714  0.8421  0.7670  1.0000  1.0000      62 
--------------------------------------------------------------------------- 
Format: T20Is
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&amp;#39;s 
0.05556 0.50000 0.85714 0.73084 1.00000 1.00000      73 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/top-batsmen-in-cricket/index_files/figure-html/inningsToMatches-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;distinguish-batsmen-from-bowlers&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Distinguish batsmen from bowlers&lt;/h3&gt;
&lt;p&gt;To identify true batsmen in the dataset, I use 2 filters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number of matches played should be above the 75th percentile&lt;/li&gt;
&lt;li&gt;Ratio of innings to matches played should be above the 25th percentile&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first filter eliminates newcomers and players who have had very short-lived careers. The second filter eliminates specialist bowlers from the dataset.&lt;/p&gt;
&lt;p&gt;Let’s take a look at the distribution of these metrics for these batsmen. These numbers appear more reasonable.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/top-batsmen-in-cricket/index_files/figure-html/matchesByBatsmen-1.png&#34; width=&#34;1056&#34; /&gt;&lt;img src=&#34;/post/top-batsmen-in-cricket/index_files/figure-html/matchesByBatsmen-2.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;career-averages-of-batsmen&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Career averages of batsmen&lt;/h3&gt;
&lt;p&gt;Let’s take a look at the career batting averages of batsmen in each format. We see a few low averaging players left in the dataset who are potentially specialist bowlers. But without resorting to hand-picking, the two empirical filters chosen earlier did a good job of eliminating specialist bowlers from the dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Career batting averages of batsmen in each format
Format: Tests
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   4.84   24.84   33.59   33.11   42.13   99.94 
--------------------------------------------------------------------------- 
Format: ODIs
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   6.63   23.11   29.71   29.55   35.88   55.03 
--------------------------------------------------------------------------- 
Format: T20Is
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   4.10   18.15   21.91   23.27   28.44   53.40 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/top-batsmen-in-cricket/index_files/figure-html/careerAverages-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;top-batsmen&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Top batsmen&lt;/h3&gt;
&lt;p&gt;The hallmark of a top batsman is consistency in scoring runs. From the distribution of career averages, it could be seen that very few batsmen have managed to score at an average of 50 or more runs in their career. So keeping things simple, this would be my sole criteria for selecting top batsmen amongst the rest. All batsmen who have averaged 50 or more in &lt;strong&gt;any&lt;/strong&gt; format would be included in my list of top batsmen.&lt;/p&gt;
&lt;p&gt;This gives us a unique list of 42 elite batsmen who are legendary in their achievements.&lt;/p&gt;
&lt;p&gt;Let’s take a look how they compare by various stats.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;div id=&#34;career-span&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Career Span&lt;/h4&gt;
&lt;p&gt;The career span of these elite batsmen is plotted in the order in which they debuted in their international career. Career spans of batsmen who are still actively playing in any format are labeled with blue color.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/top-batsmen-in-cricket/index_files/figure-html/compareCareerSpan-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Summary of career spans in years of top batsmen
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   5.00   11.00   16.00   14.71   18.75   24.00 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sachin Tendulkar and George Headley have had the longest career spans of 24 years each. But the international career of Headley along with that of almost all of the early era greats were interrupted by world wars. So even though they resumed their careers after the end of the wars, they were able to play relatively few matches (more on this below).&lt;/p&gt;
&lt;p&gt;Graeme Pollock has had the shortest career of 7 years amongst all retired players. Being a South African, his career was cut short when ICC suspended South Africa from competing internationally in 1970 because of apartheid.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;number-of-matches-played&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Number of matches played&lt;/h4&gt;
&lt;p&gt;Among the many records in Tendulkar’s name, the number of Tests and ODIs played are at the forefront. He played a single T20I, because he, Dravid and other senior members of the then Indian team used to withdraw their names for team selection in T20Is. His records of playing in 200 Tests and 463 ODIs are a testament to his brilliance, his passion for the game and his fitness over the years. Among the active players only AB de Villiers and Hashim Amla are at the halfway mark relatively to Tendulkar’s mark in Tests. But both AB and Amla are already well into their prime years. Younis Khan is on the verge of riding into the sunset. The rest of the pack including Virat Kohli, Kane Williamson, Steve Smith have a looong way to catch up.&lt;/p&gt;
&lt;p&gt;It could be observed how few matches did the early era greats play relatively. Not only were few teams competing at the time, but also interruptions by world wars.&lt;/p&gt;
&lt;p&gt;Michael Bevan established himself as an ODI specialist and played only 18 tests. Cheteshwar Pujara appears to have barely made the cutoffs for this list. He is a Tests specialist.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/top-batsmen-in-cricket/index_files/figure-html/compareTopBatsmen-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;career-runs-scored&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Career runs scored&lt;/h4&gt;
&lt;p&gt;Once again Tendulkar’s records here are at the top. It is highly unlikely that any other batsman could ever come close in the next 5-10 years.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/top-batsmen-in-cricket/index_files/figure-html/compareRuns-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;career-batting-averages&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Career batting averages&lt;/h4&gt;
&lt;p&gt;First and foremost is Sir Don Bradman’s career batting average record of 99.94 in Tests. He is widely regarded to be the greatest batsman of all time. This is one record that’s impossible to beat. The second batsman on this list is Graeme Pollock at 60.97. Steve Smith is the only active batsman who is anywhere close to second, a phenomenal achievement in its own right.&lt;/p&gt;
&lt;p&gt;Secondly, it becomes clear why each batsman was selected for this list, a batting average of 50 or more in any single format. Though most batsmen have a Tests average above 50, Jonathan Trott, MS Dhoni and Michael Bevan made it by virtue of their superlative averages in the ODIs.&lt;/p&gt;
&lt;p&gt;Virat Kohli is the only batsman who has averaged above 50 in all the 3 formats.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/top-batsmen-in-cricket/index_files/figure-html/compareAverage-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hundreds-scored-in-career&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Hundreds scored in career&lt;/h4&gt;
&lt;p&gt;Once again Tendulkar leads in the standings with 51 hundreds in Tests and 49 hundreds in ODIs. Sir Don’s record Test average of 99.94, Tendulkar’s records of number of matches played, runs scored and hundreds scored in career are some of the records that are going to be next to impossible to beat.&lt;/p&gt;
&lt;p&gt;Surprisingly none of the batsmen on this list have scored a hundred in T20Is. There are a few other batsmen who have scored hundreds in T20Is but none of them were consistent enough to make it to this list.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/top-batsmen-in-cricket/index_files/figure-html/compareHundreds-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;highest-scores-in-an-innings&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Highest scores in an innings&lt;/h4&gt;
&lt;p&gt;Here we see the great Brian Lara at the top of the Tests table with a highest score of 400 not out. I remember Sir Garfield Sobers’ score of 365 not out was one of the longest standing records in Test cricket until Lara surpassed it. Only Matthew Hayden was able to come any closer to Lara.&lt;/p&gt;
&lt;p&gt;In ODIs, it was apt that the champion, Sachin Tendulkar was the first one to score a double hundred. A few other batsmen have scored 200+ scores in ODIs since, but none have had the consistency that Tendulkar had.&lt;/p&gt;
&lt;p&gt;In T20Is, as mentioned above none of these batsman scored a hundred. Though a few have come close. I expect some of the active players to reach this milestone in the near future.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/top-batsmen-in-cricket/index_files/figure-html/compareHighestScores-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A brief history of cricket</title>
      <link>/post/a-brief-history-of-cricket/</link>
      <pubDate>Wed, 08 Feb 2017 00:00:00 +0000</pubDate>
      <guid>/post/a-brief-history-of-cricket/</guid>
      <description>


&lt;p&gt;Since early childhood, I have been a big fan of cricket. Some of my earliest childhood memories are watching the game along with my loved ones in India. As much as I can recall, the first series I saw was between India and the West Indies during the early 80s, on a black and white TV set. Watching Sunil Gavaskar play the fearsome West Indian fast bowlers with aplomb, made me his fan…until a kid named Sachin Tendulkar arrived in 1989.&lt;/p&gt;
&lt;p&gt;As the saying goes…In India, cricket is religion and Tendulkar is God! Even after his retirement in 2013, Tendulkar is still the most loved sportsperson or celebrity in India. A billion people passionately follow the game and voice their opinion on matters of team selection, players’ performances, umpiring…you name it. Armchair analysts abound, especially if the Indian cricket team loses a consequential match or a series.&lt;/p&gt;
&lt;p&gt;I was excited to find that detailed records and stats on all cricket players were available on &lt;a href=&#34;http://stats.espncricinfo.com/ci/engine/stats/index.html&#34;&gt;Statsguru&lt;/a&gt;. The database covers all formats of the game:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tests - Test matches (80 overs a day, each team playing upto 2 innings in 5 days)&lt;/li&gt;
&lt;li&gt;ODIs - One-Day Internationals (50 overs an innings)&lt;/li&gt;
&lt;li&gt;T20Is - Twenty20 Internationals (20 overs an innings)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this post, I’ve tried to examine a brief history of the game using the debut years and career spans of players.&lt;/p&gt;
&lt;p&gt;Here’s a sample of batting data:&lt;/p&gt;
&lt;!-- html table generated in R 3.6.3 by xtable 1.8-4 package --&gt;
&lt;!-- Thu Jun 11 14:54:44 2020 --&gt;
&lt;table border=&#34;1&#34;&gt;
&lt;caption align=&#34;bottom&#34;&gt;
Batting records of players who have represented Australia in ODIs
&lt;/caption&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
Player
&lt;/th&gt;
&lt;th&gt;
Span
&lt;/th&gt;
&lt;th&gt;
Mat
&lt;/th&gt;
&lt;th&gt;
Inns
&lt;/th&gt;
&lt;th&gt;
NO
&lt;/th&gt;
&lt;th&gt;
Runs
&lt;/th&gt;
&lt;th&gt;
HS
&lt;/th&gt;
&lt;th&gt;
Ave
&lt;/th&gt;
&lt;th&gt;
BF
&lt;/th&gt;
&lt;th&gt;
SR
&lt;/th&gt;
&lt;th&gt;
100
&lt;/th&gt;
&lt;th&gt;
50
&lt;/th&gt;
&lt;th&gt;
0
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
RT Ponting
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1995-2012
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
374
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
364
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
39
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
13589
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
164
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
41.81
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
16944
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
80.19
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
29
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
82
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
2
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
AC Gilchrist
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1996-2008
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
286
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
278
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
11
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
9595
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
172
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
35.93
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
9902
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
96.89
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
16
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
55
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
19
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
3
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
ME Waugh
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1988-2002
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
244
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
236
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
20
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
8500
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
173
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
39.35
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
11053
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
76.9
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
18
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
50
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
16
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
MJ Clarke
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2003-2015
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
245
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
223
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
44
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
7981
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
130
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
44.58
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
10104
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
78.98
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
58
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
SR Waugh
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1986-2002
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
325
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
288
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
58
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
7569
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
120*
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
32.90
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
9971
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
75.91
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
3
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
45
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
15
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
6
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
MG Bevan
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1994-2004
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
232
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
196
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
67
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
6912
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
108*
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
53.58
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
9320
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
74.16
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
6
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
46
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Here’s a sample of bowling data:&lt;/p&gt;
&lt;!-- html table generated in R 3.6.3 by xtable 1.8-4 package --&gt;
&lt;!-- Thu Jun 11 14:54:44 2020 --&gt;
&lt;table border=&#34;1&#34;&gt;
&lt;caption align=&#34;bottom&#34;&gt;
Bowling records of players who have represented Australia in ODIs
&lt;/caption&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
Player
&lt;/th&gt;
&lt;th&gt;
Span
&lt;/th&gt;
&lt;th&gt;
Mat
&lt;/th&gt;
&lt;th&gt;
Inns
&lt;/th&gt;
&lt;th&gt;
Balls
&lt;/th&gt;
&lt;th&gt;
Runs
&lt;/th&gt;
&lt;th&gt;
Wkts
&lt;/th&gt;
&lt;th&gt;
BBI
&lt;/th&gt;
&lt;th&gt;
Ave
&lt;/th&gt;
&lt;th&gt;
Econ
&lt;/th&gt;
&lt;th&gt;
SR
&lt;/th&gt;
&lt;th&gt;
4
&lt;/th&gt;
&lt;th&gt;
5
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
B Lee
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2000-2012
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
221
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
217
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
11185
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
8877
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
380
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
5/22
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
23.36
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
4.76
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
29.4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
14
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
9
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
2
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
GD McGrath
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1993-2007
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
249
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
247
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
12928
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
8354
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
380
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
7/15
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
21.98
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
3.87
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
34.0
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
9
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
3
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
SK Warne
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1993-2003
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
193
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
190
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
10600
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
7514
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
291
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
5/33
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
25.82
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
4.25
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
36.4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
12
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
MG Johnson
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2005-2015
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
153
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
150
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
7489
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
6038
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
239
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
6/31
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
25.26
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
4.83
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
31.3
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
9
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
CJ McDermott
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1985-1996
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
138
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
138
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
7461
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
5018
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
203
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
5/44
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
24.71
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
4.03
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
36.7
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
6
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
SR Waugh
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1986-2002
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
325
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
207
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
8883
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
6761
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
195
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
4/33
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
34.67
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
4.56
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
45.5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
3
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Currently there are 10 international teams which have &lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_International_Cricket_Council_members&#34;&gt;full-membership&lt;/a&gt; of the International Cricket Council - Australia, Bangladesh, England, India, New Zealand, Pakistan, South Africa, Sri Lanka, West Indies and Zimbabwe. I downloaded batting and bowling data for all of these 10 teams in all 3 formats. The dataset is rich with features and appears to cover the entire history of international cricket.&lt;/p&gt;
&lt;p&gt;Major props to the folks at Statsguru for maintaining a clean dataset. Both batting and bowling datasets have the same number of players, as should be the case. So there wasn’t much data wrangling needed.&lt;/p&gt;
&lt;p&gt;Here are what the abbreviations mean:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;10%&#34; /&gt;
&lt;col width=&#34;89%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Term&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Span&lt;/td&gt;
&lt;td&gt;Separate into debut year and last year of career&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Mat&lt;/td&gt;
&lt;td&gt;Matches played in career&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Inns&lt;/td&gt;
&lt;td&gt;Innings played in career&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;NO&lt;/td&gt;
&lt;td&gt;Number of times not out in career&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;HS&lt;/td&gt;
&lt;td&gt;Highest batting score in career&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Ave&lt;/td&gt;
&lt;td&gt;Career Average, batting: Runs/(Innings - NotOuts), bowling: Runs/Wickets&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;BF&lt;/td&gt;
&lt;td&gt;Balls Faced&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;SR&lt;/td&gt;
&lt;td&gt;Strike Rate, batting: Runs/Balls Faced, bowling: Balls/Wickets&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;BBI&lt;/td&gt;
&lt;td&gt;Best bowling performance in an innings&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Balls&lt;/td&gt;
&lt;td&gt;Number of balls bowled in career&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Wkts&lt;/td&gt;
&lt;td&gt;Number of wickets in career&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Econ&lt;/td&gt;
&lt;td&gt;Bowling economy rate: Runs per over (Overs bowled = Balls bowled/6)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;Number of hundreds scored in career&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;Number of fifties scored in career&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;Number of times the bowler took 4 wickets in an innings&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;Number of times the bowler took 5 or more wickets in an innings&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;when-did-each-country-first-play-internationally&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;When did each country first play internationally?&lt;/h3&gt;
&lt;p&gt;From the earliest debut years of players, it could be seen when each format was started and when each country started playing in a format. Australia and England have been the pioneering nations in each format. The earliest Tests were played between Australia and England in 1877. South Africa joined the fray in 1889. For the next 40 years, these were the only 3 international teams, until West Indies, India and New Zealand started playing in quick succession. In 1970 South Africa was suspended from playing international cricket by the ICC because of prevailing apartheid in the nation. As a result, the international career of some very promising South Africans like Graeme Pollock and Barry Richards ended abruptly. Later on some other players emigrated to England and Australia to get a chance to play internationally. This ban was lifted by the ICC in 1991. The newly formed South African team under Clive Rice was given a warm welcome in India, when they visited to play ODIs. South Africa resumed playing tests in 1992.&lt;/p&gt;
&lt;p&gt;Sri Lanka, Zimbabwe and Bangladesh were Associate members with ODI status before they were elected as full members to be eligible to play Tests.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a-brief-history-of-cricket/index_files/figure-html/earliestDebutByCountry-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;number-of-players-who-have-represented-their-country-internationally&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Number of players who have represented their country internationally&lt;/h3&gt;
&lt;p&gt;Even though Australia and England started playing Tests together in 1877, there have been far more players representing the England team in Tests, when looking at the entire history. This suggests a lot more turnover in the English team.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a-brief-history-of-cricket/index_files/figure-html/numberOfPlayersByCountry-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;number-of-players-who-debuted-internationally-by-year&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Number of players who debuted internationally by year&lt;/h3&gt;
&lt;p&gt;This plot shows the number of distinct players debuting each year. So, for instance, if a player debuted in Tests, ODIs and T20Is team in the same year, then that is counted as 1. Few things could be noted here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The gaps in Australia, England and India facets show the years interrupted by the 2 world wars.&lt;/li&gt;
&lt;li&gt;As noted earlier, there’s a big gap in the South Africa facet, when the country was suspended from 1970-1991.&lt;/li&gt;
&lt;li&gt;The gaps in Bangladesh, Sri Lanka and Zimbabwe are because each country was an associate member of the ICC and was only competing in ICC ODI tournaments until the country got its full member status.&lt;/li&gt;
&lt;li&gt;There’s been a steady increase in the number of players debuting in Australia, England, India, Pakistan and West Indies, in the last decade or so. This suggests a trend of having specialist players in the team for each format. Top teams like Australia, England and India have adopted this trend. Some players are selected as Test match specialists, while the others are selected to only play in ODIs and T20Is.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/img/cricket_facets.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;career-spans-of-players&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Career spans of players&lt;/h3&gt;
&lt;p&gt;To examine the career spans of players, I filtered the dataset to only keep the players who haven’t been active in 2016-2017. Then I eliminated quick failures whose career didn’t last for more than 2 years. This meant the players left in the dataset were given enough opportunities to succeed at the international level. Then I examined the median career span by debut year.&lt;/p&gt;
&lt;p&gt;It appears a median player used to play between 6 to 10 years before retiring. Although there was more dispersion in Tests, it was clearly the norm in ODIs. 1908 was an extreme outlier year in Tests because only 1 player qualified the inclusion criteria. This was Sir Jack Hobbs of England, an early master of the game who played for 23 years. 1909 was another extreme outlier year where only 3 players were included. These were the legends Warren Bardsley of Australia (18 years), Frank Woolley of England (26 years) and Bill Whitty of Australia (4 years).&lt;/p&gt;
&lt;p&gt;An outlier in the ODIs plot corresponds to the year 1989. This was the year when some modern day legends made their debut. Most notably Sachin Tendulkar from India who played for 24 years. Saeed Anwar, Waqar Younis &amp;amp; Mushtaq Ahmed from Pakistan who each played for 15 years and Sanath Jayasuriya from Sri Lanka who played for 23 years. Alec Stewart and Nasser Hussain from England also debuted in 1989. They had checkered records in ODIs but overall their careers lasted for 15 years.&lt;/p&gt;
&lt;p&gt;An astonishing trend appears to have started with the players debuting after the early 90s. Career spans of players peaked with those debuting during early 90s. Since then the median career span started dropping year by year. This could be seen across all formats. Although the median career span would change for more recent years as active players playing from early 2000s retire in future, nevertheless this trend is persistent from early 90s.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a-brief-history-of-cricket/index_files/figure-html/medianCareerSpan-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matches-played-by-debut-years&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Matches played by debut years&lt;/h3&gt;
&lt;p&gt;The same downwards trends from early 90s could also be observed in the median number of matches played by debut year. This isn’t surprising as the number of matches played is directly correlated with career span. An upward trend from early 30s could be observed in Test matches played. This is explained by the addition of West Indies, New Zealand and India as full members of the ICC from late 20s. Every international player started playing more matches as the number of competing teams doubled to 6.&lt;/p&gt;
&lt;p&gt;A big upward trend could be seen in ODIs from its inception. Until the beginning of ODIs, cricket was only played in Tests format that was characteristically very laid back. Both teams got a chance to bat upto 2 innings alternately over 5 days. More often than not the matches would end in a draw with no win/loss result. The introduction of 60 overs a side ODIs (later reduced to 50 overs a side) format was to be played in a single day. The team batting first would set a target score. Then the team batting second would chase this target. This ensured guaranteed results although there have been few occasions where the ODIs have resulted in ties at the same score. Naturally this format was more palatable to spectators and TV audiences that contributed to a rapid growth in its popularity. The ODIs format is still the most popular format till date.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a-brief-history-of-cricket/index_files/figure-html/matchesByDebutYear-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the next couple of posts I will analyze batting performances.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Ames Housing - Part 2 - Building Models</title>
      <link>/casestudies/ames-housing-part2-models/</link>
      <pubDate>Mon, 26 Dec 2016 00:00:00 +0000</pubDate>
      <guid>/casestudies/ames-housing-part2-models/</guid>
      <description>


&lt;p&gt;In a &lt;a href=&#34;https://www.nitingupta.com/casestudies/ames-housing-part1-eda/&#34;&gt;previous post&lt;/a&gt; in this series, we did an exploratory data analysis of the &lt;a href=&#34;http://www.amstat.org/publications/jse/v19n3/decock.pdf&#34;&gt;Ames Housing dataset&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this post, we will build linear and non-linear models and see how well they predict the &lt;code&gt;SalePrice&lt;/code&gt; of properties.&lt;/p&gt;
&lt;div id=&#34;evaluation-criteria&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Evaluation Criteria&lt;/h2&gt;
&lt;p&gt;Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed &lt;code&gt;SalePrice&lt;/code&gt; will be our evaluation criteria. Taking the log ensures that errors in predicting expensive and cheap houses will affect the result equally.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;steps-for-building-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Steps for Building Models&lt;/h2&gt;
&lt;p&gt;Here are the steps for building models and determining the best hyperparameter combinations by K-fold cross validation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Partition the training dataset into model training and validation sets. Use stratified sampling such that each partition has a similar distribution of the target variable - &lt;code&gt;SalePrice&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Define linear and non-linear models.&lt;/li&gt;
&lt;li&gt;For each model, create a grid of hyperparameter combinations that are equally spaced.&lt;/li&gt;
&lt;li&gt;For each hyperparameter combination, fit a model on the training set and make predictions on the validation set. Repeat the process for all folds.&lt;/li&gt;
&lt;li&gt;Determine root mean squared errors (RMSE) and choose the best hyperparameter combination that corresponds to the minimum RMSE.&lt;/li&gt;
&lt;li&gt;Train each model with its best hyperparameter combination on the entire training set.&lt;/li&gt;
&lt;li&gt;Calculate RMSE of the each finalized model on the testing set.&lt;/li&gt;
&lt;li&gt;Finally, choose the best model that gives the least RMSE.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;partitioning-training-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Partitioning Training Data&lt;/h2&gt;
&lt;p&gt;We split the training data into 4 folds. Within each fold, 75% of the data is used for training models and 25% for validating the predicted values against the actual values.&lt;/p&gt;
&lt;p&gt;Let’s look at the distribution of the target variable across all folds:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part2-models/index_files/figure-html/plot_target_partitioning-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;By using stratified sampling, we ensure that the training and validation distributions of the target variable are similar.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear Models&lt;/h2&gt;
&lt;div id=&#34;ordinary-least-squares-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ordinary Least Squares Regression&lt;/h3&gt;
&lt;p&gt;Before creating any new features or indulging in more complex modelling methods, we will cross validate a simple linear model on the training data to establish a benchmark. If more complex approaches do not have a significant improvement in the model validation metrics, then they are not worthwhile to be pursued.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Linear Regression Model Specification (regression)

Computational engine: lm &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;whats-notable&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What’s notable?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;After training a linear model on all predictors, we get an RMSE of &lt;strong&gt;0.1468&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;This is the simplest and fastest model with no hyperparameters to tune.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;regularized-linear-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regularized Linear Model&lt;/h3&gt;
&lt;p&gt;We will use &lt;code&gt;glmnet&lt;/code&gt; that uses LASSO and Ridge Regression with regularization. We will do a grid search of the following hyperparameters that minimize RMSE:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;penalty&lt;/code&gt;: The total amount of regularization in the model.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mixture&lt;/code&gt;: The proportion of L1 regularization in the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;Linear Regression Model Specification (regression)

Main Arguments:
  penalty = tune()
  mixture = tune()

Computational engine: glmnet &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the top 10 RMSE values and hyperparameter combinations:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 10 x 3
    penalty mixture mean_rmse
      &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
 1 4.83e- 3  0.922      0.127
 2 3.79e- 2  0.0518     0.129
 3 1.36e- 3  0.659      0.132
 4 1.60e- 3  0.431      0.133
 5 3.50e- 3  0.177      0.133
 6 4.17e- 2  0.288      0.133
 7 5.67e- 4  0.970      0.133
 8 6.79e- 9  0.0193     0.138
 9 4.32e-10  0.337      0.138
10 1.95e- 6  0.991      0.138&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;whats-notable-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What’s notable?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;After hyperparameter tuning with cross validation, &lt;code&gt;glmnet&lt;/code&gt; gives the best RMSE of 0.127 with penalty = 0.0048 and mixture = 0.9216.&lt;/li&gt;
&lt;li&gt;It is a significant improvement over Ordinary Least Squares regression that had an RMSE of 0.1468.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;glmnet&lt;/code&gt; cross validation takes under a minute to execute.&lt;/li&gt;
&lt;li&gt;But the presence of outliers can significantly affect its performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here a plot of the &lt;code&gt;glmnet&lt;/code&gt; hyperparameter grid along with the best hyperparameter combination:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part2-models/index_files/figure-html/plot_glmnet-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;non-linear-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Non-linear Models&lt;/h2&gt;
&lt;p&gt;Next, we will train a couple of tree-based algorithms, which are not very sensitive to outliers and skewed data.&lt;/p&gt;
&lt;div id=&#34;randomforest&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;em&gt;randomForest&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;In each ensemble, we have 1000 trees and do a grid search of the following hyperparameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;mtry&lt;/code&gt;: The number of predictors to randomly sample at each split.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;min_n&lt;/code&gt;: The minimum number of data points in a node required to further split the node.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;Random Forest Model Specification (regression)

Main Arguments:
  mtry = tune()
  trees = 1000
  min_n = tune()

Engine-Specific Arguments:
  objective = reg:squarederror

Computational engine: randomForest &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the top 10 RMSE values and hyperparameter combinations:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 10 x 3
   min_n  mtry mean_rmse
   &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;
 1     4    85     0.134
 2     3   140     0.135
 3    14    90     0.135
 4     6    45     0.136
 5     9   138     0.136
 6    13   158     0.137
 7     9   183     0.137
 8    19    56     0.138
 9    21   130     0.138
10     5   218     0.138&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;whats-notable-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What’s notable?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;After cross validation, we get the best RMSE of 0.134 with mtry = 85 and min_n = 4.&lt;/li&gt;
&lt;li&gt;This is no improvement in RMSE compared to &lt;code&gt;glmnet&lt;/code&gt; and &lt;code&gt;randomForest&lt;/code&gt; cross validation takes much longer to execute than &lt;code&gt;glmnet&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here a plot of the &lt;code&gt;randomForest&lt;/code&gt; hyperparameter grid along with the best hyperparameter combination:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part2-models/index_files/figure-html/plot_randomForest-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;xgboost&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;em&gt;xgboost&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;In each ensemble we have 1000 trees and do a grid search of the following hyperparameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;min_n&lt;/code&gt;: The minimum number of data points in a node required to further split the node.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tree_depth&lt;/code&gt;: The maximum depth or the number of splits of the tree.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;learn_rate&lt;/code&gt;: The rate at which the boosting algorithm adapts from one iteration to another.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;Boosted Tree Model Specification (regression)

Main Arguments:
  trees = 1000
  min_n = tune()
  tree_depth = tune()
  learn_rate = tune()

Engine-Specific Arguments:
  objective = reg:squarederror

Computational engine: xgboost &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the top 10 RMSE values and hyperparameter combinations:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 10 x 4
   min_n tree_depth learn_rate mean_rmse
   &amp;lt;int&amp;gt;      &amp;lt;int&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
 1    13          3  0.0309        0.124
 2    40          4  0.0350        0.126
 3     6          8  0.0469        0.126
 4    34         15  0.0172        0.127
 5    28         10  0.0336        0.128
 6    20         14  0.00348       0.389
 7    22          7  0.000953      4.46 
 8     3          2  0.000528      6.81 
 9    10         12  0.000401      7.73 
10    34          3  0.0000802    10.6  &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;whats-notable-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What’s notable?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;After cross validation, we get the best RMSE of 0.124 with min_n = 13, tree_depth = 3 and learn_rate = 0.0309.&lt;/li&gt;
&lt;li&gt;Gives the best RMSE compared to &lt;code&gt;glmnet&lt;/code&gt; and &lt;code&gt;randomForest&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;However, &lt;code&gt;xgboost&lt;/code&gt; cross validation takes longer to execute than that of &lt;code&gt;glmnet&lt;/code&gt;, but is faster than that of &lt;code&gt;randomForest&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Here a 3D plot of the `xgboost` hyperparameter grid: --&gt;
&lt;!-- &lt;center&gt; --&gt;
&lt;!-- ```{r plot_xgboost} --&gt;
&lt;!-- library(plotly) --&gt;
&lt;!-- plot_ly(param_grid_xgboost, x = ~min_n, y = ~tree_depth, z = ~learn_rate) %&gt;% --&gt;
&lt;!--   add_markers() %&gt;% --&gt;
&lt;!--   layout(font = list(family = &#34;Roboto Condensed&#34;), --&gt;
&lt;!--          title = list(text = &#34;Scatterplot of min_n, tree_depth and learn_rate&#34;, font = list(size = 22)), --&gt;
&lt;!--          scene = list(xaxis = list(title = &#39;min_n&#39;), --&gt;
&lt;!--                       yaxis = list(title = &#39;tree_depth&#39;), --&gt;
&lt;!--                       zaxis = list(title = &#39;learn_rate&#39;))) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- &lt;/center&gt; --&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;finalizing-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Finalizing Models&lt;/h2&gt;
&lt;p&gt;For each model, we found the combination of hyperparameters that minimize RMSE. Using those parameters, we can now train the same models on the entire training dataset. Finally, we can use the trained models to predict log(SalePrice) on the entire training set to see the actual v/s predicted log(SalePrice) results.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part2-models/index_files/figure-html/plot_train-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;whats-notable-4&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What’s notable?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Both &lt;code&gt;randomForest&lt;/code&gt; and &lt;code&gt;xgboost&lt;/code&gt; models do a fantastic job of predicting log(SalePrice) with the tuned parameters, as the predictions lie close to the straight line drawn at 45 degrees.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;glmnet&lt;/code&gt; model shows a couple of outliers with Ids &lt;strong&gt;524&lt;/strong&gt; and &lt;strong&gt;1299&lt;/strong&gt; whose predicted values are far in excess of their actual values. Even properties whose &lt;code&gt;SalePrice&lt;/code&gt; is at the lower end, show a wide dispersion in prediced values.&lt;/li&gt;
&lt;li&gt;But the true performance can only be measured on unseen testing data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;performance-on-test-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Performance on Test Data&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part2-models/index_files/figure-html/plot_test-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 3 x 3
  model        test_rmse cv_rmse
  &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
1 glmnet           0.129   0.127
2 randomForest     0.139   0.134
3 xgboost          0.128   0.124&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;whats-notable-5&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What’s notable?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;All models have similar RMSE on the unseen testing set as their cross validated RMSE, which shows the cross validation process and hyperparameters worked very well.&lt;/li&gt;
&lt;li&gt;Records with Ids &lt;strong&gt;1537&lt;/strong&gt; and &lt;strong&gt;2217&lt;/strong&gt; are outliers, as none of the models are able to predict close to actual values.&lt;/li&gt;
&lt;li&gt;Looking at the test RMSE, we could finalize &lt;code&gt;xgboost&lt;/code&gt; as the model that generalizes very well on this dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;feature-importance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Feature Importance&lt;/h2&gt;
&lt;p&gt;Even though &lt;code&gt;xgboost&lt;/code&gt; is not as easily interpretable as a linear model, we could use variable importance plots to determine the most important features selected by the model.&lt;/p&gt;
&lt;p&gt;Let’s take a look at the top 10 most important features of our finalized &lt;code&gt;xgboost&lt;/code&gt; model:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part2-models/index_files/figure-html/feature_importance-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Correlations of numerical features are plotted side-by-side. All features have a correlation of 0.5 or more with &lt;code&gt;SalePrice&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;All of the top 10 features make sense. To evaluate &lt;code&gt;SalePrice&lt;/code&gt;, a buyer would definitely look at total square footage, overall quality, neighborhood, number of bathrooms, kitchen quality, age of property, etc.&lt;/li&gt;
&lt;li&gt;This shows, our finalized model generalizes well and makes very reasonable choices in terms of features.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;new-property-premium&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;New Property Premium&lt;/h2&gt;
&lt;p&gt;Among the top 10 features by importance in our final model, most of the features like square footage, neighborhood and number of bathrooms remain the same throughout the life of the property. Quality and condition of property does change but their evaluation is mostly subjective. The only other feature that cannot be disputed to change over time is &lt;code&gt;PropertyAge&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;So, how would the predicted &lt;code&gt;SalePrice&lt;/code&gt; differ if a property was newly constructed vis-a-vis the same property if it were constructed more than 30 years earlier, and all the times in between?&lt;/p&gt;
&lt;p&gt;We could pick a couple of properties at random, change &lt;code&gt;PropertyAge&lt;/code&gt; and see its impact on &lt;code&gt;SalePrice&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part2-models/index_files/figure-html/property_appreciation-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see there’s a small premium for a newly constructed property v/s an older property of the same build, quality and condition. This premium isn’t very much in a place like Ames, IA but we’d reckon it would be much higher in a larger metropolitan city.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Ames Housing - Part 1 - Exploratory Data Analysis</title>
      <link>/casestudies/ames-housing-part1-eda/</link>
      <pubDate>Sun, 25 Dec 2016 00:00:00 +0000</pubDate>
      <guid>/casestudies/ames-housing-part1-eda/</guid>
      <description>


&lt;p&gt;In this case study, we will use the &lt;a href=&#34;http://www.amstat.org/publications/jse/v19n3/decock.pdf&#34;&gt;Ames Housing dataset&lt;/a&gt; to explore regression techniques and predict the sale price of houses.&lt;/p&gt;
&lt;div id=&#34;data-summaries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Summaries&lt;/h2&gt;
&lt;p&gt;The Ames Housing dataset contains the sale prices of properties in Ames, Iowa along with 80 other features. Each property has an &lt;strong&gt;Id&lt;/strong&gt; associated with it.
Here are the dimensions of the training and testing sets respectively:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;Dimensions of the training set&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1460   81&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;Dimensions of the testing set&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1459   81&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s combine training and testing into a single dataset and take a look at the count of missing values:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part1-eda/index_files/figure-html/missing_values-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;whats-notable&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What’s notable?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The combined dataset has 2919 property records.&lt;/li&gt;
&lt;li&gt;Very few properties have a pool, fence or an alley access to the property.&lt;/li&gt;
&lt;li&gt;Very few properties have a miscellaneous feature that has not been covered by other features.&lt;/li&gt;
&lt;li&gt;More than a dozen features have atleast 1 missing value. Since we have a tiny dataset, we will try to impute the missing values.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-cleaning-transformation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Cleaning &amp;amp; Transformation&lt;/h2&gt;
&lt;p&gt;We will visualize features of the complete dataset and create a data cleaning pipeline.&lt;/p&gt;
&lt;div id=&#34;fixing-data-errors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fixing Data Errors&lt;/h3&gt;
&lt;p&gt;First, a few data integrity checks need to be done to ensure the quality of the data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;YearRemodAdd&lt;/code&gt; should not be earlier than &lt;code&gt;YearBuilt&lt;/code&gt;: 1 record to be fixed&lt;/li&gt;
&lt;li&gt;&lt;code&gt;YrSold&lt;/code&gt; should not be earlier than &lt;code&gt;YearRemodAdd&lt;/code&gt;: 3 records to be fixed&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 x 4
     Id YearBuilt YearRemodAdd YrSold
  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
1  1877      2002         2001   2009&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 3 x 4
     Id YearBuilt YearRemodAdd YrSold
  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
1   524      2007         2008   2007
2  2296      2007         2008   2007
3  2550      2008         2009   2007&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;GarageYrBlt&lt;/code&gt; should not be earlier than &lt;code&gt;YearBuilt&lt;/code&gt;: 18 records to be fixed&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GarageYrBlt&lt;/code&gt; should not be later than &lt;code&gt;YrSold&lt;/code&gt;: 1 record to be fixed&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 18 x 4
      Id YearBuilt GarageYrBlt YrSold
   &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
 1    30      1927        1920   2008
 2    94      1910        1900   2007
 3   325      1967        1961   2010
 4   601      2005        2003   2006
 5   737      1950        1949   2006
 6  1104      1959        1954   2006
 7  1377      1930        1925   2008
 8  1415      1923        1922   2008
 9  1419      1963        1962   2008
10  1522      1959        1956   2010
11  1577      2010        2009   2010
12  1806      1935        1920   2009
13  1841      1978        1960   2009
14  1896      1941        1940   2009
15  1898      1935        1926   2009
16  2123      1945        1925   2008
17  2264      2006        2005   2007
18  2510      2006        2005   2007&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 x 4
     Id YearBuilt GarageYrBlt YrSold
  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
1  2593      2006        2207   2007&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;imputing-missing-values-new-features&#34; class=&#34;section level3 tabset tabset-fade tabset-pills&#34;&gt;
&lt;h3&gt;Imputing Missing Values &amp;amp; New Features&lt;/h3&gt;
&lt;!-- #### &lt;span style=&#34;color:red&#34;&gt;Basement Features&lt;/span&gt; --&gt;
&lt;div id=&#34;basement-features&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Basement Features&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;There is one property (&lt;code&gt;Id&lt;/code&gt; = 2121) where all the basement features are NA. &lt;code&gt;TotalBsmtSF&lt;/code&gt; is replaced by 0.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now there are 79 properties which have no basement (&lt;code&gt;TotalBsmtSF&lt;/code&gt; = 0). All other basement features having NA values are changed to &lt;strong&gt;None&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Since qualitative features do not have the same distribution across neighborhoods, any remaining &lt;strong&gt;NA&lt;/strong&gt; values are imputed to be the most common value in that &lt;code&gt;Neighborhood&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 x 13
     Id Neighborhood BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinSF1 BsmtFinType2 BsmtFinSF2 BsmtUnfSF TotalBsmtSF BsmtFullBath BsmtHalfBath
  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;             &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;             &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
1  2121 BrkSide      &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;                 NA &amp;lt;NA&amp;gt;                 NA        NA          NA           NA           NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 79 x 13
      Id Neighborhood BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinSF1 BsmtFinType2 BsmtFinSF2 BsmtUnfSF TotalBsmtSF BsmtFullBath BsmtHalfBath
   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;             &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;             &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
 1    18 Sawyer       &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;                  0 &amp;lt;NA&amp;gt;                  0         0           0            0            0
 2    40 Edwards      &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;                  0 &amp;lt;NA&amp;gt;                  0         0           0            0            0
 3    91 NAmes        &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;                  0 &amp;lt;NA&amp;gt;                  0         0           0            0            0
 4   103 SawyerW      &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;                  0 &amp;lt;NA&amp;gt;                  0         0           0            0            0
 5   157 NAmes        &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;                  0 &amp;lt;NA&amp;gt;                  0         0           0            0            0
 6   183 Edwards      &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;                  0 &amp;lt;NA&amp;gt;                  0         0           0            0            0
 7   260 OldTown      &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;                  0 &amp;lt;NA&amp;gt;                  0         0           0            0            0
 8   343 NAmes        &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;                  0 &amp;lt;NA&amp;gt;                  0         0           0            0            0
 9   363 Edwards      &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;                  0 &amp;lt;NA&amp;gt;                  0         0           0            0            0
10   372 ClearCr      &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;         &amp;lt;NA&amp;gt;                  0 &amp;lt;NA&amp;gt;                  0         0           0            0            0
# ... with 69 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 9 x 13
     Id Neighborhood BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinSF1 BsmtFinType2 BsmtFinSF2 BsmtUnfSF TotalBsmtSF BsmtFullBath BsmtHalfBath
  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;             &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;             &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
1   333 NridgHt      Gd       TA       No           GLQ                1124 &amp;lt;NA&amp;gt;                479      1603        3206            1            0
2   949 CollgCr      Gd       TA       &amp;lt;NA&amp;gt;         Unf                   0 Unf                   0       936         936            0            0
3  1488 Somerst      Gd       TA       &amp;lt;NA&amp;gt;         Unf                   0 Unf                   0      1595        1595            0            0
4  2041 Veenker      Gd       &amp;lt;NA&amp;gt;     Mn           GLQ                1044 Rec                 382         0        1426            1            0
5  2186 Edwards      TA       &amp;lt;NA&amp;gt;     No           BLQ                1033 Unf                   0        94        1127            0            1
6  2218 IDOTRR       &amp;lt;NA&amp;gt;     Fa       No           Unf                   0 Unf                   0       173         173            0            0
7  2219 IDOTRR       &amp;lt;NA&amp;gt;     TA       No           Unf                   0 Unf                   0       356         356            0            0
8  2349 Somerst      Gd       TA       &amp;lt;NA&amp;gt;         Unf                   0 Unf                   0       725         725            0            0
9  2525 CollgCr      TA       &amp;lt;NA&amp;gt;     Av           ALQ                 755 Unf                   0       240         995            0            0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Histograms of numerical basement features and their correlations with &lt;code&gt;SalePrice&lt;/code&gt; are plotted below.&lt;/p&gt;
&lt;p&gt;It could be verified that: &lt;code&gt;TotalBsmtSF&lt;/code&gt; = &lt;code&gt;BsmtFinSF1&lt;/code&gt; + &lt;code&gt;BsmtFinSF2&lt;/code&gt; + &lt;code&gt;BsmtUnfSF&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Additionally, new features are generated where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;BsmtBath&lt;/code&gt; = &lt;code&gt;BsmtFullBath&lt;/code&gt; + 0.5 * &lt;code&gt;BsmtHalfBath&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;HasBsmt&lt;/code&gt; = &lt;code&gt;TotalBsmtSF&lt;/code&gt; &amp;gt; 0&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part1-eda/index_files/figure-html/plot_num_bsmt-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Most properties have a basement.&lt;/li&gt;
&lt;li&gt;Column plots show that &lt;code&gt;BsmtFinType2&lt;/code&gt; and &lt;code&gt;BsmtCond&lt;/code&gt; values are dominated by a single category.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part1-eda/index_files/figure-html/plot_chr_bsmt-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bathroom-features&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Bathroom Features&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A new feature is generated to determine the total number of bathrooms: &lt;code&gt;TotalBath&lt;/code&gt; = &lt;code&gt;FullBath&lt;/code&gt; + &lt;code&gt;HalfBath&lt;/code&gt; + &lt;code&gt;BsmtBath&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part1-eda/index_files/figure-html/plot_bath-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fireplace-features&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Fireplace Features&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;There are 1420 properties that have no fireplaces. &lt;code&gt;FireplaceQu&lt;/code&gt; is changed to &lt;strong&gt;None&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1,420 x 4
      Id Neighborhood Fireplaces FireplaceQu
   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;             &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;      
 1     1 CollgCr               0 &amp;lt;NA&amp;gt;       
 2     6 Mitchel               0 &amp;lt;NA&amp;gt;       
 3    11 Sawyer                0 &amp;lt;NA&amp;gt;       
 4    13 Sawyer                0 &amp;lt;NA&amp;gt;       
 5    16 BrkSide               0 &amp;lt;NA&amp;gt;       
 6    18 Sawyer                0 &amp;lt;NA&amp;gt;       
 7    19 SawyerW               0 &amp;lt;NA&amp;gt;       
 8    20 NAmes                 0 &amp;lt;NA&amp;gt;       
 9    27 NAmes                 0 &amp;lt;NA&amp;gt;       
10    30 BrkSide               0 &amp;lt;NA&amp;gt;       
# ... with 1,410 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;A new feature is generated where: &lt;code&gt;HasFireplace&lt;/code&gt; = &lt;code&gt;Fireplaces&lt;/code&gt; &amp;gt; 0&lt;/li&gt;
&lt;li&gt;A significant number of properties have fireplaces.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part1-eda/index_files/figure-html/plot_fireplace-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;garage-features&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Garage Features&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;GarageYrBlt&lt;/code&gt; where NA is set to &lt;code&gt;YearBuilt&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There are 157 properties where the property has no garage. In these records, &lt;code&gt;GarageType&lt;/code&gt;, &lt;code&gt;GarageFinish&lt;/code&gt;, &lt;code&gt;GarageQual&lt;/code&gt; and &lt;code&gt;GarageCond&lt;/code&gt; are recorded as &lt;strong&gt;None&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Since qualitative features do not have the same distribution across neighborhoods, any remaining &lt;strong&gt;NA&lt;/strong&gt; values are imputed to be the most common or median value in the &lt;code&gt;Neighborhood&lt;/code&gt; by &lt;code&gt;GarageType&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 157 x 9
      Id Neighborhood GarageType GarageYrBlt GarageFinish GarageCars GarageArea GarageQual GarageCond
   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;             &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;     
 1    40 Edwards      &amp;lt;NA&amp;gt;              1955 &amp;lt;NA&amp;gt;                  0          0 &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      
 2    49 OldTown      &amp;lt;NA&amp;gt;              1920 &amp;lt;NA&amp;gt;                  0          0 &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      
 3    79 Sawyer       &amp;lt;NA&amp;gt;              1968 &amp;lt;NA&amp;gt;                  0          0 &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      
 4    89 IDOTRR       &amp;lt;NA&amp;gt;              1915 &amp;lt;NA&amp;gt;                  0          0 &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      
 5    90 CollgCr      &amp;lt;NA&amp;gt;              1994 &amp;lt;NA&amp;gt;                  0          0 &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      
 6   100 NAmes        &amp;lt;NA&amp;gt;              1959 &amp;lt;NA&amp;gt;                  0          0 &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      
 7   109 IDOTRR       &amp;lt;NA&amp;gt;              1919 &amp;lt;NA&amp;gt;                  0          0 &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      
 8   126 IDOTRR       &amp;lt;NA&amp;gt;              1935 &amp;lt;NA&amp;gt;                  0          0 &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      
 9   128 OldTown      &amp;lt;NA&amp;gt;              1930 &amp;lt;NA&amp;gt;                  0          0 &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      
10   141 NAmes        &amp;lt;NA&amp;gt;              1971 &amp;lt;NA&amp;gt;                  0          0 &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      
# ... with 147 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 2 x 9
     Id Neighborhood GarageType GarageYrBlt GarageFinish GarageCars GarageArea GarageQual GarageCond
  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;             &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;     
1  2127 OldTown      Detchd            1910 &amp;lt;NA&amp;gt;                  1        360 &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      
2  2577 IDOTRR       Detchd            1923 &amp;lt;NA&amp;gt;                 NA         NA &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;GarageArea&lt;/code&gt; and &lt;code&gt;GarageCars&lt;/code&gt; have almost similar correlation with &lt;code&gt;SalePrice&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;A new feature is generated where: &lt;code&gt;HasGarage&lt;/code&gt; = &lt;code&gt;GarageArea&lt;/code&gt; &amp;gt; 0&lt;/li&gt;
&lt;li&gt;Most properties have a garage.&lt;/li&gt;
&lt;li&gt;Column plots show that &lt;code&gt;GarageQual&lt;/code&gt; and &lt;code&gt;GarageCond&lt;/code&gt; values are dominated by a single category.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part1-eda/index_files/figure-html/plot_garage-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;masonry-features&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Masonry Features&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;There is one property (&lt;code&gt;Id&lt;/code&gt; = 2611) where &lt;code&gt;MasVnrArea&lt;/code&gt; = 198 but &lt;code&gt;MasVnrType&lt;/code&gt; = NA. Impute &lt;code&gt;MasVnrType&lt;/code&gt; to be most common value in the neighborhood where &lt;code&gt;MasVnrArea&lt;/code&gt; &amp;gt; 0.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Impute &lt;strong&gt;NA&lt;/strong&gt; values in &lt;code&gt;MasVnrType&lt;/code&gt; to be the most common values by &lt;code&gt;Neighborhood&lt;/code&gt; and &lt;code&gt;YearRemodAdd&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Impute &lt;strong&gt;NA&lt;/strong&gt; values in &lt;code&gt;MasVnrArea&lt;/code&gt; to be the median values by &lt;code&gt;Neighborhood&lt;/code&gt; and &lt;code&gt;MasVnrType&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 x 4
     Id Neighborhood MasVnrType MasVnrArea
  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;
1  2611 Mitchel      &amp;lt;NA&amp;gt;              198&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 23 x 4
      Id Neighborhood MasVnrType MasVnrArea
   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;
 1   235 Gilbert      &amp;lt;NA&amp;gt;               NA
 2   530 Crawfor      &amp;lt;NA&amp;gt;               NA
 3   651 Somerst      &amp;lt;NA&amp;gt;               NA
 4   937 SawyerW      &amp;lt;NA&amp;gt;               NA
 5   974 Somerst      &amp;lt;NA&amp;gt;               NA
 6   978 Somerst      &amp;lt;NA&amp;gt;               NA
 7  1244 NridgHt      &amp;lt;NA&amp;gt;               NA
 8  1279 CollgCr      &amp;lt;NA&amp;gt;               NA
 9  1692 Gilbert      &amp;lt;NA&amp;gt;               NA
10  1707 Somerst      &amp;lt;NA&amp;gt;               NA
# ... with 13 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 23 x 4
      Id Neighborhood MasVnrType MasVnrArea
   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;
 1   235 Gilbert      None               NA
 2   530 Crawfor      None               NA
 3   651 Somerst      None               NA
 4   937 SawyerW      None               NA
 5   974 Somerst      Stone              NA
 6   978 Somerst      None               NA
 7  1244 NridgHt      Stone              NA
 8  1279 CollgCr      BrkFace            NA
 9  1692 Gilbert      None               NA
10  1707 Somerst      Stone              NA
# ... with 13 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;A new feature is generated where: &lt;code&gt;HasMasVnr&lt;/code&gt; = &lt;code&gt;MasVnrArea&lt;/code&gt; &amp;gt; 0&lt;/li&gt;
&lt;li&gt;A significant number of properties have masonry.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part1-eda/index_files/figure-html/plot_masvnr-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pool-features&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Pool Features&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Change values in &lt;code&gt;PoolQC&lt;/code&gt; to &lt;strong&gt;None&lt;/strong&gt; if the property has no pool&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Impute &lt;strong&gt;NA&lt;/strong&gt; values in remaining &lt;code&gt;PoolQC&lt;/code&gt; to the most common value in the Neighborhood in the properties that have a pool.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 2,906 x 4
      Id Neighborhood PoolArea PoolQC
   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; 
 1     1 CollgCr             0 &amp;lt;NA&amp;gt;  
 2     2 Veenker             0 &amp;lt;NA&amp;gt;  
 3     3 CollgCr             0 &amp;lt;NA&amp;gt;  
 4     4 Crawfor             0 &amp;lt;NA&amp;gt;  
 5     5 NoRidge             0 &amp;lt;NA&amp;gt;  
 6     6 Mitchel             0 &amp;lt;NA&amp;gt;  
 7     7 Somerst             0 &amp;lt;NA&amp;gt;  
 8     8 NWAmes              0 &amp;lt;NA&amp;gt;  
 9     9 OldTown             0 &amp;lt;NA&amp;gt;  
10    10 BrkSide             0 &amp;lt;NA&amp;gt;  
# ... with 2,896 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 3 x 4
     Id Neighborhood PoolArea PoolQC
  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; 
1  2421 NAmes             368 &amp;lt;NA&amp;gt;  
2  2504 SawyerW           444 &amp;lt;NA&amp;gt;  
3  2600 Mitchel           561 &amp;lt;NA&amp;gt;  &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;A new feature is generated where: &lt;code&gt;HasPool&lt;/code&gt; = &lt;code&gt;PoolArea&lt;/code&gt; &amp;gt; 0&lt;/li&gt;
&lt;li&gt;Most properties do not have a pool.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part1-eda/index_files/figure-html/plot_pool-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;porch-features&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Porch Features&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;New features are generated for:
&lt;ul&gt;
&lt;li&gt;Total porch area: &lt;code&gt;PorchSF&lt;/code&gt; = &lt;code&gt;OpenPorchSF&lt;/code&gt; + &lt;code&gt;EnclosedPorch&lt;/code&gt; + &lt;code&gt;3SsnPorch&lt;/code&gt; + &lt;code&gt;ScreenPorch&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Whether property has a porch: &lt;code&gt;HasPorch&lt;/code&gt; = &lt;code&gt;PorchSF&lt;/code&gt; &amp;gt; 0&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part1-eda/index_files/figure-html/plot_porch-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;built-area-features&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Built Area Features&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A new feature is added to determine the total square footage of built area: &lt;code&gt;TotalSF&lt;/code&gt; = &lt;code&gt;GrLivArea&lt;/code&gt; + &lt;code&gt;TotalBsmtSF&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part1-eda/index_files/figure-html/plot_area-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;construction-year-features&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Construction Year Features&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;New features are generated for:
&lt;ul&gt;
&lt;li&gt;Vintage of year built: &lt;strong&gt;1945 or earlier, 1946-1999, 2000 or later&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Age of property from when it was built to the time it was sold: &lt;code&gt;PropertyAge&lt;/code&gt; = &lt;code&gt;YrSold&lt;/code&gt; - &lt;code&gt;YearRemodAdd&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Indicate if the property is new or newly renovated: &lt;code&gt;IsNew&lt;/code&gt; = &lt;code&gt;YearRemodAdd&lt;/code&gt; == &lt;code&gt;YrSold&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Indicate if the property has been remodelled: &lt;code&gt;IsRemodAdd&lt;/code&gt; = &lt;code&gt;YearRemodAdd&lt;/code&gt; &amp;gt; &lt;code&gt;YearBuilt&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part1-eda/index_files/figure-html/plot_construction_years-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;neighborhood-features&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Neighborhood Features&lt;/h4&gt;
&lt;p&gt;Type of Neighborhood: There are 25 neighborhoods in the dataset.
As it is said, real estate is all about location, location, location. Clearly some neighborhoods command higher prices than others.&lt;/p&gt;
&lt;p&gt;Neighborhoods could be grouped together in fewer categories depending upon how they are ranked by their median SalePrice:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Type1&lt;/strong&gt;: StoneBr, NridgHt, NoRidge&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Type2&lt;/strong&gt;: Veenker, Timber, Somerst&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Type3&lt;/strong&gt;: Crawfor, CollgCr, ClearCr, Blmngtn, Gilbert, NWAmes, SawyerW&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Type4&lt;/strong&gt;: Mitchel, NPkVill, NAmes, SWISU, Sawyer, Blueste, BrkSide, Edwards, OldTown&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Type5&lt;/strong&gt;: IDOTRR, BrDale, MeadowV&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part1-eda/index_files/figure-html/plot_neighborhood-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-missing-features&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Other Missing Features&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;In &lt;code&gt;MiscFeature&lt;/code&gt;, &lt;code&gt;Alley&lt;/code&gt; and &lt;code&gt;Fence&lt;/code&gt; &lt;strong&gt;NA&lt;/strong&gt; values are recoded as &lt;strong&gt;None&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In &lt;code&gt;Utilities&lt;/code&gt;, &lt;code&gt;Functional&lt;/code&gt;, &lt;code&gt;SaleType&lt;/code&gt; &lt;strong&gt;NA&lt;/strong&gt; values are imputed as the most common value of each feature.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In &lt;code&gt;LotFrontage&lt;/code&gt; &lt;strong&gt;NA&lt;/strong&gt; values are imputed as the median values in the &lt;code&gt;Neighborhood&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In &lt;code&gt;MSZoning&lt;/code&gt;, &lt;code&gt;KitchenQual&lt;/code&gt;, &lt;code&gt;Exterior1st&lt;/code&gt;, &lt;code&gt;Exterior2nd&lt;/code&gt;, &lt;code&gt;Electrical&lt;/code&gt; &lt;strong&gt;NA&lt;/strong&gt; values are imputed as the most common value in the &lt;code&gt;Neighborhood&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;label-encoding&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Label Encoding&lt;/h3&gt;
&lt;p&gt;A quick look at the data description shows many features have categories that follow a specific order. These features are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;LotShape&lt;/code&gt;: Reg, IR1, IR2, IR3&lt;/li&gt;
&lt;li&gt;&lt;code&gt;LandSlope&lt;/code&gt;: Gtl, Mod, Sev&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ExterQual&lt;/code&gt;: Ex, Gd, TA, Fa, Po&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ExterCond&lt;/code&gt;: Ex, Gd, TA, Fa, Po&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BsmtQual&lt;/code&gt;: Ex, Gd, TA, Fa, Po, None&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BsmtCond&lt;/code&gt;: Ex, Gd, TA, Fa, Po, None&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BsmtExposure&lt;/code&gt;: Gd, Av, Mn, No, None&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BsmtFinType1&lt;/code&gt;: GLQ, ALQ, BLQ, Rec, LwQ, Unf, None&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BsmtFinType2&lt;/code&gt;: GLQ, ALQ, BLQ, Rec, LwQ, Unf, None&lt;/li&gt;
&lt;li&gt;&lt;code&gt;HeatingQC&lt;/code&gt;: Ex, Gd, TA, Fa, Po&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CentralAir&lt;/code&gt;: Y, N&lt;/li&gt;
&lt;li&gt;&lt;code&gt;KitchenQual&lt;/code&gt;: Ex, Gd, TA, Fa, Po&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Functional&lt;/code&gt;: Typ, Min1, Min2, Mod, Maj1, Maj2, Sev, Sal&lt;/li&gt;
&lt;li&gt;&lt;code&gt;FireplaceQu&lt;/code&gt;: Ex, Gd, TA, Fa, Po, None&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GarageFinish&lt;/code&gt;: Fin, RFn, Unf, None&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GarageQual&lt;/code&gt;: Ex, Gd, TA, Fa, Po, None&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GarageCond&lt;/code&gt;: Ex, Gd, TA, Fa, Po, None&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Street&lt;/code&gt;: Grvl, Pave&lt;/li&gt;
&lt;li&gt;&lt;code&gt;PavedDrive&lt;/code&gt;: Y, P, N&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Most of these features have a common order &lt;strong&gt;Ex, Gd, TA, Fa, Po&lt;/strong&gt;, except some are missing &lt;strong&gt;None&lt;/strong&gt; as a category. These features could be ordered with a common set of categories from &lt;strong&gt;Ex, Gd, TA, Fa, Po, None&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Some categorical features are already ordered by an integer number. These features are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;OverallQual&lt;/code&gt;: 10 to 1&lt;/li&gt;
&lt;li&gt;&lt;code&gt;OverallCond&lt;/code&gt;: 10 to 1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;MoSold&lt;/code&gt; is cyclical and should be recoded as a factor.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;YrSold&lt;/code&gt; has only 5 values from 2006-2010 and should also be recoded as a factor.&lt;/p&gt;
&lt;p&gt;Categorical features where several categories have less than 10 observations are lumped into a single category named &lt;strong&gt;Other&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;features-to-drop&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Features to Drop&lt;/h3&gt;
&lt;div id=&#34;highly-correlated-features&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Highly Correlated Features&lt;/h4&gt;
&lt;p&gt;Some features could be dropped from further analysis because either they are too correlated or replaced by a similar feature.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; [1] &amp;quot;BsmtFullBath&amp;quot;  &amp;quot;GarageCars&amp;quot;    &amp;quot;GarageYrBlt&amp;quot;   &amp;quot;GrLivArea&amp;quot;     &amp;quot;PoolArea&amp;quot;      &amp;quot;YearBuilt&amp;quot;     &amp;quot;YearRemodAdd&amp;quot;  &amp;quot;Neighborhood&amp;quot;  &amp;quot;OpenPorchSF&amp;quot;  
[10] &amp;quot;EnclosedPorch&amp;quot; &amp;quot;3SsnPorch&amp;quot;     &amp;quot;ScreenPorch&amp;quot;  &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;skewed-categorical-features&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Skewed Categorical Features&lt;/h4&gt;
&lt;p&gt;Any feature where more than 95% of the records have the same category probably doesn’t have any predictive value. An extreme case is &lt;code&gt;Utilities&lt;/code&gt; which has only 2 categories - &lt;strong&gt;AllPub&lt;/strong&gt; and &lt;strong&gt;NoSeWa&lt;/strong&gt; in the dataset. Only 1 record has &lt;strong&gt;NoSeWa&lt;/strong&gt; and the rest of the records have &lt;strong&gt;AllPub&lt;/strong&gt;. Therefore, features like these do not have any predictive value.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part1-eda/index_files/figure-html/plot_skewed_features-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;finalized-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Finalized Data&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;Dimensions of the finalized dataset&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2919   73&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Excluding Id, there are 72 features in the finalized dataset.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;There are 26 numerical, 26 ordinal and 20 nominal features.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;univariate-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Univariate Analysis&lt;/h2&gt;
&lt;p&gt;Let us look at each feature in the dataset in detail.&lt;/p&gt;
&lt;div id=&#34;numerical-features&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Numerical Features&lt;/h3&gt;
&lt;p&gt;First let’s plot all the features that are measured as area in square feet:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part1-eda/index_files/figure-html/univariate_continuous1-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;whats-notable-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What’s notable?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;All area features have outliers.&lt;/li&gt;
&lt;li&gt;Many features are heavily skewed so they need to be normalized before fitting models.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now let’s see other numerical features:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part1-eda/index_files/figure-html/univariate_continuous2-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;whats-notable-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What’s notable?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Most of the properties have been built less than 20 years prior to their sale.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s plot the distribution of &lt;code&gt;SalePrice&lt;/code&gt; in log scale:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part1-eda/index_files/figure-html/SalePrice_distribution-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;whats-notable-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What’s notable?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;We see long tailed distribution on both sides.&lt;/li&gt;
&lt;li&gt;There are 11 properties below USD 50,000 and 17 above USD 500,000.&lt;/li&gt;
&lt;li&gt;Linear models are very sensitive to the presence of outliers.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;categorical-features&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Categorical Features&lt;/h2&gt;
&lt;div id=&#34;ordinal-features&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ordinal Features&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part1-eda/index_files/figure-html/Univariate_Cat1-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;whats-notable-4&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What’s notable?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Categorical imbalances exist in many features where 1 or 2 categories are dominant. This poses a big challenge for using these features as predictors, as categories with fewer counts tend to be underrepresented in the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;nominal-features.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Nominal Features.&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part1-eda/index_files/figure-html/Univariate_Cat2-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;whats-notable-5&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What’s notable?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Categorical imbalances exist in many features where 1 or 2 categories are dominant.&lt;/li&gt;
&lt;li&gt;Most of the properties are sold during the summer months, and the least during the winter months.&lt;/li&gt;
&lt;li&gt;The effect of housing market crisis are visible in the data, as the fewest properties were sold in 2010.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bivariate-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bivariate Analysis&lt;/h2&gt;
&lt;div id=&#34;numerical-numerical&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Numerical-Numerical&lt;/h3&gt;
&lt;p&gt;Let’s examine the relationship of &lt;code&gt;SalePrice&lt;/code&gt; with other numerical features:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part1-eda/index_files/figure-html/bivariate_scatterplots-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;whats-notable-6&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What’s notable?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;From the scatterplot of &lt;code&gt;TotalSF&lt;/code&gt; v/s &lt;code&gt;SalePrice&lt;/code&gt;, it is very clear there are &lt;em&gt;high leverage&lt;/em&gt; points where the target &lt;code&gt;SalePrice&lt;/code&gt; is unusually low relative to the area in sq. ft. These points have an outsized impact on the slope of the regression line, which otherwise would be higher.&lt;/li&gt;
&lt;li&gt;The same set of points impact &lt;code&gt;TotalBsmtSF&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The Ids of these records are 524,1299,2550. Out of these 524 and 1299 are in the training set.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;correlations-with-saleprice&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Correlations with SalePrice&lt;/h3&gt;
&lt;p&gt;We isolate the features that have an absolute correlation of 0.1 or more with &lt;code&gt;SalePrice&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part1-eda/index_files/figure-html/plot_correlated-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;whats-notable-7&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What’s notable?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The top 5 features are TotalSF, GarageArea, TotalBath, TotalBsmtSF, 1stFlrSF. Quite reasonably, a buyer would look at these features to evaluate a property and its &lt;code&gt;SalePrice&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;It is somewhat counterintuitive that &lt;code&gt;PropertyAge&lt;/code&gt; shows a strong negative correlation with &lt;code&gt;SalePrice&lt;/code&gt;. It means properties that were more recently built, sell for higher prices than older properties.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;numerical-categorical-ordinal&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Numerical-Categorical (Ordinal)&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part1-eda/index_files/figure-html/Bivariate_Num_Cat1-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;whats-notable-8&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What’s notable?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;We can spot clear trends in &lt;code&gt;SalePrice&lt;/code&gt; v/s the order of the categories in almost all of these features.&lt;/li&gt;
&lt;li&gt;Overall quality and external quality show some of the strongest trends.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;numerical-categorical-nominal&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Numerical-Categorical (Nominal)&lt;/h3&gt;
&lt;p&gt;Let’s examine &lt;code&gt;SalePrice&lt;/code&gt; with respect to the nominal features in the dataset. None of these features have a natural order, but we can identify trends within categories by sorting with the median &lt;code&gt;SalePrice&lt;/code&gt;. The &lt;code&gt;SalePrice&lt;/code&gt; axis is truncated to exclude outliers.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part1-eda/index_files/figure-html/Bivariate_Num_Cat2-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;whats-notable-9&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What’s notable?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;GarageType&lt;/code&gt;: Builtin and attached garages are more preferred than detached or other types of garages.&lt;/li&gt;
&lt;li&gt;From &lt;code&gt;MSSubClass&lt;/code&gt; categories, it is evident that 1946 or newer houses are higher priced than older houses.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;multivariate-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multivariate Analysis&lt;/h2&gt;
&lt;p&gt;We will check variation of some related features with &lt;code&gt;SalePrice&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;numerical-numerical-categorical&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Numerical-Numerical-Categorical&lt;/h3&gt;
&lt;p&gt;We have determined &lt;code&gt;TotalSF&lt;/code&gt; and &lt;code&gt;GarageArea&lt;/code&gt; have among the strongest correlations with &lt;code&gt;SalePrice&lt;/code&gt;. Let’s see how they vary by &lt;code&gt;NeighborhoodType&lt;/code&gt; and &lt;code&gt;GarageType&lt;/code&gt; respectively:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part1-eda/index_files/figure-html/multivariate_num_num_cat-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For the same total area, there are neighborhoods where &lt;code&gt;SalePrice&lt;/code&gt; is higher than others.&lt;/li&gt;
&lt;li&gt;Properties with no garage are distinctly separated.&lt;/li&gt;
&lt;li&gt;Properties with built-in or attached garages tend to have higher &lt;code&gt;SalePrice&lt;/code&gt; for the same &lt;code&gt;GarageArea&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Therefore, &lt;code&gt;NeighborhoodType&lt;/code&gt; and &lt;code&gt;GarageType&lt;/code&gt; explain some variance in &lt;code&gt;SalePrice&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;categorical-categorical-numerical&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Categorical-Categorical-Numerical&lt;/h3&gt;
&lt;p&gt;We want to see if there is any interaction of &lt;code&gt;SalePrice&lt;/code&gt; with a combination of categorical features, that could provide any additional explanatory power for prediction:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part1-eda/index_files/figure-html/multivariate_cat_cat_num-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is evident that some neighborhoods have higher &lt;code&gt;OverallQual&lt;/code&gt; and therefore command higher price. However in Type4 neighborhoods, we can see a clear variation in &lt;code&gt;SalePrice&lt;/code&gt; by quality of property.&lt;/li&gt;
&lt;li&gt;It is less clear if &lt;code&gt;GarageType&lt;/code&gt; has a major impact by itself. Even though built-in and attached garages seem to be preferred, most of the variation can be explained by &lt;code&gt;NeighborhoodType&lt;/code&gt; itself.&lt;/li&gt;
&lt;li&gt;Low density and floating village residential properties tend to be higher priced in both single and multi-storied properties built after 1946.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Diamonds - Part 3 - A polished gem - Building Non-linear Models</title>
      <link>/casestudies/diamonds-part3-non-linear-models/</link>
      <pubDate>Thu, 22 Dec 2016 00:00:00 +0000</pubDate>
      <guid>/casestudies/diamonds-part3-non-linear-models/</guid>
      <description>


&lt;div id=&#34;other-posts-in-this-series&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Other posts in this series:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nitingupta.com/casestudies/diamonds-part1-eda/&#34;&gt;Diamonds - Part 1 - In the rough - An Exploratory Data Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nitingupta.com/casestudies/diamonds-part2-linear-models/&#34;&gt;Diamonds - Part 2 - A cut above - Building Linear Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In a couple of previous posts, we tried to understand what attributes of diamonds are important to determine their prices. We showed that &lt;code&gt;carat&lt;/code&gt;, &lt;code&gt;clarity&lt;/code&gt; and &lt;code&gt;color&lt;/code&gt; are the most important predictors of &lt;code&gt;price&lt;/code&gt;. We arrived at this conclusion after doing a detailed exploratory data analysis. Finally we fit linear models to predict prices and determined the best model from the metrics.&lt;/p&gt;
&lt;p&gt;In this post, we will use non-linear regression models to predict diamond prices and compare them with those from linear models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;training-non-linear-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Training Non-linear Models&lt;/h2&gt;
&lt;p&gt;We’ll follow some of the same steps as we did for linear models, while transforming some predictors:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Partition the dataset into training and testing sets in the proportion 75% and 25% respectively.&lt;/li&gt;
&lt;li&gt;Stratify the partitioning by &lt;code&gt;clarity&lt;/code&gt;, so both training and testing sets have the same distributions of this feature.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;clarity&lt;/code&gt;, &lt;code&gt;color&lt;/code&gt; and &lt;code&gt;cut&lt;/code&gt; have ordered categories from lowest to highest grades. The &lt;code&gt;randomForest&lt;/code&gt; method requires no change in representing this data before training the models, however &lt;code&gt;xgboost&lt;/code&gt; and &lt;code&gt;keras&lt;/code&gt; methods require all the predictors to be in numerical form. &lt;a href=&#34;https://statmodeling.stat.columbia.edu/2009/10/06/coding_ordinal/&#34;&gt;Two methods&lt;/a&gt; could be used for transforming the categorical data:
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Use one-hot encoding to convert categorical data to sparse data with 0s and 1s. This way, each category in &lt;code&gt;clarity&lt;/code&gt;, &lt;code&gt;color&lt;/code&gt; and &lt;code&gt;cut&lt;/code&gt; is converted to a new predictor in binary form. A disadvantage of this method is that it treates ordered categorical data the same as unordered categorical data, so the ordinality is lost in transformation. However, non-linear models should be able to infer the ordinality as our training sample is sufficiently large.&lt;/li&gt;
&lt;li&gt;Represent the ordinal categories from lowest to highest grades in integer form. However, this creates a linear gradation from one category to another, which may not be a suitable choice here.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Center and scale all values in the training set and build a matrix of predictors.&lt;/li&gt;
&lt;li&gt;Fit a non-linear model with the training set.&lt;/li&gt;
&lt;li&gt;Make predictions on the testing set and determine model metrics.&lt;/li&gt;
&lt;li&gt;Wrap all the steps above inside a function in which the model formula, and a seed could be passed that randomizes the partition of training and testing sets.&lt;/li&gt;
&lt;li&gt;Run multiple iterations of models with different seeds, and compute their average metrics, that would reflect results on unseen data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here are the average metrics for all the models trained with &lt;code&gt;keras&lt;/code&gt;, &lt;code&gt;randomForest&lt;/code&gt; and &lt;code&gt;xgboost&lt;/code&gt; regression methods:&lt;/p&gt;
&lt;table class=&#34;gmisc_table&#34; style=&#34;border-collapse: collapse; margin-top: 1em; margin-bottom: 1em;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;border-top: 2px solid grey;&#34;&gt;
&lt;/th&gt;
&lt;th colspan=&#34;3&#34; style=&#34;font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: center;&#34;&gt;
mae
&lt;/th&gt;
&lt;th style=&#34;border-top: 2px solid grey;; border-bottom: hidden;&#34;&gt;
 
&lt;/th&gt;
&lt;th colspan=&#34;3&#34; style=&#34;font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: center;&#34;&gt;
rmse
&lt;/th&gt;
&lt;th style=&#34;border-top: 2px solid grey;; border-bottom: hidden;&#34;&gt;
 
&lt;/th&gt;
&lt;th colspan=&#34;3&#34; style=&#34;font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: center;&#34;&gt;
rsq
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;border-bottom: 1px solid grey;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
keras
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
randomForest
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
xgboost
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey;&#34; colspan=&#34;1&#34;&gt;
 
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
keras
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
randomForest
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
xgboost
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey;&#34; colspan=&#34;1&#34;&gt;
 
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
keras
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
randomForest
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
xgboost
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;border-right: 1px solid black; text-align: left;&#34;&gt;
price ~ .
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
360.55
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
262.35
&lt;/td&gt;
&lt;td style=&#34;border-right: 1px solid black; text-align: right;&#34;&gt;
280.49
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
989.71
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
529.28
&lt;/td&gt;
&lt;td style=&#34;border-right: 1px solid black; text-align: right;&#34;&gt;
540.76
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
0.93
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
0.98
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
0.98
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-right: 1px solid black; text-align: left;&#34;&gt;
price ~ carat
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
860.29
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
816.1
&lt;/td&gt;
&lt;td style=&#34;border-right: 1px solid black; text-align: right;&#34;&gt;
815.76
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
1499.2
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
1427.25
&lt;/td&gt;
&lt;td style=&#34;border-right: 1px solid black; text-align: right;&#34;&gt;
1427.35
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
0.86
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
0.87
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
0.87
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-right: 1px solid black; text-align: left;&#34;&gt;
price ~ carat + clarity
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
590.32
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
548.67
&lt;/td&gt;
&lt;td style=&#34;border-right: 1px solid black; text-align: right;&#34;&gt;
544.48
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
1040.69
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
1006.61
&lt;/td&gt;
&lt;td style=&#34;border-right: 1px solid black; text-align: right;&#34;&gt;
992.46
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
0.93
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
0.94
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
0.94
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-right: 1px solid black; text-align: left;&#34;&gt;
price ~ carat + clarity + color
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
358.85
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
305.17
&lt;/td&gt;
&lt;td style=&#34;border-right: 1px solid black; text-align: right;&#34;&gt;
306.86
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
645.4
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
571.73
&lt;/td&gt;
&lt;td style=&#34;border-right: 1px solid black; text-align: right;&#34;&gt;
575.3
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
0.97
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
0.98
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
0.98
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; border-right: 1px solid black; text-align: left;&#34;&gt;
price ~ carat + clarity + color + cut
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: right;&#34;&gt;
347.99
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: right;&#34;&gt;
285.96
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; border-right: 1px solid black; text-align: right;&#34;&gt;
282.38
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey;&#34; colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: right;&#34;&gt;
626.78
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: right;&#34;&gt;
545.02
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; border-right: 1px solid black; text-align: right;&#34;&gt;
541.63
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey;&#34; colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: right;&#34;&gt;
0.98
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: right;&#34;&gt;
0.98
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: right;&#34;&gt;
0.98
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Looking at the r-squared terms, it is remarkable how well all the models have been able to infer the complex relationship between &lt;code&gt;price&lt;/code&gt; and &lt;code&gt;carat&lt;/code&gt;. To fit linear models, we needed to transform &lt;code&gt;price&lt;/code&gt; to logarithmic terms and take the cube root of &lt;code&gt;carat&lt;/code&gt;. The neural network as well as the decision tree based models do this all on their own. The root mean squared error is in $ terms so it is easier to interpret. Considering the mean and standard deviation of &lt;code&gt;price&lt;/code&gt; in the dataset is about $4000, the root mean squared errors of the models are very low.&lt;/p&gt;
&lt;p&gt;Exploratory data analysis adds value here, as the models with &lt;code&gt;carat&lt;/code&gt;, &lt;code&gt;clarity&lt;/code&gt; and &lt;code&gt;color&lt;/code&gt; give excellent results. Including &lt;code&gt;cut&lt;/code&gt; in the models does not provide any significant benefits and results in overfitted models.&lt;/p&gt;
&lt;p&gt;Even the base models with all predictors: &lt;strong&gt;price ~ .&lt;/strong&gt; (where some of them are confounders), do a very good job of explaning the variance. Decision tree and neural network models are unaffected by multi-collinearity. We can use local model interpretations to determine the most important predictors from these models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;local-interpretable-model-agnostic-explanations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Local Interpretable Model-agnostic Explanations&lt;/h2&gt;
&lt;p&gt;LIME is a method for explaining black-box machine learning models. It can help visualize and explain individual predictions. It makes the assumption that every complex model is linear on a local scale. So it is possible to fit a simple model around a single observation that will behave how the global model behaves at that locality. The simple model can be used to explain the predictions of the more complex model locally.&lt;/p&gt;
&lt;p&gt;The generalized algorithm LIME applies is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Given an observation, permute it to create replicated feature data with slight value modifications.&lt;/li&gt;
&lt;li&gt;Compute similarity distance measure between original observation and permuted observations.&lt;/li&gt;
&lt;li&gt;Apply selected machine learning model to predict outcomes of permuted data.&lt;/li&gt;
&lt;li&gt;Select m number of features to best describe predicted outcomes.&lt;/li&gt;
&lt;li&gt;Fit a simple model to the permuted data, explaining the complex model outcome with m features from the permuted data weighted by its similarity to the original observation .&lt;/li&gt;
&lt;li&gt;Use the resulting feature weights to explain local behavior.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here we will select 5 features that best describe the predicted outcomes for 6 random observations from the testing set.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part3-non-linear-models/index_files/figure-html/plot_feature_importance-1.png&#34; width=&#34;960&#34; /&gt;&lt;img src=&#34;/casestudies/diamonds-part3-non-linear-models/index_files/figure-html/plot_feature_importance-2.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The features by importance that best explain the predictions in these 6 random samples are &lt;code&gt;carat&lt;/code&gt;, &lt;code&gt;clarity&lt;/code&gt;, &lt;code&gt;color&lt;/code&gt;, &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part3-non-linear-models/index_files/figure-html/plot_feature_heatmap-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/casestudies/diamonds-part3-non-linear-models/index_files/figure-html/plot_feature_heatmap-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We know that &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are co-linear with &lt;code&gt;carat&lt;/code&gt;, which is why it is good practice to remove any redundant features from the training data before applying any machine learning algorithm. We find the model with the best metrics turns out to be the one using &lt;code&gt;carat&lt;/code&gt;, &lt;code&gt;clarity&lt;/code&gt; and &lt;code&gt;color&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;actual-vs-predicted&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Actual v/s Predicted&lt;/h2&gt;
&lt;p&gt;Finally, here are the scatterplots of actual v/s predicted &lt;code&gt;price&lt;/code&gt; from the best model on the testing set, using the 3 regression methods:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part3-non-linear-models/index_files/figure-html/best_model_plot-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The scatterplots are shown with both linear and logarithmic axes. Even though the results from all the 3 methods have roughly similar &lt;strong&gt;r-squared&lt;/strong&gt; and &lt;strong&gt;rmse&lt;/strong&gt; values, we can see predicted prices from keras have more dispersion than the two decision-tree methods at the higher end. The decision-tree based methods appear do a better job of predicting prices at the lower end with lesser dispersion.&lt;/p&gt;
&lt;p&gt;As in the case with linear models, the variance in predicted diamond prices increases with &lt;code&gt;price&lt;/code&gt;. But unlike linear models, the non-linear models do not produce extreme outliers in predicted prices. So, not only do non-linear methods do a fantastic job in inferring the relationships between &lt;code&gt;price&lt;/code&gt; and its predictors, they also predict prices within a reasonable range.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;All the 3 non-linear regression methods can infer the complex relationship between &lt;code&gt;price&lt;/code&gt;, &lt;code&gt;carat&lt;/code&gt; and other predictors, without the need for feature engineering.&lt;/li&gt;
&lt;li&gt;Exploratory Data Analysis is useful in removing the redundant features from the training dataset, resulting in both faster execution, as well as much better metrics.&lt;/li&gt;
&lt;li&gt;In terms of time taken to train the models, &lt;code&gt;keras&lt;/code&gt; neural network models execute the fastest by virtue of being able to use GPUs.&lt;/li&gt;
&lt;li&gt;Among the decision-tree based methods, &lt;code&gt;xgboost&lt;/code&gt; models train much faster than &lt;code&gt;randomForest&lt;/code&gt; models.&lt;/li&gt;
&lt;li&gt;Multiple CPUs can be used to run &lt;code&gt;randomForest&lt;/code&gt; and &lt;code&gt;xgboost&lt;/code&gt; methods. RAM is the only limiting constraint, when trained on a local machine.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Diamonds - Part 2 - A cut above - Building Linear Models</title>
      <link>/casestudies/diamonds-part2-linear-models/</link>
      <pubDate>Wed, 21 Dec 2016 00:00:00 +0000</pubDate>
      <guid>/casestudies/diamonds-part2-linear-models/</guid>
      <description>


&lt;p&gt;In a &lt;a href=&#34;https://www.nitingupta.com/casestudies/diamonds-part1-eda/&#34;&gt;previous post&lt;/a&gt; in this series, we did an exploratory data analysis of the &lt;code&gt;diamonds&lt;/code&gt; dataset and found that &lt;code&gt;carat&lt;/code&gt;, &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;, &lt;code&gt;z&lt;/code&gt; were strongly correlated with &lt;code&gt;price&lt;/code&gt;. To some extent, &lt;code&gt;clarity&lt;/code&gt; also appeared to provide some predictive ability.&lt;/p&gt;
&lt;p&gt;In this post, we will build linear models and see how well they predict the &lt;code&gt;price&lt;/code&gt; of diamonds.&lt;/p&gt;
&lt;p&gt;Before we do any transformations, feature engineering or feature selections for our model, let’s see what kind of results we get from a base linear model, that uses all the features to predict &lt;code&gt;price&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
Call:
lm(formula = price ~ ., data = diamonds)

Residuals:
   Min     1Q Median     3Q    Max 
-21376   -592   -183    376  10694 

Coefficients:
            Estimate Std. Error t value             Pr(&amp;gt;|t|)    
(Intercept)  5753.76     396.63   14.51 &amp;lt; 0.0000000000000002 ***
carat       11256.98      48.63  231.49 &amp;lt; 0.0000000000000002 ***
cut.L         584.46      22.48   26.00 &amp;lt; 0.0000000000000002 ***
cut.Q        -301.91      17.99  -16.78 &amp;lt; 0.0000000000000002 ***
cut.C         148.03      15.48    9.56 &amp;lt; 0.0000000000000002 ***
cut^4         -20.79      12.38   -1.68               0.0929 .  
color.L     -1952.16      17.34 -112.57 &amp;lt; 0.0000000000000002 ***
color.Q      -672.05      15.78  -42.60 &amp;lt; 0.0000000000000002 ***
color.C      -165.28      14.72  -11.22 &amp;lt; 0.0000000000000002 ***
color^4        38.20      13.53    2.82               0.0047 ** 
color^5       -95.79      12.78   -7.50    0.000000000000066 ***
color^6       -48.47      11.61   -4.17    0.000030090737193 ***
clarity.L    4097.43      30.26  135.41 &amp;lt; 0.0000000000000002 ***
clarity.Q   -1925.00      28.23  -68.20 &amp;lt; 0.0000000000000002 ***
clarity.C     982.20      24.15   40.67 &amp;lt; 0.0000000000000002 ***
clarity^4    -364.92      19.29  -18.92 &amp;lt; 0.0000000000000002 ***
clarity^5     233.56      15.75   14.83 &amp;lt; 0.0000000000000002 ***
clarity^6       6.88      13.72    0.50               0.6157    
clarity^7      90.64      12.10    7.49    0.000000000000071 ***
depth         -63.81       4.53  -14.07 &amp;lt; 0.0000000000000002 ***
table         -26.47       2.91   -9.09 &amp;lt; 0.0000000000000002 ***
x           -1008.26      32.90  -30.65 &amp;lt; 0.0000000000000002 ***
y               9.61      19.33    0.50               0.6192    
z             -50.12      33.49   -1.50               0.1345    
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

Residual standard error: 1130 on 53916 degrees of freedom
Multiple R-squared:  0.92,  Adjusted R-squared:  0.92 
F-statistic: 2.69e+04 on 23 and 53916 DF,  p-value: &amp;lt;0.0000000000000002&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 3 x 3
  .metric .estimator .estimate
  &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
1 rmse    standard    1130.   
2 rsq     standard       0.920
3 mae     standard     740.   &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model summary shows it is an overfitted model. Among other things, we know that &lt;code&gt;depth&lt;/code&gt; and &lt;code&gt;table&lt;/code&gt; have no impact on &lt;code&gt;price&lt;/code&gt;, yet these are shown to be highly significant. Root Mean Squared Error (rmse) and other metrics are also shown above.&lt;/p&gt;
&lt;p&gt;Let’s make a plot of actual v/s predicted prices to visualize how well this base model performs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part2-linear-models/index_files/figure-html/simple_lm_model_plot-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If the predictions are good, the points should lie close to a straight line drawn at 45 degrees. We can see this base model does a poor job of predicting prices. Worst of all, the model predicts negative prices on the lower end.
It shows that &lt;code&gt;price&lt;/code&gt; has to be log transformed to avoid these absurdities.&lt;/p&gt;
&lt;div id=&#34;feature-engineering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Feature Engineering&lt;/h2&gt;
&lt;p&gt;We know the price of a diamond is strongly correlated with its size. All things equal, the larger the diamond, the greater its price.&lt;/p&gt;
&lt;p&gt;As a first approximation, we can assume a diamond is a cuboid with dimensions &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;z&lt;/code&gt;. Then, we can compute its &lt;code&gt;volume&lt;/code&gt; as x * y * z.
As these 3 dimensions are highly correlated, we can compute a geometrical average dimension by taking the cube root of &lt;code&gt;volume&lt;/code&gt;, and retain a linear relationship with &lt;code&gt;log(price)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Another way to calculate an average dimension is by using high school chemistry. Mass, volume and density are related to each other by the equation:&lt;/p&gt;
&lt;p&gt;$ density = mass/volume $&lt;/p&gt;
&lt;p&gt;We can find out that 1 carat = 0.2 gms. Dividing by the density of diamond (3.51 gms/cc) would give us its volume in cc, which could be converted to a geometrical average dimension by taking the cube root.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part2-linear-models/index_files/figure-html/feature_engineering-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Even though both methods yield similar results, we could see that the density method results in a narrower range. But which method would be more robust?
Keep in mind there are 20 &lt;code&gt;z&lt;/code&gt; values that are 0. In 7 of these records both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are 0 too, which means these values were not recorded reliably.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 20 x 10
   carat cut       color clarity depth table price     x     y     z
   &amp;lt;dbl&amp;gt; &amp;lt;ord&amp;gt;     &amp;lt;ord&amp;gt; &amp;lt;ord&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
 1  1    Premium   G     SI2      59.1    59  3142  6.55  6.48     0
 2  1.01 Premium   H     I1       58.1    59  3167  6.66  6.6      0
 3  1.1  Premium   G     SI2      63      59  3696  6.5   6.47     0
 4  1.01 Premium   F     SI2      59.2    58  3837  6.5   6.47     0
 5  1.5  Good      G     I1       64      61  4731  7.15  7.04     0
 6  1.07 Ideal     F     SI2      61.6    56  4954  0     6.62     0
 7  1    Very Good H     VS2      63.3    53  5139  0     0        0
 8  1.15 Ideal     G     VS2      59.2    56  5564  6.88  6.83     0
 9  1.14 Fair      G     VS1      57.5    67  6381  0     0        0
10  2.18 Premium   H     SI2      59.4    61 12631  8.49  8.45     0
11  1.56 Ideal     G     VS2      62.2    54 12800  0     0        0
12  2.25 Premium   I     SI1      61.3    58 15397  8.52  8.42     0
13  1.2  Premium   D     VVS1     62.1    59 15686  0     0        0
14  2.2  Premium   H     SI1      61.2    59 17265  8.42  8.37     0
15  2.25 Premium   H     SI2      62.8    59 18034  0     0        0
16  2.02 Premium   H     VS2      62.7    53 18207  8.02  7.95     0
17  2.8  Good      G     SI2      63.8    58 18788  8.9   8.85     0
18  0.71 Good      F     SI2      64.1    60  2130  0     0        0
19  0.71 Good      F     SI2      64.1    60  2130  0     0        0
20  1.12 Premium   G     I1       60.4    59  2383  6.71  6.67     0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In all of these records, the &lt;code&gt;carat&lt;/code&gt; values were recorded reliably and are probably more accurate than the dimensions.
Hence, we might prefer the density method of generating this feature.&lt;/p&gt;
&lt;p&gt;Furthermore, since density is a constant, dividing by a constant to calculate volume isn’t really necessary. Instead, a cube root transformation could be applied to &lt;code&gt;carat&lt;/code&gt; itself for the purposes of predictive modelling that would result in a linear relationship between &lt;span class=&#34;math inline&#34;&gt;\(log(price)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(carat^{1/3}\)&lt;/span&gt;.
It is the reason why we’re fitting a linear model because the model is linear in its parameters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;training-linear-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Training Linear Models&lt;/h2&gt;
&lt;p&gt;Here are the steps for building linear models and computing metrics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Partition the dataset into training and testing sets in the proportion 75% and 25% respectively.&lt;/li&gt;
&lt;li&gt;Since &lt;code&gt;clarity&lt;/code&gt; is one of the main predictors, stratify the partitioning by &lt;code&gt;clarity&lt;/code&gt;, so both training and testing sets have the same distributions of this feature.&lt;/li&gt;
&lt;li&gt;Fit a linear model with the training set.&lt;/li&gt;
&lt;li&gt;Make predictions on the testing set and determine model metrics.&lt;/li&gt;
&lt;li&gt;Wrap all the steps above inside a function in which the model formula and a seed could be passed. Since the seed determines the random partitioning, it helps to minimize vagaries in partitioning the training and testing sets before fitting models.&lt;/li&gt;
&lt;li&gt;Run multiple iterations of a model with different seeds, and compute its average metrics, that would reflect the results on unseen data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here’s a sample split of training and testing set, stratified by &lt;code&gt;clarity&lt;/code&gt;. As we can see, the training and testing sets have similar distributions.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dfTrain$clarity 
       n  missing distinct 
   40457        0        8 

lowest : I1   SI2  SI1  VS2  VS1 , highest: VS2  VS1  VVS2 VVS1 IF  
                                                          
Value         I1   SI2   SI1   VS2   VS1  VVS2  VVS1    IF
Frequency    552  6895  9826  9222  6125  3780  2722  1335
Proportion 0.014 0.170 0.243 0.228 0.151 0.093 0.067 0.033&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;dfTest$clarity 
       n  missing distinct 
   13483        0        8 

lowest : I1   SI2  SI1  VS2  VS1 , highest: VS2  VS1  VVS2 VVS1 IF  
                                                          
Value         I1   SI2   SI1   VS2   VS1  VVS2  VVS1    IF
Frequency    189  2299  3239  3036  2046  1286   933   455
Proportion 0.014 0.171 0.240 0.225 0.152 0.095 0.069 0.034&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After running 5 iterations of each model with a different seed, here are the average metrics:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 5 x 4
  model                                                 rmse   rsq   mae
  &amp;lt;chr&amp;gt;                                                &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
1 log(price) ~ .                                      11055. 0.670  570.
2 log(price) ~ I(carat^(1/3))                          2893. 0.687 1039.
3 log(price) ~ I(carat^(1/3)) + clarity                2312. 0.807  881.
4 log(price) ~ I(carat^(1/3)) + clarity + color        1870. 0.870  631.
5 log(price) ~ I(carat^(1/3)) + clarity + color + cut  1848. 0.875  625.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first model with all predictors is an overfitted one.&lt;/p&gt;
&lt;p&gt;The model with &lt;code&gt;carat&lt;/code&gt;, &lt;code&gt;clarity&lt;/code&gt; and &lt;code&gt;color&lt;/code&gt; provides the best combination of root mean squared error and r-squared, that explains the most variance.
This is our final model.
Including &lt;code&gt;cut&lt;/code&gt; in the model has diminishing benefits, and tends to overfit the data.&lt;/p&gt;
&lt;p&gt;Here’s the summary of our final model:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
Call:
lm(formula = model_formula, data = dfTrain)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.6022 -0.1034  0.0145  0.1066  1.7941 

Coefficients:
                Estimate Std. Error t value             Pr(&amp;gt;|t|)    
(Intercept)     2.147009   0.004993  429.99 &amp;lt; 0.0000000000000002 ***
I(carat^(1/3))  6.246412   0.005365 1164.27 &amp;lt; 0.0000000000000002 ***
clarity.L       0.922295   0.005036  183.15 &amp;lt; 0.0000000000000002 ***
clarity.Q      -0.295539   0.004734  -62.43 &amp;lt; 0.0000000000000002 ***
clarity.C       0.166979   0.004068   41.05 &amp;lt; 0.0000000000000002 ***
clarity^4      -0.068591   0.003260  -21.04 &amp;lt; 0.0000000000000002 ***
clarity^5       0.032833   0.002669   12.30 &amp;lt; 0.0000000000000002 ***
clarity^6      -0.001904   0.002325   -0.82              0.41288    
clarity^7       0.025508   0.002049   12.45 &amp;lt; 0.0000000000000002 ***
color.L        -0.488882   0.002927 -167.05 &amp;lt; 0.0000000000000002 ***
color.Q        -0.117319   0.002680  -43.78 &amp;lt; 0.0000000000000002 ***
color.C        -0.012230   0.002497   -4.90           0.00000098 ***
color^4         0.019007   0.002288    8.31 &amp;lt; 0.0000000000000002 ***
color^5        -0.008110   0.002159   -3.76              0.00017 ***
color^6        -0.000396   0.001967   -0.20              0.84055    
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

Residual standard error: 0.166 on 40442 degrees of freedom
Multiple R-squared:  0.973, Adjusted R-squared:  0.973 
F-statistic: 1.05e+05 on 14 and 40442 DF,  p-value: &amp;lt;0.0000000000000002&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part2-linear-models/index_files/figure-html/final_model_summary-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here’s a scatterplot of actual v/s predicted log(price) from our final model on the testing set:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part2-linear-models/index_files/figure-html/final_model_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The points lie close to the 45 degress line. However, on the high end, there are many outliers where actual and predicted values have very high variance.
Nevertheless, this is as good as it gets.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Diamonds - Part 1 - In the rough - An Exploratory Data Analysis</title>
      <link>/casestudies/diamonds-part1-eda/</link>
      <pubDate>Tue, 20 Dec 2016 00:00:00 +0000</pubDate>
      <guid>/casestudies/diamonds-part1-eda/</guid>
      <description>


&lt;p&gt;In this case study, we will explore the &lt;code&gt;diamonds&lt;/code&gt; dataset, then build linear and non-linear regression models to predict the price of diamonds.&lt;/p&gt;
&lt;div id=&#34;data-description&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Description&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;diamonds&lt;/code&gt; dataset contains the prices in 2008 USD terms, and other attributes of almost 54,000 diamonds.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Attribute&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;price&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;price in 2008 USD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;carat&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;weight of a diamond (1 carat = 0.2 gms)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;cut&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;quality of the cut (Fair, Good, Very Good, Premium, Ideal)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;color&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;diamond color from D (best) to J (worst)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;clarity&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;x&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;length in mm&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;y&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;width in mm&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;z&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;depth in mm&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;depth&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;total depth percentage = z/mean(x, y)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;table&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;width of the top of diamond relative to widest point&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;center&gt;
&lt;p&gt;&lt;img src=&#34;xyz.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;color.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;clarity.png&#34; /&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;div id=&#34;data-summaries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Summaries&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part1-eda/index_files/figure-html/summary_visual-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A preliminary visual summary of the whole dataset shows all the features and their types. There are no missing values (NAs) in this dataset.&lt;/p&gt;
&lt;p&gt;Let’s examine each feature numerically:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dfInput 

 10  Variables      53940  Observations
----------------------------------------------------------------------------------------------------------------------------------------------------------------
price 
       n  missing distinct     Info     Mean      Gmd      .05      .10      .25      .50      .75      .90      .95 
   53940        0    11602        1     3933     4012      544      646      950     2401     5324     9821    13107 

lowest :   326   327   334   335   336, highest: 18803 18804 18806 18818 18823
----------------------------------------------------------------------------------------------------------------------------------------------------------------
carat 
       n  missing distinct     Info     Mean      Gmd      .05      .10      .25      .50      .75      .90      .95 
   53940        0      273    0.999   0.7979   0.5122     0.30     0.31     0.40     0.70     1.04     1.51     1.70 

lowest : 0.20 0.21 0.22 0.23 0.24, highest: 4.00 4.01 4.13 4.50 5.01
----------------------------------------------------------------------------------------------------------------------------------------------------------------
cut 
       n  missing distinct 
   53940        0        5 

lowest : Fair      Good      Very Good Premium   Ideal    , highest: Fair      Good      Very Good Premium   Ideal    
                                                            
Value           Fair      Good Very Good   Premium     Ideal
Frequency       1610      4906     12082     13791     21551
Proportion     0.030     0.091     0.224     0.256     0.400
----------------------------------------------------------------------------------------------------------------------------------------------------------------
color 
       n  missing distinct 
   53940        0        7 

lowest : J I H G F, highest: H G F E D
                                                    
Value          J     I     H     G     F     E     D
Frequency   2808  5422  8304 11292  9542  9797  6775
Proportion 0.052 0.101 0.154 0.209 0.177 0.182 0.126
----------------------------------------------------------------------------------------------------------------------------------------------------------------
clarity 
       n  missing distinct 
   53940        0        8 

lowest : I1   SI2  SI1  VS2  VS1 , highest: VS2  VS1  VVS2 VVS1 IF  
                                                          
Value         I1   SI2   SI1   VS2   VS1  VVS2  VVS1    IF
Frequency    741  9194 13065 12258  8171  5066  3655  1790
Proportion 0.014 0.170 0.242 0.227 0.151 0.094 0.068 0.033
----------------------------------------------------------------------------------------------------------------------------------------------------------------
depth 
       n  missing distinct     Info     Mean      Gmd      .05      .10      .25      .50      .75      .90      .95 
   53940        0      184    0.999    61.75    1.515     59.3     60.0     61.0     61.8     62.5     63.3     63.8 

lowest : 43.0 44.0 50.8 51.0 52.2, highest: 72.2 72.9 73.6 78.2 79.0
----------------------------------------------------------------------------------------------------------------------------------------------------------------
table 
       n  missing distinct     Info     Mean      Gmd      .05      .10      .25      .50      .75      .90      .95 
   53940        0      127     0.98    57.46    2.448       54       55       56       57       59       60       61 

lowest : 43.0 44.0 49.0 50.0 50.1, highest: 71.0 73.0 76.0 79.0 95.0
----------------------------------------------------------------------------------------------------------------------------------------------------------------
x 
       n  missing distinct     Info     Mean      Gmd      .05      .10      .25      .50      .75      .90      .95 
   53940        0      554        1    5.731    1.276     4.29     4.36     4.71     5.70     6.54     7.31     7.66 

lowest :  0.00  3.73  3.74  3.76  3.77, highest: 10.01 10.02 10.14 10.23 10.74
----------------------------------------------------------------------------------------------------------------------------------------------------------------
y 
       n  missing distinct     Info     Mean      Gmd      .05      .10      .25      .50      .75      .90      .95 
   53940        0      552        1    5.735    1.269     4.30     4.36     4.72     5.71     6.54     7.30     7.65 

lowest :  0.00  3.68  3.71  3.72  3.73, highest: 10.10 10.16 10.54 31.80 58.90
                                                                                                                      
Value        0.0   3.5   4.0   4.5   5.0   5.5   6.0   6.5   7.0   7.5   8.0   8.5   9.0   9.5  10.0  10.5  32.0  59.0
Frequency      7     5  1731 12305  7817  5994  6742  9260  4298  3402  1635   652    69    14     6     1     1     1
Proportion 0.000 0.000 0.032 0.228 0.145 0.111 0.125 0.172 0.080 0.063 0.030 0.012 0.001 0.000 0.000 0.000 0.000 0.000

For the frequency table, variable is rounded to the nearest 0.5
----------------------------------------------------------------------------------------------------------------------------------------------------------------
z 
       n  missing distinct     Info     Mean      Gmd      .05      .10      .25      .50      .75      .90      .95 
   53940        0      375        1    3.539   0.7901     2.65     2.69     2.91     3.53     4.04     4.52     4.73 

lowest :  0.00  1.07  1.41  1.53  2.06, highest:  6.43  6.72  6.98  8.06 31.80
                                                                                                          
Value        0.0   1.0   1.5   2.0   2.5   3.0   3.5   4.0   4.5   5.0   5.5   6.0   6.5   7.0   8.0  32.0
Frequency     20     1     2     3  8807 13809  9474 13682  5525  2352   237    20     5     1     1     1
Proportion 0.000 0.000 0.000 0.000 0.163 0.256 0.176 0.254 0.102 0.044 0.004 0.000 0.000 0.000 0.000 0.000

For the frequency table, variable is rounded to the nearest 0.5
----------------------------------------------------------------------------------------------------------------------------------------------------------------&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;price&lt;/code&gt;: The average price of a diamond in this dataset is ~ USD 4000. There are many outliers on the high end.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;carat&lt;/code&gt;: The average carat weight is ~ 0.8. About 75% of the diamonds are under 1 carat. The top 5 values show presence of many outliers on the high end.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cut&lt;/code&gt;: About 40% of the diamonds are of &lt;em&gt;Ideal&lt;/em&gt; cut. Only 3% are &lt;em&gt;Fair&lt;/em&gt; cut. So there is a lot of imbalance in the categories.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;color&lt;/code&gt;: Most of the diamonds are rated &lt;em&gt;E&lt;/em&gt; to &lt;em&gt;H&lt;/em&gt; color. Relatively fewer are rated &lt;em&gt;J&lt;/em&gt; color.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;clarity&lt;/code&gt;: Most of the diamonds are rated &lt;em&gt;SI2&lt;/em&gt; to &lt;em&gt;VS1&lt;/em&gt; clarity. About 1% are rated the worst &lt;em&gt;I1&lt;/em&gt; clarity, where as only ~ 3% are rated &lt;em&gt;IF&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;depth&lt;/code&gt;: Most of the depth values are between 60 and 64. There are outliers on both low end and high end.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;table&lt;/code&gt;: Most of the table values are between 54 and 65. There are outliers on both ends.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt;: Denotes the dimension along the x-axis. Most values are between 4 and 8. There are some 0 values too which means they were not recorded.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt;: Denotes the dimension along the y-axis. Most values are between 3.5 and 8. There are 7 records where the values are 0.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;z&lt;/code&gt;: Denotes the dimension along the z-axis. Most values are between 2.5 and 8.5. There are 20 records where the values are 0.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;univariate-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Univariate Analysis&lt;/h2&gt;
&lt;p&gt;Let us look at each feature in the dataset in detail.&lt;/p&gt;
&lt;div id=&#34;numerical-features&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Numerical Features&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part1-eda/index_files/figure-html/univariate_continuous-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The plots show presence of outliers within each feature. Let’s exclude the outliers and plot them again.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part1-eda/index_files/figure-html/univariate_continuous_ex_outliers-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Excluding outliers, the range of values are more reasonable. We can see that &lt;code&gt;carat&lt;/code&gt; and &lt;code&gt;price&lt;/code&gt; are heavily right skewed.&lt;/p&gt;
&lt;p&gt;Let’s plot the distribution of &lt;code&gt;price&lt;/code&gt; in log scale:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part1-eda/index_files/figure-html/price_distribution-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Two peaks in the log transformed plot show a bimodal distribution of prices. This implies two price points of diamonds are most popular among customers -
one at just below USD 1000 and the other around USD 5000. Intriguingly, there are no diamonds in the dataset that are around USD 1500. Hence, a big gap is visible around that price.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;categorical-features&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Categorical Features&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part1-eda/index_files/figure-html/Univariate_Categorical-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The categorical imbalance in &lt;code&gt;cut&lt;/code&gt; and &lt;code&gt;clarity&lt;/code&gt; can be clearly noticed.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bivariate-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bivariate Analysis&lt;/h2&gt;
&lt;p&gt;Let’s examine the relationship of &lt;code&gt;price&lt;/code&gt; with other features.&lt;/p&gt;
&lt;div id=&#34;numerical-numerical&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Numerical-numerical&lt;/h4&gt;
&lt;p&gt;First and foremost, let’s do a correlation analysis to see how &lt;code&gt;price&lt;/code&gt; is correlated with other numerical features:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part1-eda/index_files/figure-html/bivariate_correlations-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that &lt;code&gt;price&lt;/code&gt; is very strongly correlated with &lt;code&gt;carat&lt;/code&gt;, &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;, and &lt;code&gt;z&lt;/code&gt; dimensions. If a predictive linear regression model is built,
some of these features would act as confounders. &lt;code&gt;table&lt;/code&gt; and &lt;code&gt;depth&lt;/code&gt; have almost no correlation with &lt;code&gt;price&lt;/code&gt; so they are not so interesting for
predictive modelling.&lt;/p&gt;
&lt;p&gt;Now let’s see the scatter plots:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part1-eda/index_files/figure-html/bivariate_scatterplots-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;After removing outliers, it could be noted that &lt;code&gt;price&lt;/code&gt; increases exponentially with &lt;code&gt;carat&lt;/code&gt;, as well as &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;z&lt;/code&gt; dimensions. So &lt;code&gt;price&lt;/code&gt; should be plotted with a log tranformation. Let’s do that:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part1-eda/index_files/figure-html/log_scatterplots-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, the relationship between &lt;code&gt;log(price)&lt;/code&gt; appears to be linear with &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;z&lt;/code&gt;. But, not so much with &lt;code&gt;carat&lt;/code&gt;. Variance in &lt;code&gt;price&lt;/code&gt; tends to
increase both by &lt;code&gt;carat&lt;/code&gt; and its dimensions. Log transforming &lt;code&gt;carat&lt;/code&gt; wouldn’t help because &lt;code&gt;carat&lt;/code&gt; does not have a wide range.
We will find ways to deal with this when we do Feature Engineering.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;numerical-categorical&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Numerical-Categorical&lt;/h4&gt;
&lt;p&gt;Let’s examine &lt;code&gt;price&lt;/code&gt; with respect to the categorical features in the dataset:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part1-eda/index_files/figure-html/Bivariate_Cont_Cat-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The boxplots above are plotted with truncated &lt;code&gt;price&lt;/code&gt; axis for better visualization of trends. All the boxplots are counter-intuitive - median prices tend to decline as we move from lowest grade to highest grade in terms of &lt;code&gt;cut&lt;/code&gt;, &lt;code&gt;color&lt;/code&gt; and &lt;code&gt;clarity&lt;/code&gt;. This is very odd.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The median &lt;code&gt;price&lt;/code&gt; declines monotonically from &lt;em&gt;Fair&lt;/em&gt; &lt;code&gt;cut&lt;/code&gt; to &lt;em&gt;Ideal&lt;/em&gt; &lt;code&gt;cut&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;In terms of &lt;code&gt;color&lt;/code&gt;, the median &lt;code&gt;price&lt;/code&gt; decreases from &lt;em&gt;J&lt;/em&gt; (worst) to &lt;em&gt;G&lt;/em&gt; (mid-grade), then increases and finally decreases for &lt;em&gt;D&lt;/em&gt; (best).&lt;/li&gt;
&lt;li&gt;The median &lt;code&gt;price&lt;/code&gt; increases when &lt;code&gt;clarity&lt;/code&gt; improves from &lt;em&gt;I1&lt;/em&gt; to &lt;em&gt;SI2&lt;/em&gt;, and then decreases monotonically to &lt;em&gt;IF&lt;/em&gt; grade.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;multivariate-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multivariate Analysis&lt;/h2&gt;
&lt;p&gt;So far, we have determined &lt;code&gt;carat&lt;/code&gt;, &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;, and &lt;code&gt;z&lt;/code&gt; have the strongest relationship with &lt;code&gt;price.&lt;/code&gt; Different grades of &lt;code&gt;cut&lt;/code&gt;, &lt;code&gt;color&lt;/code&gt; and &lt;code&gt;clarity&lt;/code&gt; also seem to have some impact on median &lt;code&gt;price&lt;/code&gt;. So let’s make some scatter plots to see these relationships:&lt;/p&gt;
&lt;div id=&#34;numerical-numerical-categorical&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Numerical-Numerical-Categorical&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part1-eda/index_files/figure-html/multivariate_num_num_cat-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Although there is a lot of overlap, but there is a clear trend of &lt;code&gt;price&lt;/code&gt; increasing with &lt;code&gt;clarity&lt;/code&gt;, at a given &lt;code&gt;carat&lt;/code&gt; weight. The same pattern could also be observed in the plot with increasing grades of &lt;code&gt;color&lt;/code&gt;, though not to the same extent. There is no evidence of any relationship between &lt;code&gt;price&lt;/code&gt; and &lt;code&gt;carat&lt;/code&gt; with &lt;code&gt;cut&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can conclude both &lt;code&gt;color&lt;/code&gt; and &lt;code&gt;clarity&lt;/code&gt; explain some variance in &lt;code&gt;price&lt;/code&gt; at a given &lt;code&gt;carat&lt;/code&gt; weight.&lt;/p&gt;
&lt;p&gt;To be sure of any interaction between &lt;code&gt;table&lt;/code&gt; and &lt;code&gt;depth&lt;/code&gt;, with &lt;code&gt;color&lt;/code&gt; and &lt;code&gt;clarity&lt;/code&gt;, let’s plot these:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part1-eda/index_files/figure-html/multivariate_plots_other-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is no pattern in the interaction of &lt;code&gt;price&lt;/code&gt; v/s &lt;code&gt;depth&lt;/code&gt; and &lt;code&gt;table&lt;/code&gt; values when plotted by &lt;code&gt;color&lt;/code&gt; and &lt;code&gt;clarity&lt;/code&gt;. So, these features do not have any predictive ability to determine &lt;code&gt;price&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;categorical-categorical-numerical&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Categorical-Categorical-Numerical&lt;/h4&gt;
&lt;p&gt;We want to see if there is any interaction of &lt;code&gt;clarity&lt;/code&gt; with &lt;code&gt;cut&lt;/code&gt; and &lt;code&gt;color&lt;/code&gt;, that could provide any additional explanatory power to predict &lt;code&gt;price&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part1-eda/index_files/figure-html/multivariate_cat_cat_num-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The second heatmap appears to be more interesting. From bottom left to top right, with increasing grades of &lt;code&gt;color&lt;/code&gt; and &lt;code&gt;clarity&lt;/code&gt;, &lt;code&gt;price&lt;/code&gt; tends to decrease on average. Once again, this runs counter to our intuition; after all prices of diamonds with the best &lt;code&gt;color&lt;/code&gt; and &lt;code&gt;clarity&lt;/code&gt; should be the highest. Nevertheless this counter-trend persists in the dataset.&lt;/p&gt;
&lt;p&gt;With respect to &lt;code&gt;cut&lt;/code&gt; and &lt;code&gt;clarity&lt;/code&gt;, the mean prices do not show any discernable pattern.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;To summarize, here’s what we found interesting in this dataset, after doing an exploratory data analysis:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;price&lt;/code&gt; is heavily right-skewed, and when log tranformed, has a bimodal distribution which implies there is demand in 2 different price ranges.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;carat&lt;/code&gt; about 75% of the diamonds are below 1 carat. The variance in price increases with carat weight.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cut&lt;/code&gt; is imbalanced with about 40% of the diamonds rated &lt;em&gt;Ideal&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;color&lt;/code&gt; is imbalanced with about 5% of the diamonds rated &lt;em&gt;J&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;clarity&lt;/code&gt; is imbalanced at the extremes, with only 1.5% of the diamonds rated &lt;em&gt;I1&lt;/em&gt; and 3.3% of the diamonds rated &lt;em&gt;IF&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;price&lt;/code&gt; is strongly correlated with &lt;code&gt;carat&lt;/code&gt; and &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;, &lt;code&gt;z&lt;/code&gt; dimensions of the diamonds. &lt;code&gt;table&lt;/code&gt; and &lt;code&gt;depth&lt;/code&gt; have almost no correlation with &lt;code&gt;price&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Both &lt;code&gt;clarity&lt;/code&gt; and &lt;code&gt;color&lt;/code&gt; appear to explain some variance in &lt;code&gt;price&lt;/code&gt; for a given &lt;code&gt;carat&lt;/code&gt; weight.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
