<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>models | Nitin Gupta</title>
    <link>/tags/models/</link>
      <atom:link href="/tags/models/index.xml" rel="self" type="application/rss+xml" />
    <description>models</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Nitin Gupta. All Rights Reserved.</copyright><lastBuildDate>Tue, 27 Jun 2017 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>models</title>
      <link>/tags/models/</link>
    </image>
    
    <item>
      <title>Investment Strategy Diversification</title>
      <link>/investing/investment-strategy-diversification/</link>
      <pubDate>Tue, 27 Jun 2017 00:00:00 +0000</pubDate>
      <guid>/investing/investment-strategy-diversification/</guid>
      <description>


&lt;p&gt;By all conventional and even some unconventional measures, the US stock market is trading way &lt;a href=&#34;http://mebfaber.com/2017/03/22/risky-stock-market-retail-investors-4x-risky-advisors/&#34;&gt;beyond historical valuation averages&lt;/a&gt; and is closer to all time highs. Passive stock index investors have enjoyed a period of extraordinary gains in one of the longest running bull markets. No other major asset class has come close in the last 7 years. Well diversified portfolios have had lackluster returns while the stock market keeps making new highs.&lt;/p&gt;
&lt;table class=&#34;gmisc_table&#34; style=&#34;border-collapse: collapse; margin-top: 1em; margin-bottom: 1em;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;td colspan=&#34;9&#34; style=&#34;text-align: left;&#34;&gt;
Table 1: Performance by Asset Class
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
US Stocks
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
Foreign Stocks
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
US Bonds
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
Foreign Bonds
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
Treasuries
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
Gold
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
Commodities
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
REITs
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td colspan=&#34;9&#34; style=&#34;font-weight: 900;&#34;&gt;
7 Years
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2010 - Apr 2017 Annualized Return
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
12.72%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
4.47%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
3.36%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.79%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
7.28%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.66%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-6.91%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.09%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan=&#34;9&#34; style=&#34;font-weight: 900;&#34;&gt;
5 Years
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2012 - Apr 2017 Annualized Return
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
13.54%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
5.40%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
2.23%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-0.61%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
3.53%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-5.69%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-12.23%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
9.16%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan=&#34;9&#34; style=&#34;font-weight: 900;&#34;&gt;
3 Years
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: left;&#34;&gt;
  May 2014 - Apr 2017 Annualized Return
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
10.35%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
1.09%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
2.64%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-3.14%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
5.95%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-0.93%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-17.55%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
8.82%
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;But overstretched valuations in a contentious political climate raise fears of what’s around the corner. Small market gains accumulated gradually over months could be wiped out within days or weeks, when the sentiment turns. So what should investors do?&lt;/p&gt;
&lt;p&gt;Momentum strategies provide an attractive alternative in this scenario. Momentum has been described as a premier market anomaly. Studies have been published for over 2 decades showing why and how well it works. Despite being well publicized in the investment world, momentum strategies have continued to work well over long periods of time.&lt;/p&gt;
&lt;p&gt;Some of the biggest advantages of momentum strategies are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Their entry and exit rules could be well-defined and understood&lt;/li&gt;
&lt;li&gt;They could be implemented using highly liquid, low cost ETFs&lt;/li&gt;
&lt;li&gt;They are relatively easier to scale and execute&lt;/li&gt;
&lt;li&gt;During market downturns, these strategies exhibit lower volatilities and drawdowns as compared to value and passive investing strategies&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That being said, momentum is not a single monolithic strategy as many academics portray it to be. It could be implemented in many different ways. Here I present three momentum strategies that could be combined in simple ways to mitigate overall volatility and drawdowns, while maintaining attractive returns.&lt;/p&gt;
&lt;div id=&#34;global-equities-momentum-strategy&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Global Equities Momentum Strategy&lt;/h3&gt;
&lt;p&gt;The first of these is Global Equities Momentum (“GEM”), which has been detailed by Gary Antonacci in his fantastic book, &lt;a href=&#34;https://www.amazon.com/Dual-Momentum-Investing-Innovative-Strategy/dp/0071849440/ref=sr_1_1?ie=UTF8&amp;amp;qid=1498666714&amp;amp;sr=8-1&amp;amp;keywords=dual+momentum+investing&#34;&gt;Dual Momentum Investing&lt;/a&gt;. It is one of the simplest strategies to understand and implement. GEM effectively makes use of zero correlation betweeen stocks and bonds. It utilizes absolute momentum to switch to bonds when stocks show weak performance, and bonds tend to do well. Additionally, it uses relative momentum to switch between domestic and foreign stocks during times of stock market strength.&lt;/p&gt;
&lt;p&gt;Here are the concrete rules, dataset and assumptions to backtest GEM:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Rules&lt;/em&gt;&lt;/strong&gt; : Between domestic and foreign equities, invest in the one which had higher outperformance relative to the risk free rate in the past 12 months. Otherwise invest in bonds.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Dataset&lt;/em&gt;&lt;/strong&gt; : Total returns calculated from daily adjusted closing prices downloaded from Yahoo! Finance&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Domestic stocks (SPY, combined with VFINX prior to Feb 1993)&lt;/li&gt;
&lt;li&gt;Foreign stocks (CWI, combined with VGTSX prior to Jan 2007)&lt;/li&gt;
&lt;li&gt;Bonds (AGG, combined with VBMFX prior to Oct 2003)&lt;/li&gt;
&lt;li&gt;Risk free rate determined by T-bills data downloaded from &lt;a href=&#34;https://fred.stlouisfed.org/series/TB3MS&#34;&gt;FRED&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Assumptions&lt;/em&gt;&lt;/strong&gt; : Signals and weights determined at the end of month (t), rebalanced at the end of first business day of next month (t+1), transaction costs 20 bps per trade.&lt;/p&gt;
&lt;table class=&#34;gmisc_table&#34; style=&#34;border-collapse: collapse; margin-top: 1em; margin-bottom: 1em;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;td colspan=&#34;5&#34; style=&#34;text-align: left;&#34;&gt;
Table 2: Comparison of GEM strategy with an equally weighted portfolio of SPY, CWI and AGG
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
SP500TR
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
GEM_Antonacci
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
GEM
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
EqualWts
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
May 1997 - Apr 2017 Annualized Return
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
7.52%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
12.66%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.79%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
6.24%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
May 1997 - Apr 2017 Annualized Std Dev
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
15.17%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.93%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.97%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.68%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
May 1997 - Apr 2017 Annualized Sharpe (Rf=2.04%)
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.35
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.87
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.80
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.38
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: left;&#34;&gt;
May 1997 - Apr 2017 Worst Drawdown
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-50.95%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-17.84%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-19.32%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-38.77%
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It is easy to cross-check the backtesting results with simulated results from Gary’s &lt;a href=&#34;http://www.optimalmomentum.com/gem_trackrecord.html&#34;&gt;website&lt;/a&gt;. I do not expect to match the annualized returns as Gary’s simulations use theoretical indexes, do not apply any fees or transaction costs, and monthly performance numbers on his website are rounded to the first decimal place. But a correlation of monthly returns of 0.99, annualized standard deviations, shape of the equity curves and the close alignment of drawdowns all suggest a close match.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/investing/investment-strategy-diversification/index_files/figure-html/GEM_weights-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The GEM strategy performance shows simple rules could be used for timing entries and exits and work exceedingly well over complete market cycles. GEM trounces the S&amp;amp;P 500 Total Returns Index on both absolute and risk adjusted basis over 2 market cycles in this backtesting period.&lt;/p&gt;
&lt;p&gt;The results of an equally weighted long only strategy of SPY, CWI and AGG are also shown. Essentially it’s a simplified and slightly aggressive 66.67% stocks and 33.33% bonds portfolio, which gives a sense of the performance of a passive structurally allocated portfolio strategy. As compared to the broader market, it does reduce the volatility but a worst drawdown of ~ 40% is still driven by stocks.&lt;/p&gt;
&lt;p&gt;The edge provided by simple market timing rules in GEM is evident as compared to an equally weighted long only portfolio of the 3 underlying assets. The drawdowns are much lower as there’s a clear rule for exiting the market when it underperforms relative to the risk free rate. Moreover, there have been very few trades over the past 20 years, and most of the gains have been long term in nature. Hence GEM is quite attractive from a tax standpoint too.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;spy-tlt-universal-investment-strategy&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;SPY-TLT Universal Investment Strategy&lt;/h3&gt;
&lt;p&gt;This strategy has been detailed on &lt;a href=&#34;https://logical-invest.com/universal-investment-strategy/&#34;&gt;LogicalInvest&lt;/a&gt;. It utilizes just 2 asset classes, stocks and long term treasuries, and takes advantage of a slighly inverse correlation between the two. While GEM rotates between stocks and bonds in a 100% risk-on / risk-off manner, this strategy is more nuanced. It rotates between stocks and long term treasuries in a more measured way. It selects between a mix of stocks and treasuries which has performed the best on a risk-adjusted basis, in the recent past.&lt;/p&gt;
&lt;p&gt;Here are the concrete rules, dataset and assumptions to backtest UIS:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Rules&lt;/em&gt;&lt;/strong&gt; : Construct 11 portfolios ranging from 100% SPY, 0% TLT to 0% SPY, 100% TLT, by successively incrementing the weight of TLT by 10%. Determine the returns and volatility of these 11 portfolios in the past 60 days (~ 3 months). Select the portfolio that gives the best risk adjusted returns in this period and apply the same weights to the following month.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Dataset&lt;/em&gt;&lt;/strong&gt; : Total returns calculated from daily adjusted closing prices downloaded from Yahoo! Finance&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stocks (SPY, combined with VFINX prior to Feb 1993)&lt;/li&gt;
&lt;li&gt;Long-term Treasuries (TLT, combined with VUSTX prior to Aug 2002).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Assumptions&lt;/em&gt;&lt;/strong&gt; : Weights determined at the end of month (t), rebalanced at the end of first business day of next month (t+1), transaction costs 20 bps per trade.&lt;/p&gt;
&lt;table class=&#34;gmisc_table&#34; style=&#34;border-collapse: collapse; margin-top: 1em; margin-bottom: 1em;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;td colspan=&#34;4&#34; style=&#34;text-align: left;&#34;&gt;
Table 3: Comparison of UIS strategy with an equally weighted portfolio of SPY and TLT
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
SP500TR
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
UIS
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
EqualWts
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
Sep 1986 - Apr 2017 Annualized Return
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.01%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.46%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
9.12%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
Sep 1986 - Apr 2017 Annualized Std Dev
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
14.99%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.22%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
8.83%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
Sep 1986 - Apr 2017 Annualized Sharpe (Rf=3.23%)
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.44
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.71
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.64
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: left;&#34;&gt;
Sep 1986 - Apr 2017 Worst Drawdown
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-50.95%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-23.51%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-22.35%
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img src=&#34;/investing/investment-strategy-diversification/index_files/figure-html/UIS_weights-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The performance of both UIS and an equally weighted long only mix of stocks and treasuries show the benefits of diversication between these 2 asset classes. In fact an equally weighted portfolio would’ve done just about as well as the S&amp;amp;P 500 Total Returns Index with far lower volatility and drawdowns. Since the global financial crisis of 2008, UIS has performed exceedingly well, which has led to its outperformance on both absolute as well as risk-adjusted basis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;three-way-momentum-strategy&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Three-Way Momentum Strategy&lt;/h3&gt;
&lt;p&gt;This strategy is a more simplified version of a &lt;a href=&#34;http://mebfaber.com/2015/06/16/three-way-model/&#34;&gt;three-way momentum model&lt;/a&gt; posted by Meb Faber on his blog, that was originally done by Ned Davis Research. As Meb has suggested, it is conceptually similar to the strategies in his &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=962461&#34;&gt;QTAA paper&lt;/a&gt;. This strategy is also one of the simplest to understand and implement. It utilizes just 3 asset classes: stocks, bonds and gold and invests equally in all asset classes that show positive momentum.&lt;/p&gt;
&lt;p&gt;Here are the concrete rules, dataset and assumptions to backtest this strategy:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Rules&lt;/em&gt;&lt;/strong&gt; : Invest equally in stocks, bonds and gold as long as they trade above their past 200 days moving average. If none of them trades above their 200 days moving average, then invest in T-bills.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Dataset&lt;/em&gt;&lt;/strong&gt; : Total returns calculated from daily adjusted closing prices downloaded from Yahoo! Finance&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stocks (SPY, combined with VFINX prior to Feb 1993)&lt;/li&gt;
&lt;li&gt;Bonds (AGG, combined with VBMFX prior to Oct 2003)&lt;/li&gt;
&lt;li&gt;Gold (GLD, combined with &lt;a href=&#34;https://www.quandl.com/data/LBMA/GOLD-Gold-Price-London-Fixing&#34;&gt;LBMA/GOLD&lt;/a&gt; prior to Nov 2004).&lt;/li&gt;
&lt;li&gt;Risk free rate determined by T-bills data downloaded from &lt;a href=&#34;https://fred.stlouisfed.org/series/TB3MS&#34;&gt;FRED&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Assumptions&lt;/em&gt;&lt;/strong&gt; : Signals and weights determined at the end of month (t), rebalanced at the end of first business day of next month (t+1), transaction costs 20 bps per trade.&lt;/p&gt;
&lt;table class=&#34;gmisc_table&#34; style=&#34;border-collapse: collapse; margin-top: 1em; margin-bottom: 1em;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;td colspan=&#34;4&#34; style=&#34;text-align: left;&#34;&gt;
Table 4: Comparison of Three-Way Momentum Strategy with an equally weighted portfolio of SPY, AGG and GLD
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
SP500TR
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
ThreeWay
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
EqualWts
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
Nov 1987 - Apr 2017 Annualized Return
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.31%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
8.60%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
7.05%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
Nov 1987 - Apr 2017 Annualized Std Dev
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
14.31%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
7.57%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
7.22%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
Nov 1987 - Apr 2017 Annualized Sharpe (Rf=3.13%)
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.48
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.70
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.52
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: left;&#34;&gt;
Nov 1987 - Apr 2017 Worst Drawdown
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-50.95%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-13.34%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-18.71%
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img src=&#34;/investing/investment-strategy-diversification/index_files/figure-html/ThreeWay_weights-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;strategy-correlations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Strategy Correlations&lt;/h3&gt;
&lt;p&gt;Although GEM works very well on its own, it is unlikely that the other two strategies could be followed on a stand-alone basis. Even the 100% risk-on / risk-off nature of GEM make it difficult to employ it at scale.&lt;/p&gt;
&lt;p&gt;But let’s look at the correlations of these strategies:&lt;/p&gt;
&lt;table class=&#34;gmisc_table&#34; style=&#34;border-collapse: collapse; margin-top: 1em; margin-bottom: 1em;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;td colspan=&#34;5&#34; style=&#34;text-align: left;&#34;&gt;
Table 5: Correlation of monthly returns (May 1997 to April 2017)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
SP500TR
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
GEM
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
UIS
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
ThreeWay
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
SP500TR
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.67
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.26
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.35
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
GEM
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.67
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.31
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.55
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
UIS
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.26
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.31
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.37
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: left;&#34;&gt;
ThreeWay
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
0.35
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
0.55
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
0.37
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The UIS strategy has low correlations with both GEM and the Three-Way strategy. Even GEM and the Three-Way Strategy have a correlation of ~ 0.55. So they could be effectively blended together to form a portfolio of diversified strategies.&lt;/p&gt;
&lt;p&gt;Here are a couple of simple ways to blend these strategies:&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;strategy-blends&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Strategy Blends&lt;/h3&gt;
&lt;p&gt;In the last 20 years backtesting period, an equal weight combination of GEM and UIS results in a significant reduction in portfolio volatility without compromising returns. Hence the blended strategy gets a solid boost on risk adjusted returns.&lt;/p&gt;
&lt;table class=&#34;gmisc_table&#34; style=&#34;border-collapse: collapse; margin-top: 1em; margin-bottom: 1em;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;td colspan=&#34;5&#34; style=&#34;text-align: left;&#34;&gt;
Table 6: Comparison of a blended strategy with 50% each of GEM &amp;amp; UIS strategies
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
SP500TR
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
GEM
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
UIS
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
Equal_Blend
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
May 1997 - Apr 2017 Annualized Return
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
7.52%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.79%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
12.10%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
12.20%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
May 1997 - Apr 2017 Annualized Std Dev
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
15.17%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.97%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.15%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
9.38%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
May 1997 - Apr 2017 Annualized Sharpe (Rf=2.04%)
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.35
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.80
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.88
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.06
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: left;&#34;&gt;
May 1997 - Apr 2017 Worst Drawdown
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-50.95%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-19.32%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-17.81%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-11.81%
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Combining GEM, UIS and Three-Way strategies on an equal weight basis, leads to even further reduction in portfolio volatility. At 8% it is closer to the lowest volatility of the three strategies. Even though the annualized return of the blended strategy is slightly lower than that of GEM, the risk adjusted return is ~ 40% higher.&lt;/p&gt;
&lt;table class=&#34;gmisc_table&#34; style=&#34;border-collapse: collapse; margin-top: 1em; margin-bottom: 1em;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;td colspan=&#34;6&#34; style=&#34;text-align: left;&#34;&gt;
Table 7: Comparison of a blended strategy with 33.33% each of GEM, UIS &amp;amp; ThreeWay strategies
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
SP500TR
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
GEM
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
UIS
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
ThreeWay
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
Equal_Blend
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td colspan=&#34;6&#34; style=&#34;font-weight: 900;&#34;&gt;
ALL Years
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 1997 - Apr 2017 Annualized Return
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
7.52%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.79%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
12.10%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
9.34%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.32%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 1997 - Apr 2017 Annualized Std Dev
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
15.17%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.97%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.15%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
7.72%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
8.00%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 1997 - Apr 2017 Annualized Sharpe (Rf=2.04%)
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.35
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.80
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.88
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.93
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.14
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 1997 - Apr 2017 Worst Drawdown
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-50.95%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-19.32%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-17.81%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-9.39%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-9.46%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan=&#34;6&#34; style=&#34;font-weight: 900;&#34;&gt;
10 Years
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2007 - Apr 2017 Annualized Return
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
7.00%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
6.86%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
16.07%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
8.39%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.69%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2007 - Apr 2017 Annualized Std Dev
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
15.25%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.83%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
12.56%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
8.13%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
7.91%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2007 - Apr 2017 Annualized Sharpe (Rf=0.52%)
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.42
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.53
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.23
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.96
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.28
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2007 - Apr 2017 Worst Drawdown
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-50.95%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-19.32%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-14.40%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-9.39%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-9.46%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan=&#34;6&#34; style=&#34;font-weight: 900;&#34;&gt;
5 Years
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2012 - Apr 2017 Annualized Return
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
13.36%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
9.05%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.24%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
5.29%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
8.25%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2012 - Apr 2017 Annualized Std Dev
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.19%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
9.00%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
8.08%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
6.70%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
7.07%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2012 - Apr 2017 Annualized Sharpe (Rf=0.15%)
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.29
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.99
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.25
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.77
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.14
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2012 - Apr 2017 Worst Drawdown
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-8.36%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-14.52%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-10.50%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-5.04%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-9.46%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan=&#34;6&#34; style=&#34;font-weight: 900;&#34;&gt;
1 Year
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2016 - Apr 2017 Annualized Return
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
17.77%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
17.77%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
17.53%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
8.05%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
14.43%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2016 - Apr 2017 Annualized Std Dev
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
6.05%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
6.05%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
6.34%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
7.32%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
5.52%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2016 - Apr 2017 Annualized Sharpe (Rf=0.44%)
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
2.85
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
2.85
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
2.69
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.04
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
2.52
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: left;&#34;&gt;
  May 2016 - Apr 2017 Worst Drawdown
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-1.73%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-1.73%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-3.02%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-5.04%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-2.44%
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here are the equity curves and the distribution of weights in the blended strategy:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/investing/investment-strategy-diversification/index_files/figure-html/Blend_performance_weights-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;advantages&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Advantages&lt;/h3&gt;
&lt;p&gt;There are many advantages of this blended strategy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Works well in both bull and bear markets with low volatility&lt;/li&gt;
&lt;li&gt;Uses low cost, highly liquid ETFs&lt;/li&gt;
&lt;li&gt;With just 5 ETFs, it is very easy to implement and execute&lt;/li&gt;
&lt;li&gt;Low cost substitute ETFs and funds exist to employ it at scale&lt;/li&gt;
&lt;li&gt;Highly competitive with more complex strategies&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;leveraged-blends&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Leveraged Blends&lt;/h3&gt;
&lt;p&gt;Given that the blended strategy exhibits very low volatility, the returns could be boosted by using leverage. To get a good sense of leveraged returns, I benchmark the margin rate at the prevailing &lt;a href=&#34;https://fred.stlouisfed.org/series/USD1MTD156N&#34;&gt;1-month USD LIBOR rate&lt;/a&gt; + 2%. This is well above the rates offered by brokerages catering to professional investors.&lt;/p&gt;
&lt;p&gt;At the same level of volatility, the blended strategy would have produced over twice the returns of S&amp;amp;P 500 Total Returns Index, with much lower drawdowns.&lt;/p&gt;
&lt;table class=&#34;gmisc_table&#34; style=&#34;border-collapse: collapse; margin-top: 1em; margin-bottom: 1em;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;td colspan=&#34;6&#34; style=&#34;text-align: left;&#34;&gt;
Table 8: Comparison of leveraged blends
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
SP500TR
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
Blend_1X
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
Blend_1.3X
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
Blend_1.5X
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
Blend_2X
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td colspan=&#34;6&#34; style=&#34;font-weight: 900;&#34;&gt;
ALL Years
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 1997 - Apr 2017 Annualized Return
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
7.52%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.32%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
13.30%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
14.60%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
17.80%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 1997 - Apr 2017 Annualized Std Dev
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
15.17%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
8.00%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.40%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.99%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
15.99%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 1997 - Apr 2017 Annualized Sharpe (Rf=2.04%)
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.35
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.14
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.06
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.03
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.97
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 1997 - Apr 2017 Worst Drawdown
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-50.95%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-9.46%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-12.48%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-14.46%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-19.30%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan=&#34;6&#34; style=&#34;font-weight: 900;&#34;&gt;
10 Years
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2007 - Apr 2017 Annualized Return
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
7.00%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.69%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
12.99%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
14.52%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
18.30%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2007 - Apr 2017 Annualized Std Dev
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
15.25%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
7.91%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.29%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.88%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
15.86%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2007 - Apr 2017 Annualized Sharpe (Rf=0.52%)
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.42
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.28
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.21
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.17
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2007 - Apr 2017 Worst Drawdown
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-50.95%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-9.46%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-12.48%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-14.46%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-19.30%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan=&#34;6&#34; style=&#34;font-weight: 900;&#34;&gt;
5 Years
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2012 - Apr 2017 Annualized Return
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
13.36%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
8.25%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
9.98%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.13%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
13.93%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2012 - Apr 2017 Annualized Std Dev
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.19%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
7.07%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
9.20%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.61%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
14.14%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2012 - Apr 2017 Annualized Sharpe (Rf=0.15%)
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.29
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.14
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.07
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.03
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.97
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2012 - Apr 2017 Worst Drawdown
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-8.36%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-9.46%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-12.48%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-14.46%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-19.30%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan=&#34;6&#34; style=&#34;font-weight: 900;&#34;&gt;
1 Year
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2016 - Apr 2017 Annualized Return
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
17.77%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
14.43%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
18.13%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
20.65%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
27.08%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2016 - Apr 2017 Annualized Std Dev
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
6.05%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
5.52%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
7.18%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
8.28%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.04%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2016 - Apr 2017 Annualized Sharpe (Rf=0.44%)
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
2.85
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
2.52
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
2.46
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
2.43
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
2.40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: left;&#34;&gt;
  May 2016 - Apr 2017 Worst Drawdown
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-1.73%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-2.44%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-3.35%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-3.96%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-5.47%
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;further-ideas&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Further Ideas&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Scaling: Create weekly tranches, check signals and rebalance weekly instead of monthly. This way only 25% of the portfolio is exposed to whipsaw risk. Also, the trade sizes are reduced at any given point. I wrote about these ideas in a &lt;a href=&#34;../luck-in-rebalance-timing/&#34;&gt;prior post&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Risk Management: From the weights plot of the blended strategy, it is evident that during some periods, 100% of the portfolio risk is concentrated in SPY. A prudent risk management strategy would limit the concentration on any single asset in a portfolio. So sudden unfavorable movements have less of an impact.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Add more uncorrelated strategies: Instead of being all-in on momentum only, value strategies could be added to the mix, which exhibit low correlation to momentum strategies.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Alternative Implementation: Implement this strategy using futures products which provide greater control over margins to scale up or down.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Investors are either totally passive or too fixated on finding a single unicorn strategy that’ll work well during both good and bad times. The key is to put together a team of sufficiently uncorrelated strategies that work well together to build a robust portfolio.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;All content displayed here is for informational purposes only and is not guaranteed to be accurate, complete or up-to-date. The returns presented are hypothetical and do not represent returns attained by any investor. The content and commentary are intended to provide the views and observations of the author only, and are subject to change at any time without prior notice, and do NOT represent those of past, present or future employers. Nothing herein should be considered investment advice or recommendation to buy, sell or hold any securities.&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Luck in Rebalance Timing</title>
      <link>/investing/luck-in-rebalance-timing/</link>
      <pubDate>Mon, 29 May 2017 00:00:00 +0000</pubDate>
      <guid>/investing/luck-in-rebalance-timing/</guid>
      <description>


&lt;p&gt;An important and often overlooked topic was raised by Corey Hoffstein at NewFound Research. Here are his first couple of tweets on that topic:&lt;/p&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
&lt;a href=&#34;https://twitter.com/choffstein/status/867445823889694720&#34;&gt;&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/center&gt;
&lt;p&gt;Indeed, this is about rebalance timing and how little attention it gets. Within the construct of a systematic strategy, this is a part of the Execution Model.&lt;/p&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;buildingblocks_shaded.png&#34; alt=&#34;The Black Box Revealed&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;The Black Box Revealed&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Courtesy: &lt;a href=&#34;https://www.amazon.com/Inside-Black-Box-Quantitative-Frequency-ebook/dp/B00BZ9WAVW/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1487389686&amp;amp;sr=1-1&amp;amp;keywords=rishi+narang&#34;&gt;Inside the Black Box - The Simple Truth about Quantitative Trading&lt;/a&gt; by Rishi Narang&lt;/p&gt;
&lt;/center&gt;
&lt;p&gt;Unless it’s an intraday trading model, when it comes to portfolio rebalancing, a common convention is to rebalance at the beginning of a new month. Perhaps, these are some of the reasons behind this convention:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;In published literature, a monthly period is commonly used for portfolio rebalancing. Hence it has become a sort of common convention to follow.&lt;/li&gt;
&lt;li&gt;Most of the researchers are adept in using Excel, but less so in programming languages. While doing a backtest in Excel, it is easier to manipulate monthly data and do performance analysis, as opposed doing it with daily data.&lt;/li&gt;
&lt;li&gt;In some cases, investible products for some asset classes have not existed beyond a couple of decades at best. So longer backtests are often done using theoretical indices which are often available only in monthly format. Hence it forces the choice of a monthly rebalancing cycle. This implies rebalancing at the beginning of a new month.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;But should this be accepted just on the basis of a common convention? If a monthly rebalancing cycle is adopted, does it matter which day of the month is chosen for rebalance? Are there any major differences between portfolios rebalanced on the first business day of a month, as opposed to other days? Are these differences just completely random in nature?&lt;/p&gt;
&lt;p&gt;These are some of the questions that need to be answered.&lt;/p&gt;
&lt;div id=&#34;strategy&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Strategy&lt;/h3&gt;
&lt;p&gt;To answer these questions, I used one of the most basic market timing strategies:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Go long S&amp;amp;P500 when it trades above its 200 days simple moving average (SMA), otherwise exit and invest in T-Bills.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There is nothing magical about using a 200 days SMA. Other periods in the same ballpark work just as well, as shown by Meb Faber in his &lt;a href=&#34;https://papers.ssrn.com/sol3/Papers.cfm?abstract_id=962461&#34;&gt;QTAA paper&lt;/a&gt;. I used this simple strategy to illustrate the effect of rebalance timing on portfolio performance.&lt;/p&gt;
&lt;p&gt;I put together a historical daily total returns dataset of SPDR S&amp;amp;P 500 ETF (SPY), spliced together with historical daily total returns data of Vanguard 500 Index fund (VFINX), prior to Feb. 1993. This data was sourced from Yahoo! Finance, prior to their API fiasco earlier this month. Both of these products closely track the S&amp;amp;P500 Total Returns Index. This provides 37+ years of historical daily total returns data going back to the beginning of 1980.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;methodology&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Methodology&lt;/h3&gt;
&lt;p&gt;To test the effect of rebalance timing, I constructed 21 portfolio series, assuming 21 business days in a month. The first portfolio series is rebalanced on the first business day of a month, using the trading signal as of the end of the previous month. In short, signal @ t, rebalance @ t+1. If there are fewer than 21 business days in a month, say for instance 19 days, then portfolio series 20 and 21 are rebalanced on the first business day of the next month. A transaction cost of 10 bps is applied on each trade.&lt;/p&gt;
&lt;p&gt;Here are the results:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/investing/luck-in-rebalance-timing/index_files/figure-html/result1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The impact of luck could be observed in drawdowns as well as returns. The worst drawdown of series 13 is just a little more than half of series 14. Nevertheless, their annualized returns are almost the same. In all the cases, except series 13, the worst drawdown happened after the crash of 1987. Ironically, series 13 is the lucky one here ;). As Corey mentioned in his tweets, for an investment manager this is a difference between getting hired and fired.&lt;/p&gt;
&lt;p&gt;At the moment, series 1, gives the best return. As we progress to series 9, the returns decrease quite significantly. There is a difference of over 2% in their annualized returns. Here are their performance summaries:&lt;/p&gt;
&lt;table class=&#34;gmisc_table&#34; style=&#34;border-collapse: collapse; margin-top: 1em; margin-bottom: 1em;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;td colspan=&#34;3&#34; style=&#34;text-align: left;&#34;&gt;
Table 1: Best and worst rebalance series
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
Best
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
Worst
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td colspan=&#34;3&#34; style=&#34;font-weight: 900;&#34;&gt;
ALL Years
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  Jan 1981 - Apr 2017 Annualized Return
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.68%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
8.63%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  Jan 1981 - Apr 2017 Annualized Std Dev
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.09%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.87%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  Jan 1981 - Apr 2017 Annualized Sharpe (Rf=4.22%)
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.55
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.35
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: left;&#34;&gt;
  Jan 1981 - Apr 2017 Worst Drawdown
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-23.59%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-26.20%
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Let’s take a look at how the best (blue) and the worst (red) series have fared relative to the other 19 series in the entire historical period:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/investing/luck-in-rebalance-timing/index_files/figure-html/best_worst_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So, turns out the current best performed worse than the median for most of the historical period. But since 2003, it really took off and has managed to avoid many whipsaws that appear to have affected other series. But will it continue to do better? The impact of luck is quite obvious here. Quite surprisingly, the worst series has been at or near the worst throughout the historical period. Perhaps a longer backtest or one with a different investment strategy would be useful to investigate this further.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mitigating-the-impact-of-luck&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Mitigating the impact of luck&lt;/h3&gt;
&lt;p&gt;So how could the impact of luck be mitigated without resorting to more frequent rebalancing, and thus incurring higher portfolio turnover costs?&lt;/p&gt;
&lt;p&gt;Here are a couple of ideas:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Create four weekly portfolio series staggered by a week, where each series is rebalanced at the interval of 4 weeks. This way, at any given time, only ~ 25% of the capital is exposed to whipsaw risk. But a practical challenge here is to ensure that each series remains balanced relative to others. Say for instance, no single series should have a weight greater than a third of the entire portfolio. This could be managed by leveraging / deleveraging at rebalance time.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Stick with rebalancing at the beginning of a new month (t+1). But instead of taking the signal at the last day of the prior month (t), take the median (or mean) of the daily signals in the prior month. This is similar to Corey’s idea of signal smoothing with 21 days SMA of signals.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let’s see how these methods compare to the best and worst portfolios:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/investing/luck-in-rebalance-timing/index_files/figure-html/final_performance_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The three methods with overlapping portfolios and signal smoothing are denoted by &lt;em&gt;Weekly_avg, Signal_median, Signal_avg&lt;/em&gt;. We can observe that all of these methods have similar performance profiles in the historical period, with minor differences in shorter time periods. They perform much better than the &lt;em&gt;Worst&lt;/em&gt; portfolio series, in all time periods. And during most of the historical period, they have even performed better than the current &lt;em&gt;Best&lt;/em&gt; series.&lt;/p&gt;
&lt;table class=&#34;gmisc_table&#34; style=&#34;border-collapse: collapse; margin-top: 1em; margin-bottom: 1em;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;td colspan=&#34;7&#34; style=&#34;text-align: left;&#34;&gt;
Table 2: Comparison of rebalance methods
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
SP500TR
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
Best
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
Worst
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
Weekly_avg
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
Signal_median
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: right;&#34;&gt;
Signal_avg
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td colspan=&#34;7&#34; style=&#34;font-weight: 900;&#34;&gt;
ALL Years
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  Jan 1981 - Apr 2017 Annualized Return
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.07%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.68%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
8.63%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
9.99%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.19%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.26%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  Jan 1981 - Apr 2017 Annualized Std Dev
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
14.85%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.09%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.87%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.01%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.45%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.11%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  Jan 1981 - Apr 2017 Annualized Sharpe (Rf=4.22%)
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.44
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.55
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.35
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.50
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.49
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.51
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  Jan 1981 - Apr 2017 Worst Drawdown
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-50.95%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-23.59%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-26.20%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-21.42%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-26.87%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-26.87%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan=&#34;7&#34; style=&#34;font-weight: 900;&#34;&gt;
20 Years
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 1997 - Apr 2017 Annualized Return
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
7.52%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.18%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
7.58%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
8.15%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
9.07%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
9.17%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 1997 - Apr 2017 Annualized Std Dev
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
15.17%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.02%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.98%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.09%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.38%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
9.98%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 1997 - Apr 2017 Annualized Sharpe (Rf=2.04%)
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.35
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.80
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.49
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.59
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.66
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.70
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 1997 - Apr 2017 Worst Drawdown
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-50.95%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-15.28%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-17.37%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-16.18%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-15.28%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-15.28%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan=&#34;7&#34; style=&#34;font-weight: 900;&#34;&gt;
10 Years
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2007 - Apr 2017 Annualized Return
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
7.00%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.02%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
6.93%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
7.65%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
7.50%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
8.40%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2007 - Apr 2017 Annualized Std Dev
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
15.25%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
9.18%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.62%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
9.86%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
9.74%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
9.41%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2007 - Apr 2017 Annualized Sharpe (Rf=0.52%)
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.42
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.03
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.60
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.72
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.71
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
0.83
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2007 - Apr 2017 Worst Drawdown
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-50.95%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-10.06%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-17.37%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-16.18%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-14.77%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
-12.00%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan=&#34;7&#34; style=&#34;font-weight: 900;&#34;&gt;
5 Years
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2012 - Apr 2017 Annualized Return
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
13.36%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
11.91%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.08%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.84%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.12%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.58%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2012 - Apr 2017 Annualized Std Dev
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
10.19%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
8.77%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
9.55%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
8.82%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
9.29%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
8.98%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;
  May 2012 - Apr 2017 Annualized Sharpe (Rf=0.15%)
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.29
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.34
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.04
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.21
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.07
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; text-align: right;&#34;&gt;
1.16
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: left;&#34;&gt;
  May 2012 - Apr 2017 Worst Drawdown
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-8.36%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-7.64%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-14.78%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-9.88%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-14.77%
&lt;/td&gt;
&lt;td style=&#34;padding-left: .5em; padding-right: .5em; border-bottom: 2px solid grey; text-align: right;&#34;&gt;
-12.00%
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As we can see, relatively simple solutions work well to mitigate the impact of luck in rebalance timing.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;All content displayed here is for informational purposes only and is not guaranteed to be accurate, complete or up-to-date. Nothing herein should be considered investment advice or recommendation to buy, sell or hold any securities.&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Building a Predictive Typing Model</title>
      <link>/post/building-a-predictive-typing-model/</link>
      <pubDate>Sat, 15 Apr 2017 00:00:00 +0000</pubDate>
      <guid>/post/building-a-predictive-typing-model/</guid>
      <description>


&lt;p&gt;Everyone using a smartphone or a mobile device has used an onscreen smart keyboard that tries to predict the next set of words that the user might want to type. Typically, upto 3 words are predicted, which are displayed in a row at the top of the keyboard. Given that typing on a glass pane without tactile feedback, could be very frustrating at times, the smart keyboard goes a long way in alleviating these issues.&lt;/p&gt;
&lt;p&gt;But, how does predictive typing work? Basically, it’s a predictive typing model built using an ngram with &lt;a href=&#34;https://en.wikipedia.org/wiki/Katz%27s_back-off_model&#34;&gt;backoff prediction model&lt;/a&gt;. Makes complete sense, right? Well, it’s less complicated than it sounds, and actually not hard to understand, as we shall see further.&lt;/p&gt;
&lt;div id=&#34;corpus&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Corpus&lt;/h2&gt;
&lt;p&gt;To begin, we need a large corpus of text with a wide-ranging vocabulary. The first name that comes to mind is that of William Shakespeare. Shakespeare wrote a number of plays and poems that are classics in the English language. These plays and poems are available in the public domain and are easily accessible through a number of sources, (see &lt;a href=&#34;http://www.gutenberg.org/browse/authors/s#a65&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://shakespeare.mit.edu/&#34;&gt;here&lt;/a&gt;).
By some &lt;a href=&#34;http://www1.cmc.edu/pages/faculty/welliott/Shakespeare%20Vocabulary%20Chapter%20911.pdf&#34;&gt;accounts&lt;/a&gt;, Shakespeare’s vocabulary ranged from 15,000-30,000 words, which makes it excellent to use it for our corpus.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ropenscilabs/gutenbergr&#34;&gt;David Robinson’s&lt;/a&gt; &lt;code&gt;gutenbergr&lt;/code&gt; package makes it easy to download ‘The Complete Works Of Shakespeare’, from Project Gutenberg’s website. The downloaded dataset is a data frame with text appearing in rows along with the gutenberg id.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dfRaw &amp;lt;- gutenberg_download(100)
knitr::kable(head(dfRaw, 12))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;gutenberg_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;text&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Shakespeare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;em&gt;This Etext has certain copyright implications you should read!&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&amp;lt;&amp;lt;THIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;SHAKESPEARE IS COPYRIGHT 1990-1993 BY WORLD LIBRARY, INC., AND IS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PROVIDED BY PROJECT GUTENBERG ETEXT OF ILLINOIS BENEDICTINE COLLEGE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;WITH PERMISSION. ELECTRONIC AND MACHINE READABLE COPIES MAY BE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;DISTRIBUTED SO LONG AS SUCH COPIES (1) ARE FOR YOUR OR OTHERS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PERSONAL USE ONLY, AND (2) ARE NOT DISTRIBUTED OR USED&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;COMMERCIALLY. PROHIBITED COMMERCIAL DISTRIBUTION INCLUDES BY ANY&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;SERVICE THAT CHARGES FOR DOWNLOAD TIME OR FOR MEMBERSHIP.&amp;gt;&amp;gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(dfRaw[sample(nrow(dfRaw), 6),])&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;gutenberg_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;text&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Assure yourselves, will never be unkind.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Like one besotted on your sweet delights.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;And spurn in pieces posts of adamant;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Whereto, when they shall know what men are rich,&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TITUS. Marcus, even thou hast struck upon my crest,&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;data-preparation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Preparation&lt;/h2&gt;
&lt;p&gt;The copyright notice appears multiple times throughout the text. Fortunately, it doesn’t prevent us from using this data for non-commercial purposes. The first step is to merge the entire text and remove the copyright notices that are helpfully within &amp;lt;&amp;lt; … &amp;gt;&amp;gt;. Secondly, in plays, the characters and their relationships and titles are defined with ‘DRAMATIS PERSONAE’. These lines have to be removed too.&lt;/p&gt;
&lt;p&gt;After doing some quick cleaning, the text is reconverted to a data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;complete_works &amp;lt;- str_c(dfRaw$text, collapse = &amp;quot;.&amp;quot;)
complete_works &amp;lt;- str_split(complete_works, &amp;quot;&amp;lt;&amp;lt;[^&amp;gt;]*&amp;gt;&amp;gt;&amp;quot;)[[1]]
complete_works &amp;lt;- complete_works[!(str_detect(complete_works, &amp;quot;Dramatis Personae|DRAMATIS PERSONAE&amp;quot;))]
complete_works &amp;lt;- str_replace_all(complete_works, &amp;quot;\\[|\\]&amp;quot;, &amp;quot;.&amp;quot;)

dfContent &amp;lt;- as_tibble(complete_works) %&amp;gt;% slice(3:n())
rm(complete_works)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we parse the content into sentences. The &lt;code&gt;tidytext&lt;/code&gt; package by &lt;a href=&#34;http://varianceexplained.org/&#34;&gt;David Robinson&lt;/a&gt; and &lt;a href=&#34;http://juliasilge.com/&#34;&gt;Julia Silge&lt;/a&gt;, is most useful in doing this within the tidyverse framework.&lt;/p&gt;
&lt;p&gt;Here’s what the function does below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Replaces all ‘logical’ sentence break delimiters by a period for easier tokenization by sentence&lt;/li&gt;
&lt;li&gt;Removes extra periods and spaces&lt;/li&gt;
&lt;li&gt;Tokenizes into sentences&lt;/li&gt;
&lt;li&gt;Removes all sentences containing ACT or SCENE&lt;/li&gt;
&lt;li&gt;Removes all digits from sentences&lt;/li&gt;
&lt;li&gt;Finally adds line numbers to each row (more on the importance of this later…)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A sample of 10 sentences is shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;getSentences &amp;lt;- function(dfContent) {
    df &amp;lt;- dfContent %&amp;gt;% 
        mutate(value = str_replace_all(value, &amp;quot;;|:|!|\\?| \\-|\\- | \\- &amp;quot;, &amp;quot;\\. &amp;quot;)) %&amp;gt;% 
        mutate(value = str_replace_all(value, &amp;quot;[\\.]+&amp;quot;, &amp;quot;\\.&amp;quot;)) %&amp;gt;% 
        mutate(value = str_replace_all(value, &amp;quot;[[:space:]]+&amp;quot;, &amp;quot; &amp;quot;)) %&amp;gt;% 
        unnest_tokens(sentence, value, token = &amp;quot;sentences&amp;quot;, to_lower = FALSE) %&amp;gt;% 
        dplyr::filter(!str_detect(sentence, &amp;quot;Act|ACT|Scene|SCENE&amp;quot;)) %&amp;gt;% 
        mutate(sentence = str_replace_all(sentence, &amp;quot;[0-9]&amp;quot;,&amp;quot;&amp;quot;)) %&amp;gt;% 
        mutate(lineNumber = row_number()) %&amp;gt;% 
        select(lineNumber, everything())
}

dfSentences &amp;lt;- getSentences(dfContent)
knitr::kable(dfSentences[sample(nrow(dfSentences), 10),])&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;lineNumber&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;sentence&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;55991&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;That with the King here resteth in his tent. .&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;16097&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;You are a merry man, sir. fare you well.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;9717&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;He did ask favour.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100749&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;To the health of our general. .&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;40908&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;You are, I think, assur’d I love you not.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;117828&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Laurence. and another Watchman.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;19243&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Ere now denied the asker, and now again,.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;145024&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;I swear to do this, though a present death.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;70942&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Stocking his messenger.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;122957&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;For bringing wood in slowly.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-ngrams&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating ngrams&lt;/h2&gt;
&lt;p&gt;“ngrams are a contiguous sequence of n items from a given sequence of text…” - &lt;a href=&#34;https://en.wikipedia.org/wiki/N-gram&#34;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this case, ngrams would be a contiguous sequence of n words. In the simplest form, an ngram of size 1 is a single word.
An ngram of size 2 or more is a set of words that appear one after the other in a sequence.&lt;/p&gt;
&lt;p&gt;For instance, given a sentence - &lt;strong&gt;how are you doing?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are 4 Unigrams: how, are, you, doing&lt;/li&gt;
&lt;li&gt;There are 3 Bigrams: how are, are you, you doing&lt;/li&gt;
&lt;li&gt;There are 2 Trigrams: how are you, are you doing&lt;/li&gt;
&lt;li&gt;There is 1 Quadrigram: how are you doing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, using the data frame of clean sentences that we parsed, the next step is to create ngrams.
Once again this is made simple using the &lt;strong&gt;unnest_tokens&lt;/strong&gt; function from the &lt;code&gt;tidytext&lt;/code&gt; package. And this is where grouping by line numbers is important because we want ngrams to be created by sequence of words, only within the same line. We do not want ngrams connected by a sentence break.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;getNgrams &amp;lt;- function(dfSentences, n) {
    dfNgrams &amp;lt;- dfSentences %&amp;gt;% 
        group_by(lineNumber) %&amp;gt;% 
        unnest_tokens(word, sentence, token = &amp;quot;ngrams&amp;quot;, n = n) %&amp;gt;% 
        ungroup() %&amp;gt;% 
        count(word, sort = TRUE) %&amp;gt;% 
        rename(freq = n)
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;all-words&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;All Words&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1grams &amp;lt;- getNgrams(dfSentences, 1)
dim(df1grams)
[1] 26453     2
wordsOfFreq1 &amp;lt;- df1grams %&amp;gt;% group_by(freq) %&amp;gt;% summarise(n = sum(freq)) %&amp;gt;% dplyr::filter(freq == 1) %&amp;gt;% .$n&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are 26453 words in the corpus, that shows how vast Shakespeare’s vocabulary was. Let’s see a wordcloud of top 100 words. No surprise, the most commonly used words are: &lt;strong&gt;&lt;em&gt;the, and, i, to, of&lt;/em&gt;&lt;/strong&gt;. Excluding these commonly used ‘stop words’, in the second wordcloud, we can see the words Shakespeare used most frequently. We can see a lot of references to royalty and nobility, present specially in Shakespearean tragedies. Some names are mentioned quite frequently - Richard, John, Caesar, Brutus and Henry. Some English counties or &lt;a href=&#34;https://en.wikipedia.org/wiki/Shire&#34;&gt;shires&lt;/a&gt; are mentioned quite often - Warwick, York and Gloucester.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/building-a-predictive-typing-model/index_files/figure-html/wordcloud-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;unigrams&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Unigrams&lt;/h3&gt;
&lt;p&gt;Let’s see a plot of the distribution of words in corpus by usage.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/building-a-predictive-typing-model/index_files/figure-html/plot_freq-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here’s a strategy for building ngram tables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is a long tail of 10522 words in the corpus that appear exactly once. Replace all words appearing just once, by &lt;strong&gt;UNK&lt;/strong&gt; as a placeholder for a word not known from the corpus. There are a couple of motivations behind this:
&lt;ul&gt;
&lt;li&gt;It reduces the size of ngram tables.&lt;/li&gt;
&lt;li&gt;More importantly, when an unknown word is encountered, the model knows how to tackle it before predicting the next set of words.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Re-summarise and rearrange the table by frequency.&lt;/li&gt;
&lt;li&gt;Assign an index to each word.&lt;/li&gt;
&lt;li&gt;Convert to data.table format. Some of the biggest benefits of this approach are:
&lt;ul&gt;
&lt;li&gt;Rows could be assigned and referenced by a key. This makes it extremely simple to look up a row value.&lt;/li&gt;
&lt;li&gt;Lookup by a key is very fast compared to matching character strings.&lt;/li&gt;
&lt;li&gt;When we create tables for bigrams and higher order ngrams, we can replace the words by their corresponding integer indexes. It saves a lot of memory as compared to creating tibbles with character strings.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vTopUnigrams &amp;lt;- df1grams %&amp;gt;% dplyr::filter(freq &amp;gt;= 2) %&amp;gt;% .$word
df1grams &amp;lt;- df1grams %&amp;gt;% 
    mutate(word = ifelse(word %in% vTopUnigrams, word, &amp;quot;UNK&amp;quot;)) %&amp;gt;% 
    group_by(word) %&amp;gt;% 
    summarise(freq = sum(freq)) %&amp;gt;% 
    arrange(desc(freq)) %&amp;gt;% 
    mutate(index1 = row_number()) %&amp;gt;% 
    select(index1, freq, word)

dt1grams &amp;lt;- as.data.table(df1grams)
# set key to word
dt1grams &amp;lt;- dt1grams[,.(index1,freq),key=word]
# create another table with key=index1
dt1gramsByIndex &amp;lt;- dt1grams[,.(word,freq),key=index1]
# print dimensions
dim(dt1grams)
[1] 15932     3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the top 10 unigrams. &lt;strong&gt;UNK&lt;/strong&gt; replaces the long tail of words appearing once.
Finally, we are left with 15932 unigrams in our table.&lt;/p&gt;
&lt;center&gt;
&lt;!-- html table generated in R 3.6.3 by xtable 1.8-4 package --&gt;
&lt;!-- Thu Jun 11 14:58:51 2020 --&gt;
&lt;table border=&#34;1&#34;&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
index1
&lt;/th&gt;
&lt;th&gt;
word
&lt;/th&gt;
&lt;th&gt;
freq
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
the
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
26821
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
2
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
and
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
25670
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
3
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
3
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
20473
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
to
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
19377
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
of
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
16954
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
6
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
6
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
a
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
14351
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
7
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
7
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
you
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
13568
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
my
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
12449
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
9
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
9
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
in
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
10881
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
10
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
10
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
that
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
10869
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bigrams&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bigrams&lt;/h3&gt;
&lt;p&gt;A data table of bigrams is created where the index of the first word is the lookup key. The second word cannot be &lt;strong&gt;UNK&lt;/strong&gt; since it is a word used for making predictions from the bigram table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df2grams &amp;lt;- getNgrams(dfSentences, 2) %&amp;gt;% 
    separate(word, c(&amp;quot;word1&amp;quot;, &amp;quot;word2&amp;quot;), sep = &amp;quot; &amp;quot;) %&amp;gt;% 
    mutate(word1 = ifelse(word1 %in% vTopUnigrams, word1, &amp;quot;UNK&amp;quot;)) %&amp;gt;% 
    dplyr::filter(word2 %in% vTopUnigrams) %&amp;gt;% 
    group_by(word1, word2) %&amp;gt;% 
    summarise(freq = sum(freq)) %&amp;gt;% 
    arrange(desc(freq))

dt2grams &amp;lt;- as.data.table(df2grams)
dt2grams$index1 &amp;lt;- dt1grams[dt2grams$word1]$index1
dt2grams$index2 &amp;lt;- dt1grams[dt2grams$word2]$index1
dt2grams &amp;lt;- dt2grams[,.(index1,index2,freq)]
setkey(dt2grams, index1)

# print dimensions
dim(dt2grams)
[1] 254884      3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the top 10 bigrams:&lt;/p&gt;
&lt;center&gt;
&lt;!-- html table generated in R 3.6.3 by xtable 1.8-4 package --&gt;
&lt;!-- Thu Jun 11 14:59:03 2020 --&gt;
&lt;table border=&#34;1&#34;&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
word1
&lt;/th&gt;
&lt;th&gt;
word2
&lt;/th&gt;
&lt;th&gt;
freq
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
30512
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
2
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
am
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1846
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
3
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
my
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
lord
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1640
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
in
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
the
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1611
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
have
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1603
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
6
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
will
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1553
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
7
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
of
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
the
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1417
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
to
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
the
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1380
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
9
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
it
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
is
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1066
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
10
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
to
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
be
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
973
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trigrams&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Trigrams&lt;/h3&gt;
&lt;p&gt;Similarly, a data table of trigrams is created where indexes of the first and second words form the lookup key.
The third word cannot be &lt;strong&gt;UNK&lt;/strong&gt; since it is a word used for making predictions from the trigram table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df3grams &amp;lt;- getNgrams(dfSentences, 3) %&amp;gt;% 
    separate(word, c(&amp;quot;word1&amp;quot;, &amp;quot;word2&amp;quot;, &amp;quot;word3&amp;quot;), sep = &amp;quot; &amp;quot;) %&amp;gt;% 
    mutate(word1 = ifelse(word1 %in% vTopUnigrams, word1, &amp;quot;UNK&amp;quot;)) %&amp;gt;% 
    mutate(word2 = ifelse(word2 %in% vTopUnigrams, word2, &amp;quot;UNK&amp;quot;)) %&amp;gt;% 
    dplyr::filter(word3 %in% vTopUnigrams) %&amp;gt;% 
    group_by(word1, word2, word3) %&amp;gt;% 
    summarise(freq = sum(freq)) %&amp;gt;% 
    arrange(desc(freq))

dt3grams &amp;lt;- as.data.table(df3grams)
dt3grams$index1 &amp;lt;- dt1grams[dt3grams$word1]$index1
dt3grams$index2 &amp;lt;- dt1grams[dt3grams$word2]$index1
dt3grams$index3 &amp;lt;- dt1grams[dt3grams$word3]$index1
dt3grams &amp;lt;- dt3grams[,.(index1,index2,index3,freq)]
setkey(dt3grams, index1,index2)

# print dimensions
dim(dt3grams)
[1] 476098      4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the top 10 trigrams, we can get an idea how &lt;strong&gt;UNK&lt;/strong&gt; will be useful in this context. &lt;strong&gt;&lt;em&gt;the UNK of&lt;/em&gt;&lt;/strong&gt; is one of the most frequently used word sequences in the trigram table, which implies that if we encounter an unknown word after ‘the’, then ‘of’ is the most likely word to be predicted from the trigram table.&lt;/p&gt;
&lt;center&gt;
&lt;!-- html table generated in R 3.6.3 by xtable 1.8-4 package --&gt;
&lt;!-- Thu Jun 11 14:59:27 2020 --&gt;
&lt;table border=&#34;1&#34;&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
word1
&lt;/th&gt;
&lt;th&gt;
word2
&lt;/th&gt;
&lt;th&gt;
word3
&lt;/th&gt;
&lt;th&gt;
freq
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
38526
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
2
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
pray
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
you
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
238
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
3
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
will
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
not
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
213
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
know
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
not
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
159
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
do
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
not
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
157
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
6
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
the
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
UNK
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
of
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
155
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
7
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
the
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
duke
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
of
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
141
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
am
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
not
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
139
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
9
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
am
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
a
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
138
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
10
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
would
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
not
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
127
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quadrigrams&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Quadrigrams&lt;/h3&gt;
&lt;p&gt;Here are the top 10 quadrigrams:&lt;/p&gt;
&lt;center&gt;
&lt;!-- html table generated in R 3.6.3 by xtable 1.8-4 package --&gt;
&lt;!-- Thu Jun 11 14:59:54 2020 --&gt;
&lt;table border=&#34;1&#34;&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
word1
&lt;/th&gt;
&lt;th&gt;
word2
&lt;/th&gt;
&lt;th&gt;
word3
&lt;/th&gt;
&lt;th&gt;
word4
&lt;/th&gt;
&lt;th&gt;
freq
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
47569
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
2
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
with
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
all
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
my
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
heart
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
46
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
3
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
know
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
not
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
what
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
38
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
give
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
me
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
your
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
hand
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
31
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
do
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
beseech
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
you
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
31
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
6
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
give
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
me
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
thy
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
hand
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
30
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
7
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
the
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
duke
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
of
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
york
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
do
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
not
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
know
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
28
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
9
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
would
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
not
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
have
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
27
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
10
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
ay
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
my
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
good
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
lord
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
25
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pentagrams&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Pentagrams&lt;/h3&gt;
&lt;p&gt;Here are the top 10 pentagrams:&lt;/p&gt;
&lt;center&gt;
&lt;!-- html table generated in R 3.6.3 by xtable 1.8-4 package --&gt;
&lt;!-- Thu Jun 11 15:00:17 2020 --&gt;
&lt;table border=&#34;1&#34;&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
word1
&lt;/th&gt;
&lt;th&gt;
word2
&lt;/th&gt;
&lt;th&gt;
word3
&lt;/th&gt;
&lt;th&gt;
word4
&lt;/th&gt;
&lt;th&gt;
word5
&lt;/th&gt;
&lt;th&gt;
freq
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
60534
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
2
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
am
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
glad
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
to
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
see
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
15
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
3
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
thank
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
you
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
for
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
your
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
had
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
rather
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
be
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
a
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
9
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
am
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
glad
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
to
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
see
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
you
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
6
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
as
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
am
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
a
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
gentleman
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
7
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
for
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
mine
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
own
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
part
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
pray
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
you
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
tell
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
me
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
9
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
know
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
not
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
what
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
to
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
say
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
10
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
and
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
so
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
i
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
take
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
my
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;These ngram tables comprise all the information needed for the prediction model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;memory-usage-comparison&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Memory Usage Comparison&lt;/h3&gt;
&lt;p&gt;As mentioned above, storing words as integer indexes in a data table is far more efficient as compared to a data frame with character strings.
For instance, here’s how a pentagram data table looks like that contains just the integer indexes of the words:&lt;/p&gt;
&lt;center&gt;
&lt;!-- html table generated in R 3.6.3 by xtable 1.8-4 package --&gt;
&lt;!-- Thu Jun 11 15:00:17 2020 --&gt;
&lt;table border=&#34;1&#34;&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
index1
&lt;/th&gt;
&lt;th&gt;
index2
&lt;/th&gt;
&lt;th&gt;
index3
&lt;/th&gt;
&lt;th&gt;
index4
&lt;/th&gt;
&lt;th&gt;
index5
&lt;/th&gt;
&lt;th&gt;
freq
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
9049
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
2
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
9049
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
3
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
9384
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
12918
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
4
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
687
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
31
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
4758
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
5
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
31
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
6421
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
29
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
6
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
9
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
183
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
2
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
7
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
11
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
46
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
15
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
8
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
11
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
57
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
16
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
9
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
11
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1267
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
11402
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
10
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
11
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1403
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
10842
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;In a comparison of memory needed for all ngram tables, we see a difference of a factor of 5 between the two approaches.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(object.size(df1grams)+object.size(df2grams)+object.size(df3grams)+
          object.size(df4grams)+object.size(df5grams), units = &amp;quot;auto&amp;quot;)
158 Mb

print(object.size(dt1grams)+object.size(dt1gramsByIndex)+object.size(dt2grams)+object.size(dt3grams)+
          object.size(dt4grams)+object.size(dt5grams), units = &amp;quot;auto&amp;quot;)
30.4 Mb&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ngram-with-backoff-prediction-algorithm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ngram With Backoff Prediction Algorithm&lt;/h2&gt;
&lt;p&gt;So, given the ngram data tables, here’s how the prediction algorithm works:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sanitize the input text as done above&lt;/li&gt;
&lt;li&gt;Parse the text into sentences&lt;/li&gt;
&lt;li&gt;Determine the words in the last sentence of the input text. Use upto the last 4 words from the input text.&lt;/li&gt;
&lt;li&gt;The word predictions are done starting from the ngram table containing the longest sequence of words, provided the input text contains atleast n-1 words. So, if the last 4 words from the input text match the first 4 words in the pentagram table, then the fifth word is chosen, that has the highest frequency of occurance after those 4 words. If more than one word prediction is desired, then the fifth words with the next highest frequency of occurance are also chosen.&lt;/li&gt;
&lt;li&gt;If no matches or fewer than desired number of matches are found in the pentagram table, then the algorithm backoffs to the quadrigram table using the last 3 words typed, then to the trigram table using the last 2 words typed, then to the bigram table using the last word typed and finally to the unigram table, until the desired number of word predictions are returned.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s see how this works concretely with some test cases:&lt;/p&gt;
&lt;div id=&#34;test-cases&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Test Cases&lt;/h3&gt;
&lt;p&gt;The first test is to confirm whether the word predictions from the unigram table, match with what we expect.
The top 10 unigram table contains 3 words beginning with the letter ‘t’ - &lt;code&gt;the&lt;/code&gt;(1), &lt;code&gt;to&lt;/code&gt;(4), &lt;code&gt;that&lt;/code&gt;(10). And sure enough, this is what we get from our prediction function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictNextWords(&amp;quot;t&amp;quot;)
[1] &amp;quot;Predictions from 1grams: the,to,that,this,thou&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: the,to,that&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the bigram table, we could see that &lt;code&gt;i am&lt;/code&gt;, &lt;code&gt;i have&lt;/code&gt; and &lt;code&gt;i will&lt;/code&gt; are among the top 10. So, the next test is to confirm these.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictNextWords(&amp;quot;i &amp;quot;)
[1] &amp;quot;Predictions from 2grams: am,have,will,do,would&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: am,have,will&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Narrowing down to words that begin with letter ‘h’ gives us:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictNextWords(&amp;quot;i h&amp;quot;)
[1] &amp;quot;Predictions from 2grams: have,had,hope,hear,heard&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: have,had,hope&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s predict some trigrams. &lt;code&gt;i am not&lt;/code&gt; and &lt;code&gt;i am a&lt;/code&gt; are among the top 10 trigrams, so that’s what we expect to see.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictNextWords(&amp;quot;i am &amp;quot;)
[1] &amp;quot;Predictions from 3grams: not,a,sure,glad,the&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: not,a,sure&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s examine the &lt;strong&gt;UNK&lt;/strong&gt; feature. We know that &lt;code&gt;the UNK of&lt;/code&gt; is number 5 most common sequence in the trigram table so we should expect to see ‘of’ as the first prediction if the input text has an unknown word after ‘the’. So let’s input some gibberish after ‘the’ to make sure this word doesn’t exist in our vocabulary:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictNextWords(&amp;quot;the sjdhsbhbfh &amp;quot;)
[1] &amp;quot;Predictions from 3grams: of,and,in,that,the&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: of,and,in&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that the &lt;em&gt;known&lt;/em&gt; test cases are working well, let’s see the backoff algorithm in action:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictNextWords(&amp;quot;to be or not &amp;quot;)
[1] &amp;quot;Predictions from 5grams: to&amp;quot;
[1] &amp;quot;Predictions from 4grams: to&amp;quot;
[1] &amp;quot;Predictions from 3grams: to,at,i,allow&amp;#39;d,arriv&amp;#39;d&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: to,at,i&amp;quot;
predictNextWords(&amp;quot;to be or not to &amp;quot;)
[1] &amp;quot;Predictions from 5grams: be&amp;quot;
[1] &amp;quot;Predictions from 4grams: be,crack&amp;quot;
[1] &amp;quot;Predictions from 3grams: be,crack,the,have,me,do&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: be,crack,the&amp;quot;
predictNextWords(&amp;quot;be or not to &amp;quot;)
[1] &amp;quot;Predictions from 5grams: be&amp;quot;
[1] &amp;quot;Predictions from 4grams: be,crack&amp;quot;
[1] &amp;quot;Predictions from 3grams: be,crack,the,have,me,do&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: be,crack,the&amp;quot;
predictNextWords(&amp;quot;to be or not to be &amp;quot;)
[1] &amp;quot;Predictions from 5grams: that&amp;quot;
[1] &amp;quot;Predictions from 4grams: that,found,gone,endur&amp;#39;d,endured,seen&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: that,found,gone&amp;quot;
predictNextWords(&amp;quot;to be or not to be, that &amp;quot;)
[1] &amp;quot;Predictions from 5grams: is&amp;quot;
[1] &amp;quot;Predictions from 4grams: is,which&amp;quot;
[1] &amp;quot;Predictions from 3grams: is,which,you,he,i,thou,can&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: is,which,you&amp;quot;
predictNextWords(&amp;quot;to be or not to be, that is &amp;quot;)
[1] &amp;quot;Predictions from 5grams: the&amp;quot;
[1] &amp;quot;Predictions from 4grams: the&amp;quot;
[1] &amp;quot;Predictions from 3grams: the,not,my,to,a&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: the,not,my&amp;quot;
predictNextWords(&amp;quot;to be or not to be, that is the &amp;quot;)
[1] &amp;quot;Predictions from 5grams: question&amp;quot;
[1] &amp;quot;Predictions from 4grams: question,very,way,humour,best,brief&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: question,very,way&amp;quot;
predictNextWords(&amp;quot;be that is the &amp;quot;)
[1] &amp;quot;Predictions from 5grams: question&amp;quot;
[1] &amp;quot;Predictions from 4grams: question,very,way,humour,best,brief&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: question,very,way&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using one of the most well known speeches from Hamlet, we can see that the algorithm gets the first word prediction from the pentagram table. But since we are looking for the next 3 words, the algorithm backoffs to predictions from quadrigram and trigram tables, until it has atleast 3 words. In this case, the pentagram table gives us the best match that we are looking for. We can also observe that once we have atleast 4 words in our input text, the words prior to those do not matter for the next word predictions.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Let’s see another example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictNextWords(&amp;quot;What a lovely day &amp;quot;)
[1] &amp;quot;Predictions from 5grams: &amp;quot;
[1] &amp;quot;Predictions from 4grams: &amp;quot;
[1] &amp;quot;Predictions from 3grams: &amp;quot;
[1] &amp;quot;Predictions from 2grams: and,to,is,of,i&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: and,to,is&amp;quot;
predictNextWords(&amp;quot;a lovely day &amp;quot;)
[1] &amp;quot;Predictions from 4grams: &amp;quot;
[1] &amp;quot;Predictions from 3grams: &amp;quot;
[1] &amp;quot;Predictions from 2grams: and,to,is,of,i&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: and,to,is&amp;quot;
predictNextWords(&amp;quot;lovely day &amp;quot;)
[1] &amp;quot;Predictions from 3grams: &amp;quot;
[1] &amp;quot;Predictions from 2grams: and,to,is,of,i&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: and,to,is&amp;quot;
predictNextWords(&amp;quot;day &amp;quot;)
[1] &amp;quot;Predictions from 2grams: and,to,is,of,i&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: and,to,is&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we see that even though there are enough words to start matching from the higher ngram tables, the matches are gotten only from the bigram table using ‘day’ as the first word.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Another one:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictNextWords(&amp;quot;Beware the ides of &amp;quot;)
[1] &amp;quot;Predictions from 5grams: march&amp;quot;
[1] &amp;quot;Predictions from 4grams: march&amp;quot;
[1] &amp;quot;Predictions from 3grams: march&amp;quot;
[1] &amp;quot;Predictions from 2grams: march,the,my,his,a,this&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: march,the,my&amp;quot;
predictNextWords(&amp;quot;Beware the ides of mar&amp;quot;)
[1] &amp;quot;Predictions from 5grams: march&amp;quot;
[1] &amp;quot;Predictions from 4grams: march&amp;quot;
[1] &amp;quot;Predictions from 3grams: march&amp;quot;
[1] &amp;quot;Predictions from 2grams: march,marriage,marcius,marcus,mars&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: march,marriage,marcius&amp;quot;
predictNextWords(&amp;quot;Beware the ides of march&amp;quot;)
[1] &amp;quot;Predictions from 5grams: march&amp;quot;
[1] &amp;quot;Predictions from 4grams: march&amp;quot;
[1] &amp;quot;Predictions from 3grams: march&amp;quot;
[1] &amp;quot;Predictions from 2grams: march,marching&amp;quot;
[1] &amp;quot;Predictions from 1grams: march,marching,march&amp;#39;d,marches,marchioness&amp;quot;
[1] &amp;quot;Top 3 Final Predictions: march,marching,march&amp;#39;d&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;shiny-app&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Shiny App&lt;/h2&gt;
&lt;p&gt;Finally, here’s a &lt;a href=&#34;https://nitingupta2.shinyapps.io/ptmapp&#34;&gt;Shiny app&lt;/a&gt; that demos this predictive typing model.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;The R markdown file for this post is available &lt;a href=&#34;https://github.com/nitingupta2/nitingupta2.github.io/blob/master/post/building-a-predictive-typing-model/&#34;&gt;here&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Ames Housing - Part 2 - Building Models</title>
      <link>/casestudies/ames-housing-part2-models/</link>
      <pubDate>Mon, 26 Dec 2016 00:00:00 +0000</pubDate>
      <guid>/casestudies/ames-housing-part2-models/</guid>
      <description>


&lt;p&gt;In a &lt;a href=&#34;https://www.nitingupta.com/casestudies/ames-housing-part1-eda/&#34;&gt;previous post&lt;/a&gt; in this series, we did an exploratory data analysis of the &lt;a href=&#34;http://www.amstat.org/publications/jse/v19n3/decock.pdf&#34;&gt;Ames Housing dataset&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this post, we will build linear and non-linear models and see how well they predict the &lt;code&gt;SalePrice&lt;/code&gt; of properties.&lt;/p&gt;
&lt;div id=&#34;evaluation-criteria&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Evaluation Criteria&lt;/h2&gt;
&lt;p&gt;Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed &lt;code&gt;SalePrice&lt;/code&gt; will be our evaluation criteria. Taking the log ensures that errors in predicting expensive and cheap houses will affect the result equally.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;steps-for-building-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Steps for Building Models&lt;/h2&gt;
&lt;p&gt;Here are the steps for building models and determining the best hyperparameter combinations by K-fold cross validation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Partition the training dataset into model training and validation sets. Use stratified sampling such that each partition has a similar distribution of the target variable - &lt;code&gt;SalePrice&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Define linear and non-linear models.&lt;/li&gt;
&lt;li&gt;For each model, create a grid of hyperparameter combinations that are equally spaced.&lt;/li&gt;
&lt;li&gt;For each hyperparameter combination, fit a model on the training set and make predictions on the validation set. Repeat the process for all folds.&lt;/li&gt;
&lt;li&gt;Determine root mean squared errors (RMSE) and choose the best hyperparameter combination that corresponds to the minimum RMSE.&lt;/li&gt;
&lt;li&gt;Train each model with its best hyperparameter combination on the entire training set.&lt;/li&gt;
&lt;li&gt;Calculate RMSE of the each finalized model on the testing set.&lt;/li&gt;
&lt;li&gt;Finally, choose the best model that gives the least RMSE.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;partitioning-training-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Partitioning Training Data&lt;/h2&gt;
&lt;p&gt;We split the training data into 4 folds. Within each fold, 75% of the data is used for training models and 25% for validating the predicted values against the actual values.&lt;/p&gt;
&lt;p&gt;Let’s look at the distribution of the target variable across all folds:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part2-models/index_files/figure-html/plot_target_partitioning-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;By using stratified sampling, we ensure that the training and validation distributions of the target variable are similar.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear Models&lt;/h2&gt;
&lt;div id=&#34;ordinary-least-squares-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ordinary Least Squares Regression&lt;/h3&gt;
&lt;p&gt;Before creating any new features or indulging in more complex modelling methods, we will cross validate a simple linear model on the training data to establish a benchmark. If more complex approaches do not have a significant improvement in the model validation metrics, then they are not worthwhile to be pursued.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Linear Regression Model Specification (regression)

Computational engine: lm &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;whats-notable&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What’s notable?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;After training a linear model on all predictors, we get an RMSE of &lt;strong&gt;0.1468&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;This is the simplest and fastest model with no hyperparameters to tune.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;regularized-linear-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regularized Linear Model&lt;/h3&gt;
&lt;p&gt;We will use &lt;code&gt;glmnet&lt;/code&gt; that uses LASSO and Ridge Regression with regularization. We will do a grid search of the following hyperparameters that minimize RMSE:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;penalty&lt;/code&gt;: The total amount of regularization in the model.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mixture&lt;/code&gt;: The proportion of L1 regularization in the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;Linear Regression Model Specification (regression)

Main Arguments:
  penalty = tune()
  mixture = tune()

Computational engine: glmnet &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the top 10 RMSE values and hyperparameter combinations:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 10 x 3
    penalty mixture mean_rmse
      &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
 1 4.83e- 3  0.922      0.127
 2 3.79e- 2  0.0518     0.129
 3 1.36e- 3  0.659      0.132
 4 1.60e- 3  0.431      0.133
 5 3.50e- 3  0.177      0.133
 6 4.17e- 2  0.288      0.133
 7 5.67e- 4  0.970      0.133
 8 6.79e- 9  0.0193     0.138
 9 4.32e-10  0.337      0.138
10 1.95e- 6  0.991      0.138&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;whats-notable-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What’s notable?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;After hyperparameter tuning with cross validation, &lt;code&gt;glmnet&lt;/code&gt; gives the best RMSE of 0.127 with penalty = 0.0048 and mixture = 0.9216.&lt;/li&gt;
&lt;li&gt;It is a significant improvement over Ordinary Least Squares regression that had an RMSE of 0.1468.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;glmnet&lt;/code&gt; cross validation takes under a minute to execute.&lt;/li&gt;
&lt;li&gt;But the presence of outliers can significantly affect its performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here a plot of the &lt;code&gt;glmnet&lt;/code&gt; hyperparameter grid along with the best hyperparameter combination:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part2-models/index_files/figure-html/plot_glmnet-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;non-linear-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Non-linear Models&lt;/h2&gt;
&lt;p&gt;Next, we will train a couple of tree-based algorithms, which are not very sensitive to outliers and skewed data.&lt;/p&gt;
&lt;div id=&#34;randomforest&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;em&gt;randomForest&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;In each ensemble, we have 1000 trees and do a grid search of the following hyperparameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;mtry&lt;/code&gt;: The number of predictors to randomly sample at each split.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;min_n&lt;/code&gt;: The minimum number of data points in a node required to further split the node.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;Random Forest Model Specification (regression)

Main Arguments:
  mtry = tune()
  trees = 1000
  min_n = tune()

Engine-Specific Arguments:
  objective = reg:squarederror

Computational engine: randomForest &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the top 10 RMSE values and hyperparameter combinations:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 10 x 3
   min_n  mtry mean_rmse
   &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;
 1     4    85     0.134
 2     3   140     0.135
 3    14    90     0.135
 4     6    45     0.136
 5     9   138     0.136
 6    13   158     0.137
 7     9   183     0.137
 8    19    56     0.138
 9    21   130     0.138
10     5   218     0.138&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;whats-notable-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What’s notable?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;After cross validation, we get the best RMSE of 0.134 with mtry = 85 and min_n = 4.&lt;/li&gt;
&lt;li&gt;This is no improvement in RMSE compared to &lt;code&gt;glmnet&lt;/code&gt; and &lt;code&gt;randomForest&lt;/code&gt; cross validation takes much longer to execute than &lt;code&gt;glmnet&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here a plot of the &lt;code&gt;randomForest&lt;/code&gt; hyperparameter grid along with the best hyperparameter combination:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part2-models/index_files/figure-html/plot_randomForest-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;xgboost&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;em&gt;xgboost&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;In each ensemble we have 1000 trees and do a grid search of the following hyperparameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;min_n&lt;/code&gt;: The minimum number of data points in a node required to further split the node.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tree_depth&lt;/code&gt;: The maximum depth or the number of splits of the tree.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;learn_rate&lt;/code&gt;: The rate at which the boosting algorithm adapts from one iteration to another.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;Boosted Tree Model Specification (regression)

Main Arguments:
  trees = 1000
  min_n = tune()
  tree_depth = tune()
  learn_rate = tune()

Engine-Specific Arguments:
  objective = reg:squarederror

Computational engine: xgboost &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the top 10 RMSE values and hyperparameter combinations:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 10 x 4
   min_n tree_depth learn_rate mean_rmse
   &amp;lt;int&amp;gt;      &amp;lt;int&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
 1    13          3  0.0309        0.124
 2    40          4  0.0350        0.126
 3     6          8  0.0469        0.126
 4    34         15  0.0172        0.127
 5    28         10  0.0336        0.128
 6    20         14  0.00348       0.389
 7    22          7  0.000953      4.46 
 8     3          2  0.000528      6.81 
 9    10         12  0.000401      7.73 
10    34          3  0.0000802    10.6  &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;whats-notable-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What’s notable?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;After cross validation, we get the best RMSE of 0.124 with min_n = 13, tree_depth = 3 and learn_rate = 0.0309.&lt;/li&gt;
&lt;li&gt;Gives the best RMSE compared to &lt;code&gt;glmnet&lt;/code&gt; and &lt;code&gt;randomForest&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;However, &lt;code&gt;xgboost&lt;/code&gt; cross validation takes longer to execute than that of &lt;code&gt;glmnet&lt;/code&gt;, but is faster than that of &lt;code&gt;randomForest&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Here a 3D plot of the `xgboost` hyperparameter grid: --&gt;
&lt;!-- &lt;center&gt; --&gt;
&lt;!-- ```{r plot_xgboost} --&gt;
&lt;!-- library(plotly) --&gt;
&lt;!-- plot_ly(param_grid_xgboost, x = ~min_n, y = ~tree_depth, z = ~learn_rate) %&gt;% --&gt;
&lt;!--   add_markers() %&gt;% --&gt;
&lt;!--   layout(font = list(family = &#34;Roboto Condensed&#34;), --&gt;
&lt;!--          title = list(text = &#34;Scatterplot of min_n, tree_depth and learn_rate&#34;, font = list(size = 22)), --&gt;
&lt;!--          scene = list(xaxis = list(title = &#39;min_n&#39;), --&gt;
&lt;!--                       yaxis = list(title = &#39;tree_depth&#39;), --&gt;
&lt;!--                       zaxis = list(title = &#39;learn_rate&#39;))) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- &lt;/center&gt; --&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;finalizing-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Finalizing Models&lt;/h2&gt;
&lt;p&gt;For each model, we found the combination of hyperparameters that minimize RMSE. Using those parameters, we can now train the same models on the entire training dataset. Finally, we can use the trained models to predict log(SalePrice) on the entire training set to see the actual v/s predicted log(SalePrice) results.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part2-models/index_files/figure-html/plot_train-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;whats-notable-4&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What’s notable?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Both &lt;code&gt;randomForest&lt;/code&gt; and &lt;code&gt;xgboost&lt;/code&gt; models do a fantastic job of predicting log(SalePrice) with the tuned parameters, as the predictions lie close to the straight line drawn at 45 degrees.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;glmnet&lt;/code&gt; model shows a couple of outliers with Ids &lt;strong&gt;524&lt;/strong&gt; and &lt;strong&gt;1299&lt;/strong&gt; whose predicted values are far in excess of their actual values. Even properties whose &lt;code&gt;SalePrice&lt;/code&gt; is at the lower end, show a wide dispersion in prediced values.&lt;/li&gt;
&lt;li&gt;But the true performance can only be measured on unseen testing data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;performance-on-test-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Performance on Test Data&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part2-models/index_files/figure-html/plot_test-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 3 x 3
  model        test_rmse cv_rmse
  &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
1 glmnet           0.129   0.127
2 randomForest     0.139   0.134
3 xgboost          0.128   0.124&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;whats-notable-5&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What’s notable?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;All models have similar RMSE on the unseen testing set as their cross validated RMSE, which shows the cross validation process and hyperparameters worked very well.&lt;/li&gt;
&lt;li&gt;Records with Ids &lt;strong&gt;1537&lt;/strong&gt; and &lt;strong&gt;2217&lt;/strong&gt; are outliers, as none of the models are able to predict close to actual values.&lt;/li&gt;
&lt;li&gt;Looking at the test RMSE, we could finalize &lt;code&gt;xgboost&lt;/code&gt; as the model that generalizes very well on this dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;feature-importance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Feature Importance&lt;/h2&gt;
&lt;p&gt;Even though &lt;code&gt;xgboost&lt;/code&gt; is not as easily interpretable as a linear model, we could use variable importance plots to determine the most important features selected by the model.&lt;/p&gt;
&lt;p&gt;Let’s take a look at the top 10 most important features of our finalized &lt;code&gt;xgboost&lt;/code&gt; model:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part2-models/index_files/figure-html/feature_importance-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Correlations of numerical features are plotted side-by-side. All features have a correlation of 0.5 or more with &lt;code&gt;SalePrice&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;All of the top 10 features make sense. To evaluate &lt;code&gt;SalePrice&lt;/code&gt;, a buyer would definitely look at total square footage, overall quality, neighborhood, number of bathrooms, kitchen quality, age of property, etc.&lt;/li&gt;
&lt;li&gt;This shows, our finalized model generalizes well and makes very reasonable choices in terms of features.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;new-property-premium&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;New Property Premium&lt;/h2&gt;
&lt;p&gt;Among the top 10 features by importance in our final model, most of the features like square footage, neighborhood and number of bathrooms remain the same throughout the life of the property. Quality and condition of property does change but their evaluation is mostly subjective. The only other feature that cannot be disputed to change over time is &lt;code&gt;PropertyAge&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;So, how would the predicted &lt;code&gt;SalePrice&lt;/code&gt; differ if a property was newly constructed vis-a-vis the same property if it were constructed more than 30 years earlier, and all the times in between?&lt;/p&gt;
&lt;p&gt;We could pick a couple of properties at random, change &lt;code&gt;PropertyAge&lt;/code&gt; and see its impact on &lt;code&gt;SalePrice&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/ames-housing-part2-models/index_files/figure-html/property_appreciation-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see there’s a small premium for a newly constructed property v/s an older property of the same build, quality and condition. This premium isn’t very much in a place like Ames, IA but we’d reckon it would be much higher in a larger metropolitan city.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Diamonds - Part 3 - A polished gem - Building Non-linear Models</title>
      <link>/casestudies/diamonds-part3-non-linear-models/</link>
      <pubDate>Thu, 22 Dec 2016 00:00:00 +0000</pubDate>
      <guid>/casestudies/diamonds-part3-non-linear-models/</guid>
      <description>


&lt;div id=&#34;other-posts-in-this-series&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Other posts in this series:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nitingupta.com/casestudies/diamonds-part1-eda/&#34;&gt;Diamonds - Part 1 - In the rough - An Exploratory Data Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nitingupta.com/casestudies/diamonds-part2-linear-models/&#34;&gt;Diamonds - Part 2 - A cut above - Building Linear Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In a couple of previous posts, we tried to understand what attributes of diamonds are important to determine their prices. We showed that &lt;code&gt;carat&lt;/code&gt;, &lt;code&gt;clarity&lt;/code&gt; and &lt;code&gt;color&lt;/code&gt; are the most important predictors of &lt;code&gt;price&lt;/code&gt;. We arrived at this conclusion after doing a detailed exploratory data analysis. Finally we fit linear models to predict prices and determined the best model from the metrics.&lt;/p&gt;
&lt;p&gt;In this post, we will use non-linear regression models to predict diamond prices and compare them with those from linear models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;training-non-linear-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Training Non-linear Models&lt;/h2&gt;
&lt;p&gt;We’ll follow some of the same steps as we did for linear models, while transforming some predictors:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Partition the dataset into training and testing sets in the proportion 75% and 25% respectively.&lt;/li&gt;
&lt;li&gt;Stratify the partitioning by &lt;code&gt;clarity&lt;/code&gt;, so both training and testing sets have the same distributions of this feature.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;clarity&lt;/code&gt;, &lt;code&gt;color&lt;/code&gt; and &lt;code&gt;cut&lt;/code&gt; have ordered categories from lowest to highest grades. The &lt;code&gt;randomForest&lt;/code&gt; method requires no change in representing this data before training the models, however &lt;code&gt;xgboost&lt;/code&gt; and &lt;code&gt;keras&lt;/code&gt; methods require all the predictors to be in numerical form. &lt;a href=&#34;https://statmodeling.stat.columbia.edu/2009/10/06/coding_ordinal/&#34;&gt;Two methods&lt;/a&gt; could be used for transforming the categorical data:
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Use one-hot encoding to convert categorical data to sparse data with 0s and 1s. This way, each category in &lt;code&gt;clarity&lt;/code&gt;, &lt;code&gt;color&lt;/code&gt; and &lt;code&gt;cut&lt;/code&gt; is converted to a new predictor in binary form. A disadvantage of this method is that it treates ordered categorical data the same as unordered categorical data, so the ordinality is lost in transformation. However, non-linear models should be able to infer the ordinality as our training sample is sufficiently large.&lt;/li&gt;
&lt;li&gt;Represent the ordinal categories from lowest to highest grades in integer form. However, this creates a linear gradation from one category to another, which may not be a suitable choice here.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Center and scale all values in the training set and build a matrix of predictors.&lt;/li&gt;
&lt;li&gt;Fit a non-linear model with the training set.&lt;/li&gt;
&lt;li&gt;Make predictions on the testing set and determine model metrics.&lt;/li&gt;
&lt;li&gt;Wrap all the steps above inside a function in which the model formula, and a seed could be passed that randomizes the partition of training and testing sets.&lt;/li&gt;
&lt;li&gt;Run multiple iterations of models with different seeds, and compute their average metrics, that would reflect results on unseen data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here are the average metrics for all the models trained with &lt;code&gt;keras&lt;/code&gt;, &lt;code&gt;randomForest&lt;/code&gt; and &lt;code&gt;xgboost&lt;/code&gt; regression methods:&lt;/p&gt;
&lt;table class=&#34;gmisc_table&#34; style=&#34;border-collapse: collapse; margin-top: 1em; margin-bottom: 1em;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;border-top: 2px solid grey;&#34;&gt;
&lt;/th&gt;
&lt;th colspan=&#34;3&#34; style=&#34;font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: center;&#34;&gt;
mae
&lt;/th&gt;
&lt;th style=&#34;border-top: 2px solid grey;; border-bottom: hidden;&#34;&gt;
 
&lt;/th&gt;
&lt;th colspan=&#34;3&#34; style=&#34;font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: center;&#34;&gt;
rmse
&lt;/th&gt;
&lt;th style=&#34;border-top: 2px solid grey;; border-bottom: hidden;&#34;&gt;
 
&lt;/th&gt;
&lt;th colspan=&#34;3&#34; style=&#34;font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: center;&#34;&gt;
rsq
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;border-bottom: 1px solid grey;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
keras
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
randomForest
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
xgboost
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey;&#34; colspan=&#34;1&#34;&gt;
 
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
keras
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
randomForest
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
xgboost
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey;&#34; colspan=&#34;1&#34;&gt;
 
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
keras
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
randomForest
&lt;/th&gt;
&lt;th style=&#34;border-bottom: 1px solid grey; text-align: center;&#34;&gt;
xgboost
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;border-right: 1px solid black; text-align: left;&#34;&gt;
price ~ .
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
360.55
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
262.35
&lt;/td&gt;
&lt;td style=&#34;border-right: 1px solid black; text-align: right;&#34;&gt;
280.49
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
989.71
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
529.28
&lt;/td&gt;
&lt;td style=&#34;border-right: 1px solid black; text-align: right;&#34;&gt;
540.76
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
0.93
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
0.98
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
0.98
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-right: 1px solid black; text-align: left;&#34;&gt;
price ~ carat
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
860.29
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
816.1
&lt;/td&gt;
&lt;td style=&#34;border-right: 1px solid black; text-align: right;&#34;&gt;
815.76
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
1499.2
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
1427.25
&lt;/td&gt;
&lt;td style=&#34;border-right: 1px solid black; text-align: right;&#34;&gt;
1427.35
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
0.86
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
0.87
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
0.87
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-right: 1px solid black; text-align: left;&#34;&gt;
price ~ carat + clarity
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
590.32
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
548.67
&lt;/td&gt;
&lt;td style=&#34;border-right: 1px solid black; text-align: right;&#34;&gt;
544.48
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
1040.69
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
1006.61
&lt;/td&gt;
&lt;td style=&#34;border-right: 1px solid black; text-align: right;&#34;&gt;
992.46
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
0.93
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
0.94
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
0.94
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-right: 1px solid black; text-align: left;&#34;&gt;
price ~ carat + clarity + color
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
358.85
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
305.17
&lt;/td&gt;
&lt;td style=&#34;border-right: 1px solid black; text-align: right;&#34;&gt;
306.86
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
645.4
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
571.73
&lt;/td&gt;
&lt;td style=&#34;border-right: 1px solid black; text-align: right;&#34;&gt;
575.3
&lt;/td&gt;
&lt;td style colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
0.97
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
0.98
&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;
0.98
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; border-right: 1px solid black; text-align: left;&#34;&gt;
price ~ carat + clarity + color + cut
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: right;&#34;&gt;
347.99
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: right;&#34;&gt;
285.96
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; border-right: 1px solid black; text-align: right;&#34;&gt;
282.38
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey;&#34; colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: right;&#34;&gt;
626.78
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: right;&#34;&gt;
545.02
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; border-right: 1px solid black; text-align: right;&#34;&gt;
541.63
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey;&#34; colspan=&#34;1&#34;&gt;
 
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: right;&#34;&gt;
0.98
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: right;&#34;&gt;
0.98
&lt;/td&gt;
&lt;td style=&#34;border-bottom: 2px solid grey; text-align: right;&#34;&gt;
0.98
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Looking at the r-squared terms, it is remarkable how well all the models have been able to infer the complex relationship between &lt;code&gt;price&lt;/code&gt; and &lt;code&gt;carat&lt;/code&gt;. To fit linear models, we needed to transform &lt;code&gt;price&lt;/code&gt; to logarithmic terms and take the cube root of &lt;code&gt;carat&lt;/code&gt;. The neural network as well as the decision tree based models do this all on their own. The root mean squared error is in $ terms so it is easier to interpret. Considering the mean and standard deviation of &lt;code&gt;price&lt;/code&gt; in the dataset is about $4000, the root mean squared errors of the models are very low.&lt;/p&gt;
&lt;p&gt;Exploratory data analysis adds value here, as the models with &lt;code&gt;carat&lt;/code&gt;, &lt;code&gt;clarity&lt;/code&gt; and &lt;code&gt;color&lt;/code&gt; give excellent results. Including &lt;code&gt;cut&lt;/code&gt; in the models does not provide any significant benefits and results in overfitted models.&lt;/p&gt;
&lt;p&gt;Even the base models with all predictors: &lt;strong&gt;price ~ .&lt;/strong&gt; (where some of them are confounders), do a very good job of explaning the variance. Decision tree and neural network models are unaffected by multi-collinearity. We can use local model interpretations to determine the most important predictors from these models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;local-interpretable-model-agnostic-explanations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Local Interpretable Model-agnostic Explanations&lt;/h2&gt;
&lt;p&gt;LIME is a method for explaining black-box machine learning models. It can help visualize and explain individual predictions. It makes the assumption that every complex model is linear on a local scale. So it is possible to fit a simple model around a single observation that will behave how the global model behaves at that locality. The simple model can be used to explain the predictions of the more complex model locally.&lt;/p&gt;
&lt;p&gt;The generalized algorithm LIME applies is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Given an observation, permute it to create replicated feature data with slight value modifications.&lt;/li&gt;
&lt;li&gt;Compute similarity distance measure between original observation and permuted observations.&lt;/li&gt;
&lt;li&gt;Apply selected machine learning model to predict outcomes of permuted data.&lt;/li&gt;
&lt;li&gt;Select m number of features to best describe predicted outcomes.&lt;/li&gt;
&lt;li&gt;Fit a simple model to the permuted data, explaining the complex model outcome with m features from the permuted data weighted by its similarity to the original observation .&lt;/li&gt;
&lt;li&gt;Use the resulting feature weights to explain local behavior.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here we will select 5 features that best describe the predicted outcomes for 6 random observations from the testing set.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part3-non-linear-models/index_files/figure-html/plot_feature_importance-1.png&#34; width=&#34;960&#34; /&gt;&lt;img src=&#34;/casestudies/diamonds-part3-non-linear-models/index_files/figure-html/plot_feature_importance-2.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The features by importance that best explain the predictions in these 6 random samples are &lt;code&gt;carat&lt;/code&gt;, &lt;code&gt;clarity&lt;/code&gt;, &lt;code&gt;color&lt;/code&gt;, &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part3-non-linear-models/index_files/figure-html/plot_feature_heatmap-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/casestudies/diamonds-part3-non-linear-models/index_files/figure-html/plot_feature_heatmap-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We know that &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are co-linear with &lt;code&gt;carat&lt;/code&gt;, which is why it is good practice to remove any redundant features from the training data before applying any machine learning algorithm. We find the model with the best metrics turns out to be the one using &lt;code&gt;carat&lt;/code&gt;, &lt;code&gt;clarity&lt;/code&gt; and &lt;code&gt;color&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;actual-vs-predicted&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Actual v/s Predicted&lt;/h2&gt;
&lt;p&gt;Finally, here are the scatterplots of actual v/s predicted &lt;code&gt;price&lt;/code&gt; from the best model on the testing set, using the 3 regression methods:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part3-non-linear-models/index_files/figure-html/best_model_plot-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The scatterplots are shown with both linear and logarithmic axes. Even though the results from all the 3 methods have roughly similar &lt;strong&gt;r-squared&lt;/strong&gt; and &lt;strong&gt;rmse&lt;/strong&gt; values, we can see predicted prices from keras have more dispersion than the two decision-tree methods at the higher end. The decision-tree based methods appear do a better job of predicting prices at the lower end with lesser dispersion.&lt;/p&gt;
&lt;p&gt;As in the case with linear models, the variance in predicted diamond prices increases with &lt;code&gt;price&lt;/code&gt;. But unlike linear models, the non-linear models do not produce extreme outliers in predicted prices. So, not only do non-linear methods do a fantastic job in inferring the relationships between &lt;code&gt;price&lt;/code&gt; and its predictors, they also predict prices within a reasonable range.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;All the 3 non-linear regression methods can infer the complex relationship between &lt;code&gt;price&lt;/code&gt;, &lt;code&gt;carat&lt;/code&gt; and other predictors, without the need for feature engineering.&lt;/li&gt;
&lt;li&gt;Exploratory Data Analysis is useful in removing the redundant features from the training dataset, resulting in both faster execution, as well as much better metrics.&lt;/li&gt;
&lt;li&gt;In terms of time taken to train the models, &lt;code&gt;keras&lt;/code&gt; neural network models execute the fastest by virtue of being able to use GPUs.&lt;/li&gt;
&lt;li&gt;Among the decision-tree based methods, &lt;code&gt;xgboost&lt;/code&gt; models train much faster than &lt;code&gt;randomForest&lt;/code&gt; models.&lt;/li&gt;
&lt;li&gt;Multiple CPUs can be used to run &lt;code&gt;randomForest&lt;/code&gt; and &lt;code&gt;xgboost&lt;/code&gt; methods. RAM is the only limiting constraint, when trained on a local machine.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Diamonds - Part 2 - A cut above - Building Linear Models</title>
      <link>/casestudies/diamonds-part2-linear-models/</link>
      <pubDate>Wed, 21 Dec 2016 00:00:00 +0000</pubDate>
      <guid>/casestudies/diamonds-part2-linear-models/</guid>
      <description>


&lt;p&gt;In a &lt;a href=&#34;https://www.nitingupta.com/casestudies/diamonds-part1-eda/&#34;&gt;previous post&lt;/a&gt; in this series, we did an exploratory data analysis of the &lt;code&gt;diamonds&lt;/code&gt; dataset and found that &lt;code&gt;carat&lt;/code&gt;, &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;, &lt;code&gt;z&lt;/code&gt; were strongly correlated with &lt;code&gt;price&lt;/code&gt;. To some extent, &lt;code&gt;clarity&lt;/code&gt; also appeared to provide some predictive ability.&lt;/p&gt;
&lt;p&gt;In this post, we will build linear models and see how well they predict the &lt;code&gt;price&lt;/code&gt; of diamonds.&lt;/p&gt;
&lt;p&gt;Before we do any transformations, feature engineering or feature selections for our model, let’s see what kind of results we get from a base linear model, that uses all the features to predict &lt;code&gt;price&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
Call:
lm(formula = price ~ ., data = diamonds)

Residuals:
   Min     1Q Median     3Q    Max 
-21376   -592   -183    376  10694 

Coefficients:
            Estimate Std. Error t value             Pr(&amp;gt;|t|)    
(Intercept)  5753.76     396.63   14.51 &amp;lt; 0.0000000000000002 ***
carat       11256.98      48.63  231.49 &amp;lt; 0.0000000000000002 ***
cut.L         584.46      22.48   26.00 &amp;lt; 0.0000000000000002 ***
cut.Q        -301.91      17.99  -16.78 &amp;lt; 0.0000000000000002 ***
cut.C         148.03      15.48    9.56 &amp;lt; 0.0000000000000002 ***
cut^4         -20.79      12.38   -1.68               0.0929 .  
color.L     -1952.16      17.34 -112.57 &amp;lt; 0.0000000000000002 ***
color.Q      -672.05      15.78  -42.60 &amp;lt; 0.0000000000000002 ***
color.C      -165.28      14.72  -11.22 &amp;lt; 0.0000000000000002 ***
color^4        38.20      13.53    2.82               0.0047 ** 
color^5       -95.79      12.78   -7.50    0.000000000000066 ***
color^6       -48.47      11.61   -4.17    0.000030090737193 ***
clarity.L    4097.43      30.26  135.41 &amp;lt; 0.0000000000000002 ***
clarity.Q   -1925.00      28.23  -68.20 &amp;lt; 0.0000000000000002 ***
clarity.C     982.20      24.15   40.67 &amp;lt; 0.0000000000000002 ***
clarity^4    -364.92      19.29  -18.92 &amp;lt; 0.0000000000000002 ***
clarity^5     233.56      15.75   14.83 &amp;lt; 0.0000000000000002 ***
clarity^6       6.88      13.72    0.50               0.6157    
clarity^7      90.64      12.10    7.49    0.000000000000071 ***
depth         -63.81       4.53  -14.07 &amp;lt; 0.0000000000000002 ***
table         -26.47       2.91   -9.09 &amp;lt; 0.0000000000000002 ***
x           -1008.26      32.90  -30.65 &amp;lt; 0.0000000000000002 ***
y               9.61      19.33    0.50               0.6192    
z             -50.12      33.49   -1.50               0.1345    
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

Residual standard error: 1130 on 53916 degrees of freedom
Multiple R-squared:  0.92,  Adjusted R-squared:  0.92 
F-statistic: 2.69e+04 on 23 and 53916 DF,  p-value: &amp;lt;0.0000000000000002&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 3 x 3
  .metric .estimator .estimate
  &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
1 rmse    standard    1130.   
2 rsq     standard       0.920
3 mae     standard     740.   &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model summary shows it is an overfitted model. Among other things, we know that &lt;code&gt;depth&lt;/code&gt; and &lt;code&gt;table&lt;/code&gt; have no impact on &lt;code&gt;price&lt;/code&gt;, yet these are shown to be highly significant. Root Mean Squared Error (rmse) and other metrics are also shown above.&lt;/p&gt;
&lt;p&gt;Let’s make a plot of actual v/s predicted prices to visualize how well this base model performs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part2-linear-models/index_files/figure-html/simple_lm_model_plot-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If the predictions are good, the points should lie close to a straight line drawn at 45 degrees. We can see this base model does a poor job of predicting prices. Worst of all, the model predicts negative prices on the lower end.
It shows that &lt;code&gt;price&lt;/code&gt; has to be log transformed to avoid these absurdities.&lt;/p&gt;
&lt;div id=&#34;feature-engineering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Feature Engineering&lt;/h2&gt;
&lt;p&gt;We know the price of a diamond is strongly correlated with its size. All things equal, the larger the diamond, the greater its price.&lt;/p&gt;
&lt;p&gt;As a first approximation, we can assume a diamond is a cuboid with dimensions &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;z&lt;/code&gt;. Then, we can compute its &lt;code&gt;volume&lt;/code&gt; as x * y * z.
As these 3 dimensions are highly correlated, we can compute a geometrical average dimension by taking the cube root of &lt;code&gt;volume&lt;/code&gt;, and retain a linear relationship with &lt;code&gt;log(price)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Another way to calculate an average dimension is by using high school chemistry. Mass, volume and density are related to each other by the equation:&lt;/p&gt;
&lt;p&gt;$ density = mass/volume $&lt;/p&gt;
&lt;p&gt;We can find out that 1 carat = 0.2 gms. Dividing by the density of diamond (3.51 gms/cc) would give us its volume in cc, which could be converted to a geometrical average dimension by taking the cube root.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part2-linear-models/index_files/figure-html/feature_engineering-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Even though both methods yield similar results, we could see that the density method results in a narrower range. But which method would be more robust?
Keep in mind there are 20 &lt;code&gt;z&lt;/code&gt; values that are 0. In 7 of these records both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are 0 too, which means these values were not recorded reliably.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 20 x 10
   carat cut       color clarity depth table price     x     y     z
   &amp;lt;dbl&amp;gt; &amp;lt;ord&amp;gt;     &amp;lt;ord&amp;gt; &amp;lt;ord&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
 1  1    Premium   G     SI2      59.1    59  3142  6.55  6.48     0
 2  1.01 Premium   H     I1       58.1    59  3167  6.66  6.6      0
 3  1.1  Premium   G     SI2      63      59  3696  6.5   6.47     0
 4  1.01 Premium   F     SI2      59.2    58  3837  6.5   6.47     0
 5  1.5  Good      G     I1       64      61  4731  7.15  7.04     0
 6  1.07 Ideal     F     SI2      61.6    56  4954  0     6.62     0
 7  1    Very Good H     VS2      63.3    53  5139  0     0        0
 8  1.15 Ideal     G     VS2      59.2    56  5564  6.88  6.83     0
 9  1.14 Fair      G     VS1      57.5    67  6381  0     0        0
10  2.18 Premium   H     SI2      59.4    61 12631  8.49  8.45     0
11  1.56 Ideal     G     VS2      62.2    54 12800  0     0        0
12  2.25 Premium   I     SI1      61.3    58 15397  8.52  8.42     0
13  1.2  Premium   D     VVS1     62.1    59 15686  0     0        0
14  2.2  Premium   H     SI1      61.2    59 17265  8.42  8.37     0
15  2.25 Premium   H     SI2      62.8    59 18034  0     0        0
16  2.02 Premium   H     VS2      62.7    53 18207  8.02  7.95     0
17  2.8  Good      G     SI2      63.8    58 18788  8.9   8.85     0
18  0.71 Good      F     SI2      64.1    60  2130  0     0        0
19  0.71 Good      F     SI2      64.1    60  2130  0     0        0
20  1.12 Premium   G     I1       60.4    59  2383  6.71  6.67     0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In all of these records, the &lt;code&gt;carat&lt;/code&gt; values were recorded reliably and are probably more accurate than the dimensions.
Hence, we might prefer the density method of generating this feature.&lt;/p&gt;
&lt;p&gt;Furthermore, since density is a constant, dividing by a constant to calculate volume isn’t really necessary. Instead, a cube root transformation could be applied to &lt;code&gt;carat&lt;/code&gt; itself for the purposes of predictive modelling that would result in a linear relationship between &lt;span class=&#34;math inline&#34;&gt;\(log(price)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(carat^{1/3}\)&lt;/span&gt;.
It is the reason why we’re fitting a linear model because the model is linear in its parameters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;training-linear-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Training Linear Models&lt;/h2&gt;
&lt;p&gt;Here are the steps for building linear models and computing metrics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Partition the dataset into training and testing sets in the proportion 75% and 25% respectively.&lt;/li&gt;
&lt;li&gt;Since &lt;code&gt;clarity&lt;/code&gt; is one of the main predictors, stratify the partitioning by &lt;code&gt;clarity&lt;/code&gt;, so both training and testing sets have the same distributions of this feature.&lt;/li&gt;
&lt;li&gt;Fit a linear model with the training set.&lt;/li&gt;
&lt;li&gt;Make predictions on the testing set and determine model metrics.&lt;/li&gt;
&lt;li&gt;Wrap all the steps above inside a function in which the model formula and a seed could be passed. Since the seed determines the random partitioning, it helps to minimize vagaries in partitioning the training and testing sets before fitting models.&lt;/li&gt;
&lt;li&gt;Run multiple iterations of a model with different seeds, and compute its average metrics, that would reflect the results on unseen data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here’s a sample split of training and testing set, stratified by &lt;code&gt;clarity&lt;/code&gt;. As we can see, the training and testing sets have similar distributions.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dfTrain$clarity 
       n  missing distinct 
   40457        0        8 

lowest : I1   SI2  SI1  VS2  VS1 , highest: VS2  VS1  VVS2 VVS1 IF  
                                                          
Value         I1   SI2   SI1   VS2   VS1  VVS2  VVS1    IF
Frequency    552  6895  9826  9222  6125  3780  2722  1335
Proportion 0.014 0.170 0.243 0.228 0.151 0.093 0.067 0.033&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;dfTest$clarity 
       n  missing distinct 
   13483        0        8 

lowest : I1   SI2  SI1  VS2  VS1 , highest: VS2  VS1  VVS2 VVS1 IF  
                                                          
Value         I1   SI2   SI1   VS2   VS1  VVS2  VVS1    IF
Frequency    189  2299  3239  3036  2046  1286   933   455
Proportion 0.014 0.171 0.240 0.225 0.152 0.095 0.069 0.034&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After running 5 iterations of each model with a different seed, here are the average metrics:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 5 x 4
  model                                                 rmse   rsq   mae
  &amp;lt;chr&amp;gt;                                                &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
1 log(price) ~ .                                      11055. 0.670  570.
2 log(price) ~ I(carat^(1/3))                          2893. 0.687 1039.
3 log(price) ~ I(carat^(1/3)) + clarity                2312. 0.807  881.
4 log(price) ~ I(carat^(1/3)) + clarity + color        1870. 0.870  631.
5 log(price) ~ I(carat^(1/3)) + clarity + color + cut  1848. 0.875  625.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first model with all predictors is an overfitted one.&lt;/p&gt;
&lt;p&gt;The model with &lt;code&gt;carat&lt;/code&gt;, &lt;code&gt;clarity&lt;/code&gt; and &lt;code&gt;color&lt;/code&gt; provides the best combination of root mean squared error and r-squared, that explains the most variance.
This is our final model.
Including &lt;code&gt;cut&lt;/code&gt; in the model has diminishing benefits, and tends to overfit the data.&lt;/p&gt;
&lt;p&gt;Here’s the summary of our final model:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
Call:
lm(formula = model_formula, data = dfTrain)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.6022 -0.1034  0.0145  0.1066  1.7941 

Coefficients:
                Estimate Std. Error t value             Pr(&amp;gt;|t|)    
(Intercept)     2.147009   0.004993  429.99 &amp;lt; 0.0000000000000002 ***
I(carat^(1/3))  6.246412   0.005365 1164.27 &amp;lt; 0.0000000000000002 ***
clarity.L       0.922295   0.005036  183.15 &amp;lt; 0.0000000000000002 ***
clarity.Q      -0.295539   0.004734  -62.43 &amp;lt; 0.0000000000000002 ***
clarity.C       0.166979   0.004068   41.05 &amp;lt; 0.0000000000000002 ***
clarity^4      -0.068591   0.003260  -21.04 &amp;lt; 0.0000000000000002 ***
clarity^5       0.032833   0.002669   12.30 &amp;lt; 0.0000000000000002 ***
clarity^6      -0.001904   0.002325   -0.82              0.41288    
clarity^7       0.025508   0.002049   12.45 &amp;lt; 0.0000000000000002 ***
color.L        -0.488882   0.002927 -167.05 &amp;lt; 0.0000000000000002 ***
color.Q        -0.117319   0.002680  -43.78 &amp;lt; 0.0000000000000002 ***
color.C        -0.012230   0.002497   -4.90           0.00000098 ***
color^4         0.019007   0.002288    8.31 &amp;lt; 0.0000000000000002 ***
color^5        -0.008110   0.002159   -3.76              0.00017 ***
color^6        -0.000396   0.001967   -0.20              0.84055    
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

Residual standard error: 0.166 on 40442 degrees of freedom
Multiple R-squared:  0.973, Adjusted R-squared:  0.973 
F-statistic: 1.05e+05 on 14 and 40442 DF,  p-value: &amp;lt;0.0000000000000002&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part2-linear-models/index_files/figure-html/final_model_summary-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here’s a scatterplot of actual v/s predicted log(price) from our final model on the testing set:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/casestudies/diamonds-part2-linear-models/index_files/figure-html/final_model_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The points lie close to the 45 degress line. However, on the high end, there are many outliers where actual and predicted values have very high variance.
Nevertheless, this is as good as it gets.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Building blocks of systematic investment strategies</title>
      <link>/investing/building-blocks-of-investment-strategies/</link>
      <pubDate>Fri, 22 Jul 2016 00:00:00 +0000</pubDate>
      <guid>/investing/building-blocks-of-investment-strategies/</guid>
      <description>


&lt;p&gt;It is important to understand the building blocks of systematic investing strategies before learning how to build them. Here is a schematic from the book, &lt;a href=&#34;https://www.amazon.com/Inside-Black-Box-Quantitative-Frequency-ebook/dp/B00BZ9WAVW/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1487389686&amp;amp;sr=1-1&amp;amp;keywords=rishi+narang&#34;&gt;Inside the Black Box - The Simple Truth about Quantitative Trading&lt;/a&gt; by Rishi Narang, that provides a good way to visualize these building blocks and how they fit together in a system.&lt;/p&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;buildingblocks.png&#34; alt=&#34;The Black Box Revealed&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;The Black Box Revealed&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;p&gt;The author had aptly titled it - The Black Box Revealed, since that is how most people perceive it to be from the outside. From this schematic, it is easier to understand and visualize how a system is built with a systematic approach.&lt;/p&gt;
&lt;p&gt;Here are the components of the system:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data - Encapsulates the processes of raw data collection, cleaning and preparation in tidy form to be used by the models.&lt;/li&gt;
&lt;li&gt;Alpha Model - Describes the core strategy, product universe (stocks, ETFs, futures) and the rules to be implemented.&lt;/li&gt;
&lt;li&gt;Risk Model - Describes position sizes or how capital/risk is allocated. This is just as much important as the alpha model itself.&lt;/li&gt;
&lt;li&gt;Transaction Cost Model - Describes the transaction costs by product. These may be theoretical numbers while backtesting the strategy using historical data and later adjusted based on live execution data.&lt;/li&gt;
&lt;li&gt;Portfolio Construction Model - Describes how a portfolio is constructed after putting together positions by product across one or more rules of the core strategy.&lt;/li&gt;
&lt;li&gt;Execution Model - Encapsulates a live trading system. Describes the tactical implementation of the core strategy.&lt;/li&gt;
&lt;li&gt;Research - Sound research forms the underpinning of all models and is driven by quantitative and statistical analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The interactions shown are typical, although there could be more depending upon the implementation. More than one model could also be combined together, depending upon how the strategy is formulated. Nonetheless, for most practitioners looking to build a complete system from scratch, this is a pretty good blueprint.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
